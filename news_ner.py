#!/usr/bin/env python3
"""
NER –º–æ–¥—É–ª—å –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏–º–µ–Ω–æ–≤–∞–Ω–Ω—ã—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π –∏–∑ –Ω–æ–≤–æ—Å—Ç–µ–π
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç natasha –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞
"""

from natasha import (
    Segmenter,
    MorphVocab,
    NewsEmbedding,
    NewsMorphTagger,
    NewsNERTagger,
    Doc
)
from typing import List, Dict, Set
import re
import pymorphy2

class NewsNERExtractor:
    """–ò–∑–≤–ª–µ–∫–∞—Ç–µ–ª—å –∏–º–µ–Ω–æ–≤–∞–Ω–Ω—ã—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π –∏–∑ –Ω–æ–≤–æ—Å—Ç–µ–π"""

    def __init__(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è NER-–º–æ–¥–µ–ª–µ–π"""
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã natasha
        self.segmenter = Segmenter()
        self.morph_vocab = MorphVocab()
        self.emb = NewsEmbedding()
        self.morph_tagger = NewsMorphTagger(self.emb)
        self.ner_tagger = NewsNERTagger(self.emb)

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º pymorphy2 –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
        self.morph = pymorphy2.MorphAnalyzer()

        # –°–ª–æ–≤–∞—Ä—å –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ —Ç–∏–ø–æ–≤ —Å—É—â–Ω–æ—Å—Ç–µ–π
        self.entity_type_map = {
            'PER': 'person',      # –ü–µ—Ä—Å–æ–Ω–∞
            'LOC': 'location',    # –õ–æ–∫–∞—Ü–∏—è
            'ORG': 'organization' # –û—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è
        }

    def normalize_entity(self, entity_text: str, entity_type: str) -> str:
        """
        –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞—Ç—å —Å—É—â–Ω–æ—Å—Ç—å (–ø—Ä–∏–≤–µ—Å—Ç–∏ –∫ –Ω–∞—á–∞–ª—å–Ω–æ–π —Ñ–æ—Ä–º–µ)

        Args:
            entity_text: —Ç–µ–∫—Å—Ç —Å—É—â–Ω–æ—Å—Ç–∏
            entity_type: —Ç–∏–ø —Å—É—â–Ω–æ—Å—Ç–∏ (person/organization/location)

        Returns:
            –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è —Ñ–æ—Ä–º–∞ —Å—É—â–Ω–æ—Å—Ç–∏
        """
        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —Å–ª–æ–≤–∞
        words = entity_text.split()
        normalized_words = []

        for word in words:
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —Å–ª–æ–≤–∞ –∏ –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—ã (–¶–ë, –†–§ –∏ —Ç.–¥.)
            if len(word) <= 2 or word.isupper():
                normalized_words.append(word)
                continue

            # –î–ª—è –ø–µ—Ä—Å–æ–Ω –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∫–∞–∂–¥–æ–µ —Å–ª–æ–≤–æ (—Ñ–∞–º–∏–ª–∏—è, –∏–º—è)
            if entity_type == 'person':
                parsed = self.morph.parse(word)
                if parsed:
                    # –ë–µ—Ä–µ–º –∏–º–µ–Ω–∏—Ç–µ–ª—å–Ω—ã–π –ø–∞–¥–µ–∂
                    normal_form = parsed[0].normal_form
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∑–∞–≥–ª–∞–≤–Ω—É—é –±—É–∫–≤—É
                    if word[0].isupper():
                        normal_form = normal_form.capitalize()
                    normalized_words.append(normal_form)
                else:
                    normalized_words.append(word)
            else:
                # –î–ª—è –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π –∏ –ª–æ–∫–∞—Ü–∏–π –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º
                parsed = self.morph.parse(word)
                if parsed:
                    normal_form = parsed[0].normal_form
                    if word[0].isupper():
                        normal_form = normal_form.capitalize()
                    normalized_words.append(normal_form)
                else:
                    normalized_words.append(word)

        return ' '.join(normalized_words)

    def extract_entities(self, text: str) -> List[Dict]:
        """
        –ò–∑–≤–ª–µ—á—å –≤—Å–µ NER-—Å—É—â–Ω–æ—Å—Ç–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞

        Args:
            text: —Ç–µ–∫—Å—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞

        Returns:
            –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å —Å—É—â–Ω–æ—Å—Ç—è–º–∏: [{'text': '–°–±–µ—Ä–±–∞–Ω–∫', 'type': 'organization'}, ...]
        """
        if not text or len(text.strip()) < 3:
            return []

        try:
            # –°–æ–∑–¥–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç –∏ –∏–∑–≤–ª–µ–∫–∞–µ–º —Å—É—â–Ω–æ—Å—Ç–∏
            doc = Doc(text)
            doc.segment(self.segmenter)
            doc.tag_morph(self.morph_tagger)
            doc.tag_ner(self.ner_tagger)

            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —Å—É—â–Ω–æ—Å—Ç–∏
            entities = []
            for span in doc.spans:
                entity_type = self.entity_type_map.get(span.type, span.type.lower())
                normalized = self.normalize_entity(span.text, entity_type)
                entities.append({
                    'text': span.text,
                    'normalized': normalized,
                    'type': entity_type,
                    'start': span.start,
                    'stop': span.stop
                })

            return entities

        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ NER extraction: {e}")
            return []

    def extract_from_news(self, title: str, description: str = "") -> Dict:
        """
        –ò–∑–≤–ª–µ—á—å —Å—É—â–Ω–æ—Å—Ç–∏ –∏–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞ –∏ –æ–ø–∏—Å–∞–Ω–∏—è –Ω–æ–≤–æ—Å—Ç–∏

        Args:
            title: –∑–∞–≥–æ–ª–æ–≤–æ–∫ –Ω–æ–≤–æ—Å—Ç–∏
            description: –æ–ø–∏—Å–∞–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)

        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å —Å—É—â–Ω–æ—Å—Ç—è–º–∏ –ø–æ —Ç–∏–ø–∞–º –∏ –ø–æ–ª–Ω—ã–π —Å–ø–∏—Å–æ–∫
        """
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞ (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω–µ–µ)
        title_entities = self.extract_entities(title)

        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–∑ –æ–ø–∏—Å–∞–Ω–∏—è
        desc_entities = self.extract_entities(description) if description else []

        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –∏ –¥–µ–¥—É–ø–ª–∏—Ü–∏—Ä—É–µ–º –ø–æ —Ç–µ–∫—Å—Ç—É + —Ç–∏–ø—É
        all_entities = title_entities + desc_entities
        unique_entities = {}

        for entity in all_entities:
            key = (entity['text'].lower(), entity['type'])
            if key not in unique_entities:
                unique_entities[key] = entity

        entities_list = list(unique_entities.values())

        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ —Ç–∏–ø–∞–º
        by_type = {
            'persons': [],
            'locations': [],
            'organizations': []
        }

        for entity in entities_list:
            if entity['type'] == 'person':
                by_type['persons'].append(entity['text'])
            elif entity['type'] == 'location':
                by_type['locations'].append(entity['text'])
            elif entity['type'] == 'organization':
                by_type['organizations'].append(entity['text'])

        return {
            'all': entities_list,
            'by_type': by_type,
            'count': len(entities_list)
        }

    def extract_key_entities(self, title: str, description: str = "") -> Set[str]:
        """
        –ò–∑–≤–ª–µ—á—å —É–Ω–∏–∫–∞–ª—å–Ω—ã–π —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π (–¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏)

        Returns:
            Set —Å —Ç–µ–∫—Å—Ç–∞–º–∏ –≤—Å–µ—Ö –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π
        """
        result = self.extract_from_news(title, description)
        return {entity['text'] for entity in result['all']}

    def is_banking_entity(self, entity_text: str) -> bool:
        """
        –ü—Ä–æ–≤–µ—Ä–∏—Ç—å, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Å—É—â–Ω–æ—Å—Ç—å –±–∞–Ω–∫–æ–≤—Å–∫–æ–π/—Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–π

        Args:
            entity_text: —Ç–µ–∫—Å—Ç —Å—É—â–Ω–æ—Å—Ç–∏

        Returns:
            True –µ—Å–ª–∏ —ç—Ç–æ –±–∞–Ω–∫ –∏–ª–∏ —Ñ–∏–Ω–∞–Ω—Å–æ–≤–∞—è –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è
        """
        banking_keywords = {
            '–±–∞–Ω–∫', 'bank', '—Å–±–µ—Ä', '–≤—Ç–±', '–∞–ª—å—Ñ–∞', '—Ç–∏–Ω—å–∫–æ—Ñ—Ñ',
            '–≥–∞–∑–ø—Ä–æ–º–±–∞–Ω–∫', '—Ä–æ—Å—Å–µ–ª—å—Ö–æ–∑–±–∞–Ω–∫', '—É—Ä–∞–ª—Å–∏–±', '–æ—Ç–∫—Ä—ã—Ç–∏–µ',
            '—Ü–±', '—Ü–µ–Ω—Ç—Ä–æ–±–∞–Ω–∫', '—Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω—ã–π –±–∞–Ω–∫', '—Ñ—Ä—Å', 'ecb'
        }

        entity_lower = entity_text.lower()
        return any(keyword in entity_lower for keyword in banking_keywords)


def main():
    """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ NER-–º–æ–¥—É–ª—è"""
    print("="*70)
    print("üîç –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ NER-–º–æ–¥—É–ª—è –¥–ª—è –Ω–æ–≤–æ—Å—Ç–µ–π")
    print("="*70)
    print()

    extractor = NewsNERExtractor()

    # –¢–µ—Å—Ç–æ–≤—ã–µ –Ω–æ–≤–æ—Å—Ç–∏
    test_cases = [
        {
            'title': '–°–±–µ—Ä–±–∞–Ω–∫ –ø–æ–≤—ã—Å–∏–ª —Å—Ç–∞–≤–∫–∏ –ø–æ –≤–∫–ª–∞–¥–∞–º –≤ —Ä—É–±–ª—è—Ö',
            'description': '–ö—Ä—É–ø–Ω–µ–π—à–∏–π –±–∞–Ω–∫ –†–æ—Å—Å–∏–∏ –°–±–µ—Ä–±–∞–Ω–∫ –æ–±—ä—è–≤–∏–ª –æ –ø–æ–≤—ã—à–µ–Ω–∏–∏ —Å—Ç–∞–≤–æ–∫ –ø–æ –≤–∫–ª–∞–¥–∞–º'
        },
        {
            'title': '–¶–ë –†–§ –æ—Å—Ç–∞–≤–∏–ª –∫–ª—é—á–µ–≤—É—é —Å—Ç–∞–≤–∫—É –Ω–∞ —É—Ä–æ–≤–Ω–µ 16%',
            'description': '–¶–µ–Ω—Ç—Ä–∞–ª—å–Ω—ã–π –±–∞–Ω–∫ –†–æ—Å—Å–∏–∏ –ø—Ä–∏–Ω—è–ª —Ä–µ—à–µ–Ω–∏–µ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –∫–ª—é—á–µ–≤—É—é —Å—Ç–∞–≤–∫—É –Ω–∞ –∑–∞—Å–µ–¥–∞–Ω–∏–∏ –≤ –ú–æ—Å–∫–≤–µ'
        },
        {
            'title': '–í–ª–∞–¥–∏–º–∏—Ä –ü—É—Ç–∏–Ω –≤—Å—Ç—Ä–µ—Ç–∏–ª—Å—è —Å –≥–ª–∞–≤–æ–π –í–¢–ë –ê–Ω–¥—Ä–µ–µ–º –ö–æ—Å—Ç–∏–Ω—ã–º',
            'description': '–ü—Ä–µ–∑–∏–¥–µ–Ω—Ç –†–æ—Å—Å–∏–∏ –æ–±—Å—É–¥–∏–ª —Å –≥–ª–∞–≤–æ–π –±–∞–Ω–∫–∞ –í–¢–ë –≤–æ–ø—Ä–æ—Å—ã —Ä–∞–∑–≤–∏—Ç–∏—è —Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–≥–æ —Å–µ–∫—Ç–æ—Ä–∞'
        }
    ]

    for i, test in enumerate(test_cases, 1):
        print(f"–¢–µ—Å—Ç {i}: {test['title']}")
        print("-" * 70)

        result = extractor.extract_from_news(test['title'], test['description'])

        print(f"–í—Å–µ–≥–æ —Å—É—â–Ω–æ—Å—Ç–µ–π: {result['count']}")

        if result['by_type']['persons']:
            print(f"  –ü–µ—Ä—Å–æ–Ω—ã: {', '.join(result['by_type']['persons'])}")

        if result['by_type']['locations']:
            print(f"  –õ–æ–∫–∞—Ü–∏–∏: {', '.join(result['by_type']['locations'])}")

        if result['by_type']['organizations']:
            print(f"  –û—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏: {', '.join(result['by_type']['organizations'])}")

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –±–∞–Ω–∫–æ–≤—Å–∫–∏–µ —Å—É—â–Ω–æ—Å—Ç–∏
        banking_entities = [e['text'] for e in result['all'] if extractor.is_banking_entity(e['text'])]
        if banking_entities:
            print(f"  üè¶ –ë–∞–Ω–∫–æ–≤—Å–∫–∏–µ: {', '.join(banking_entities)}")

        print()


if __name__ == "__main__":
    main()
