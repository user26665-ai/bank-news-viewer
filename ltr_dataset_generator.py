#!/usr/bin/env python3
"""
–ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –¥–∞—Ç–∞—Å–µ—Ç–∞ –¥–ª—è Learning to Rank
–°–æ–∑–¥–∞–µ—Ç –ø–∞—Ä—ã (–∑–∞–ø—Ä–æ—Å, –Ω–æ–≤–æ—Å—Ç—å) —Å —Ñ–∏—á–∞–º–∏ –¥–ª—è —Ä—É—á–Ω–æ–π —Ä–∞–∑–º–µ—Ç–∫–∏
"""

import sqlite3
import json
import numpy as np
from news_rag_system import NewsRAGSystem
from typing import List, Dict
import re

DB_PATH = "/Users/david/bank_news_agent/news_database.db"

# –¢–∏–ø–∏—á–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã –¥–ª—è –±–∞–Ω–∫–æ–≤—Å–∫–æ–π —Ç–µ–º–∞—Ç–∏–∫–∏
SAMPLE_QUERIES = [
    # –ë–∞–Ω–∫–∏
    "–°–±–µ—Ä–±–∞–Ω–∫",
    "–í–¢–ë",
    "–ê–ª—å—Ñ–∞-–ë–∞–Ω–∫",
    "–¢–∏–Ω—å–∫–æ—Ñ—Ñ",
    "–ì–∞–∑–ø—Ä–æ–º–±–∞–Ω–∫",

    # –†–µ–≥—É–ª—è—Ç–æ—Ä—ã
    "–¶–µ–Ω—Ç—Ä–∞–ª—å–Ω—ã–π –±–∞–Ω–∫",
    "–¶–ë –†–§",
    "–ë–∞–Ω–∫ –†–æ—Å—Å–∏–∏",

    # –ü—Ä–æ–¥—É–∫—Ç—ã
    "–∏–ø–æ—Ç–µ–∫–∞",
    "–∫—Ä–µ–¥–∏—Ç",
    "–≤–∫–ª–∞–¥—ã",
    "–∫–ª—é—á–µ–≤–∞—è —Å—Ç–∞–≤–∫–∞",

    # –°–æ–±—ã—Ç–∏—è
    "—Å–∞–Ω–∫—Ü–∏–∏",
    "–∫—É—Ä—Å –¥–æ–ª–ª–∞—Ä–∞",
    "–∫—É—Ä—Å –µ–≤—Ä–æ",
    "–∏–Ω—Ñ–ª—è—Ü–∏—è",

    # –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ
    "–°–±–µ—Ä–±–∞–Ω–∫ –∏–ø–æ—Ç–µ–∫–∞",
    "–¶–ë –ø–æ–≤—ã—Å–∏–ª —Å—Ç–∞–≤–∫—É",
    "–í–¢–ë –∫—Ä–µ–¥–∏—Ç—ã",
    "—Å–∞–Ω–∫—Ü–∏–∏ –ø—Ä–æ—Ç–∏–≤ –±–∞–Ω–∫–æ–≤",
    "–¥–æ–ª–ª–∞—Ä —Ä–∞—Å—Ç–µ—Ç",
]


class LTRDatasetGenerator:
    def __init__(self):
        self.rag = NewsRAGSystem()
        self.conn = sqlite3.connect(DB_PATH)
        self.cursor = self.conn.cursor()

    def calculate_features(self, query: str, news: Dict) -> Dict:
        """–í—ã—á–∏—Å–ª—è–µ—Ç –≤—Å–µ —Ñ–∏—á–∏ –¥–ª—è –ø–∞—Ä—ã (–∑–∞–ø—Ä–æ—Å, –Ω–æ–≤–æ—Å—Ç—å)"""

        features = {}

        # 1. Embedding similarity (—É–∂–µ –µ—Å—Ç—å)
        features['embedding_score'] = news.get('similarity', 0.0)

        # 2. BM25 score (–∞–ø–ø—Ä–æ–∫—Å–∏–º–∞—Ü–∏—è —á–µ—Ä–µ–∑ TF-IDF)
        features['bm25_score'] = self._calculate_bm25_approx(query, news)

        # 3. NER overlap
        features['ner_overlap'] = self._calculate_ner_overlap(query, news['id'])

        # 4. Morphological match
        features['morpho_match'] = self._calculate_morpho_match(query, news)

        # 5. Title match
        features['title_match'] = self._calculate_title_match(query, news['title'])

        # 6. Exact match
        features['exact_match'] = 1.0 if query.lower() in news['title'].lower() else 0.0

        # 7. Date recency (–¥–Ω–µ–π –Ω–∞–∑–∞–¥)
        features['days_ago'] = self._calculate_days_ago(news.get('published', ''))

        # 8. Source authority (–ø–æ–∫–∞ –ø—Ä–æ—Å—Ç–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞)
        features['source_authority'] = self._get_source_authority(news.get('source', ''))

        # 9. Text length
        title_len = len(news.get('title', ''))
        desc_len = len(news.get('description', ''))
        features['text_length'] = title_len + desc_len

        return features

    def _calculate_bm25_approx(self, query: str, news: Dict) -> float:
        """–£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –∞–ø–ø—Ä–æ–∫—Å–∏–º–∞—Ü–∏—è BM25 —á–µ—Ä–µ–∑ –ø–æ–¥—Å—á–µ—Ç —Å–æ–≤–ø–∞–¥–∞—é—â–∏—Ö —Å–ª–æ–≤"""
        query_words = set(query.lower().split())
        text = f"{news.get('title', '')} {news.get('description', '')}".lower()

        matches = sum(1 for word in query_words if word in text)
        return matches / len(query_words) if query_words else 0.0

    def _calculate_ner_overlap(self, query: str, news_id: int) -> float:
        """–ü—Ä–æ—Ü–µ–Ω—Ç —Å–æ–≤–ø–∞–¥–∞—é—â–∏—Ö NER-—Å—É—â–Ω–æ—Å—Ç–µ–π"""
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å—É—â–Ω–æ—Å—Ç–∏ –∏–∑ –∑–∞–ø—Ä–æ—Å–∞
        query_entities = self.rag.ner_extractor.extract_from_news(query, "")
        query_ner_set = set(e['normalized'].lower() for e in query_entities['all'])

        if not query_ner_set:
            return 0.0

        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å—É—â–Ω–æ—Å—Ç–∏ –∏–∑ –Ω–æ–≤–æ—Å—Ç–∏
        self.cursor.execute('''
            SELECT normalized_text FROM entities WHERE news_id = ?
        ''', (news_id,))

        news_ner_set = set(row[0].lower() for row in self.cursor.fetchall() if row[0])

        if not news_ner_set:
            return 0.0

        # Jaccard similarity
        intersection = query_ner_set & news_ner_set
        union = query_ner_set | news_ner_set

        return len(intersection) / len(union) if union else 0.0

    def _calculate_morpho_match(self, query: str, news: Dict) -> float:
        """–ú–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ"""
        import pymorphy2
        morph = pymorphy2.MorphAnalyzer()

        query_words = query.lower().split()
        text = f"{news.get('title', '')} {news.get('description', '')}".lower()

        matches = 0
        for word in query_words:
            parsed = morph.parse(word)[0]
            normal_form = parsed.normal_form

            if normal_form in text or word in text:
                matches += 1

        return matches / len(query_words) if query_words else 0.0

    def _calculate_title_match(self, query: str, title: str) -> float:
        """–°–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Å –∑–∞–≥–æ–ª–æ–≤–∫–æ–º"""
        query_words = set(query.lower().split())
        title_words = set(title.lower().split())

        if not query_words:
            return 0.0

        matches = query_words & title_words
        return len(matches) / len(query_words)

    def _calculate_days_ago(self, published: str) -> float:
        """–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–Ω–µ–π –Ω–∞–∑–∞–¥ (—á–µ–º –º–µ–Ω—å—à–µ, —Ç–µ–º —Å–≤–µ–∂–µ–µ)"""
        if not published:
            return 999.0  # –æ—á–µ–Ω—å —Å—Ç–∞—Ä–∞—è

        try:
            from datetime import datetime
            from email.utils import parsedate_to_datetime

            if published.startswith(('Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun')):
                pub_date = parsedate_to_datetime(published)
            else:
                pub_date = datetime.fromisoformat(published.replace('Z', '+00:00'))

            now = datetime.now(pub_date.tzinfo)
            delta = (now - pub_date).days
            return max(0, delta)
        except:
            return 999.0

    def _get_source_authority(self, source: str) -> float:
        """–û—Ü–µ–Ω–∫–∞ –∞–≤—Ç–æ—Ä–∏—Ç–µ—Ç–Ω–æ—Å—Ç–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∞"""
        high_authority = ['–†–ë–ö', '–ö–æ–º–º–µ—Ä—Å–∞–Ω—Ç—ä', '–í–µ–¥–æ–º–æ—Å—Ç–∏', '–ò–Ω—Ç–µ—Ä—Ñ–∞–∫—Å', '–¢–ê–°–°']
        medium_authority = ['–ò–∑–≤–µ—Å—Ç–∏—è', '–†–æ—Å—Å–∏–π—Å–∫–∞—è –≥–∞–∑–µ—Ç–∞', 'Banki.ru']

        for auth_source in high_authority:
            if auth_source.lower() in source.lower():
                return 1.0

        for auth_source in medium_authority:
            if auth_source.lower() in source.lower():
                return 0.5

        return 0.0

    def generate_candidates(self, query: str, top_k: int = 20) -> List[Dict]:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ç–æ–ø-K –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞"""
        print(f"\nüîç –ó–∞–ø—Ä–æ—Å: '{query}'")

        # –ü–æ–ª—É—á–∞–µ–º —Ç–æ–ø-K –∏–∑ —Ç–µ–∫—É—â–µ–π —Å–∏—Å—Ç–µ–º—ã
        results = self.rag.search_similar(query, top_k=top_k)

        candidates = []
        for rank, news in enumerate(results, 1):
            features = self.calculate_features(query, news)

            candidate = {
                'query': query,
                'rank': rank,
                'news_id': news['id'],
                'title': news['title'],
                'description': news.get('description', ''),
                'source': news.get('source', ''),
                'published': news.get('published', ''),
                'link': news.get('link', ''),
                'features': features,
                'label': None  # –ó–∞–ø–æ–ª–Ω–∏—Ç—Å—è –ø—Ä–∏ —Ä–∞–∑–º–µ—Ç–∫–µ
            }

            candidates.append(candidate)

        print(f"  ‚úì –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ {len(candidates)} –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤")
        return candidates

    def generate_dataset(self, queries: List[str] = None, output_file: str = "ltr_dataset.json"):
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –ø–æ–ª–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è —Ä–∞–∑–º–µ—Ç–∫–∏"""
        if queries is None:
            queries = SAMPLE_QUERIES

        print("=" * 70)
        print("üéØ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞ –¥–ª—è Learning to Rank")
        print("=" * 70)

        dataset = []

        for query in queries:
            candidates = self.generate_candidates(query, top_k=20)
            dataset.extend(candidates)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(dataset, f, ensure_ascii=False, indent=2)

        print(f"\n‚úÖ –î–∞—Ç–∞—Å–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {output_file}")
        print(f"   –í—Å–µ–≥–æ –ø–∞—Ä (–∑–∞–ø—Ä–æ—Å, –Ω–æ–≤–æ—Å—Ç—å): {len(dataset)}")
        print(f"   –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤: {len(queries)}")
        print(f"\nüí° –°–ª–µ–¥—É—é—â–∏–π —à–∞–≥: –æ—Ç–∫—Ä–æ–π—Ç–µ ltr_annotator.html –¥–ª—è —Ä–∞–∑–º–µ—Ç–∫–∏")

        self.conn.close()
        return dataset


if __name__ == "__main__":
    generator = LTRDatasetGenerator()
    dataset = generator.generate_dataset()
