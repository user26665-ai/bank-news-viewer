#!/usr/bin/env python3
"""
–°–∫–ª–µ–π–∫–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö LTR –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ –≤ –æ–¥–∏–Ω

–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
    python3 merge_ltr_datasets.py dataset1.json dataset2.json dataset3.json -o merged_dataset.json
"""

import json
import argparse
from pathlib import Path
from typing import List, Dict


def merge_datasets(input_files: List[str], output_file: str, remove_duplicates: bool = True):
    """
    –°–∫–ª–µ–∏–≤–∞–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ JSON –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ –≤ –æ–¥–∏–Ω

    Args:
        input_files: –°–ø–∏—Å–æ–∫ –ø—É—Ç–µ–π –∫ –≤—Ö–æ–¥–Ω—ã–º JSON —Ñ–∞–π–ª–∞–º
        output_file: –ü—É—Ç—å –∫ –≤—ã—Ö–æ–¥–Ω–æ–º—É JSON —Ñ–∞–π–ª—É
        remove_duplicates: –£–¥–∞–ª—è—Ç—å –ª–∏ –¥—É–±–ª–∏–∫–∞—Ç—ã (–ø–æ query + news_id)
    """

    all_data = []
    seen_pairs = set()  # (query, news_id) –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤

    print("=" * 70)
    print("üîó –°–∫–ª–µ–π–∫–∞ LTR –¥–∞—Ç–∞—Å–µ—Ç–æ–≤")
    print("=" * 70)
    print()

    # –ß–∏—Ç–∞–µ–º –≤—Å–µ —Ñ–∞–π–ª—ã
    for i, file_path in enumerate(input_files, 1):
        print(f"{i}. –ó–∞–≥—Ä—É–∂–∞—é {file_path}...", end=" ")

        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)

            if not isinstance(data, list):
                print(f"‚ùå –ù–µ–≤–µ—Ä–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç (–æ–∂–∏–¥–∞–ª—Å—è —Å–ø–∏—Å–æ–∫)")
                continue

            # –§–∏–ª—å—Ç—Ä—É–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
            added = 0
            for item in data:
                pair_key = (item.get('query', ''), item.get('news_id', -1))

                if remove_duplicates:
                    if pair_key not in seen_pairs:
                        all_data.append(item)
                        seen_pairs.add(pair_key)
                        added += 1
                else:
                    all_data.append(item)
                    added += 1

            print(f"‚úì –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(data)} –ø—Ä–∏–º–µ—Ä–æ–≤, –¥–æ–±–∞–≤–ª–µ–Ω–æ {added}")

        except FileNotFoundError:
            print(f"‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω")
        except json.JSONDecodeError:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON")
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞: {e}")

    print()
    print("=" * 70)
    print(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
    print(f"   –í—Å–µ–≥–æ –ø—Ä–∏–º–µ—Ä–æ–≤: {len(all_data)}")

    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ label
    labeled = [item for item in all_data if item.get('label') is not None]
    print(f"   –†–∞–∑–º–µ—á–µ–Ω–æ: {len(labeled)}")

    if labeled:
        label_counts = {}
        for item in labeled:
            label = item.get('label', -1)
            label_counts[label] = label_counts.get(label, 0) + 1

        print(f"   –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ:")
        for label in sorted(label_counts.keys()):
            emoji = {0: "‚ùå", 1: "ü§î", 2: "‚úÖ", 3: "‚≠ê"}.get(label, "?")
            count = label_counts[label]
            bar = "‚ñà" * int(count / len(labeled) * 30)
            print(f"      {emoji} {label}: {count:4d} ({count/len(labeled)*100:.1f}%) {bar}")

    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∑–∞–ø—Ä–æ—Å–∞–º
    unique_queries = len(set(item.get('query', '') for item in all_data))
    print(f"   –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤: {unique_queries}")

    print("=" * 70)
    print()

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
    print(f"üíæ –°–æ—Ö—Ä–∞–Ω—è—é –≤ {output_file}...", end=" ")
    try:
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(all_data, f, ensure_ascii=False, indent=2)
        print("‚úì –ì–æ—Ç–æ–≤–æ!")
        print()
        print(f"‚úÖ –£—Å–ø–µ—à–Ω–æ —Å–∫–ª–µ–µ–Ω–æ {len(input_files)} –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ ‚Üí {len(all_data)} –ø—Ä–∏–º–µ—Ä–æ–≤")
        return True
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
        return False


def main():
    parser = argparse.ArgumentParser(
        description="–°–∫–ª–µ–∏–≤–∞–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ LTR –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ –≤ –æ–¥–∏–Ω",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
–ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:

  # –°–∫–ª–µ–∏—Ç—å 2 —Ñ–∞–π–ª–∞
  python3 merge_ltr_datasets.py day1.json day2.json -o merged.json

  # –°–∫–ª–µ–∏—Ç—å –≤—Å–µ JSON —Ñ–∞–π–ª—ã –≤ –ø–∞–ø–∫–µ
  python3 merge_ltr_datasets.py datasets/*.json -o all_data.json

  # –°–∫–ª–µ–∏—Ç—å —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
  python3 merge_ltr_datasets.py file1.json file2.json -o merged.json --keep-duplicates
        """
    )

    parser.add_argument('inputs', nargs='+', help='–í—Ö–æ–¥–Ω—ã–µ JSON —Ñ–∞–π–ª—ã —Å –¥–∞—Ç–∞—Å–µ—Ç–∞–º–∏')
    parser.add_argument('-o', '--output', required=True, help='–í—ã—Ö–æ–¥–Ω–æ–π JSON —Ñ–∞–π–ª')
    parser.add_argument('--keep-duplicates', action='store_true',
                        help='–ù–µ —É–¥–∞–ª—è—Ç—å –¥—É–±–ª–∏–∫–∞—Ç—ã (–ø–æ query + news_id)')

    args = parser.parse_args()

    success = merge_datasets(
        input_files=args.inputs,
        output_file=args.output,
        remove_duplicates=not args.keep_duplicates
    )

    exit(0 if success else 1)


if __name__ == "__main__":
    main()
