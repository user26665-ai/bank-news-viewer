#!/usr/bin/env python3
"""
–ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–∞—è RAG-—Å–∏—Å—Ç–µ–º–∞ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –Ω–æ–≤–æ—Å—Ç—è–º–∏
–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –º–Ω–æ–∂–µ—Å—Ç–≤–æ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤, –≤–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫ –∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫—É—é –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—é
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç –ª–µ–≥–∫–æ–≤–µ—Å–Ω—ã–µ RSS-—Ñ–∏–¥—ã –±–µ–∑ web scraping
"""

import sqlite3
import json
import feedparser
import requests
import hashlib
from datetime import datetime, timedelta
import numpy as np
from typing import List, Dict, Optional
import time
import asyncio
import httpx
import re
from html import unescape
from news_ner import NewsNERExtractor
from sentence_transformers import SentenceTransformer

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏
DB_PATH = "/Users/david/bank_news_agent/news_database.db"
SOURCES_PATH = "/Users/david/bank_news_agent/news_sources.json"
LM_STUDIO_API = "http://localhost:1234/v1"
EMBEDDING_MODEL = "BAAI/bge-m3"  # –ò–∑–º–µ–Ω–µ–Ω–æ –Ω–∞ BGE-M3

class NewsRAGSystem:
    def __init__(self):
        self.db_path = DB_PATH
        self.ner_extractor = NewsNERExtractor()

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è BGE-M3 –º–æ–¥–µ–ª–∏
        print("–ó–∞–≥—Ä—É–∑–∫–∞ BGE-M3 –º–æ–¥–µ–ª–∏...")
        self.embedding_model = SentenceTransformer(EMBEDDING_MODEL)
        print("‚úì BGE-M3 –∑–∞–≥—Ä—É–∂–µ–Ω–∞")

        self.init_database()

    def init_database(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()

        # –¢–∞–±–ª–∏—Ü–∞ –Ω–æ–≤–æ—Å—Ç–µ–π
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS news (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                hash TEXT UNIQUE,
                source TEXT,
                category TEXT,
                title TEXT,
                description TEXT,
                link TEXT,
                published TEXT,
                fetched_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                embedding BLOB
            )
        ''')

        # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–ª–µ full_text –µ—Å–ª–∏ –µ–≥–æ –Ω–µ—Ç
        cursor.execute("PRAGMA table_info(news)")
        columns = [col[1] for col in cursor.fetchall()]

        if 'full_text' not in columns:
            cursor.execute('ALTER TABLE news ADD COLUMN full_text TEXT')

        if 'content_hash' not in columns:
            cursor.execute('ALTER TABLE news ADD COLUMN content_hash TEXT')
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_content_hash ON news(content_hash)')

        # –ò–Ω–¥–µ–∫—Å –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_source ON news(source)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_category ON news(category)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_published ON news(published)')

        # –¢–∞–±–ª–∏—Ü–∞ NER-—Å—É—â–Ω–æ—Å—Ç–µ–π
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS entities (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                news_id INTEGER,
                entity_text TEXT,
                entity_type TEXT,
                position INTEGER,
                is_banking BOOLEAN DEFAULT 0,
                FOREIGN KEY (news_id) REFERENCES news(id) ON DELETE CASCADE
            )
        ''')

        # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–ª–µ normalized_text –µ—Å–ª–∏ –µ–≥–æ –Ω–µ—Ç
        cursor.execute("PRAGMA table_info(entities)")
        entity_columns = [col[1] for col in cursor.fetchall()]

        if 'normalized_text' not in entity_columns:
            cursor.execute('ALTER TABLE entities ADD COLUMN normalized_text TEXT')
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_normalized_text ON entities(normalized_text)')

        # –ò–Ω–¥–µ–∫—Å—ã –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞ –ø–æ —Å—É—â–Ω–æ—Å—Ç—è–º
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_entity_text ON entities(entity_text)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_entity_type ON entities(entity_type)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_news_id ON entities(news_id)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_is_banking ON entities(is_banking)')

        conn.commit()
        conn.close()

    def load_sources(self) -> List[Dict]:
        """–ó–∞–≥—Ä—É–∑–∏—Ç—å —Å–ø–∏—Å–æ–∫ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤"""
        try:
            with open(SOURCES_PATH, 'r', encoding='utf-8') as f:
                data = json.load(f)
                return [s for s in data['sources'] if s.get('enabled', True)]
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤: {e}")
            return []

    def generate_hash(self, title: str, link: str) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ö–µ—à–∞ –¥–ª—è –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏"""
        content = f"{title}:{link}"
        return hashlib.md5(content.encode()).hexdigest()

    def clean_html(self, text: str) -> str:
        """–û—á–∏—Å—Ç–∫–∞ HTML —Ç–µ–≥–æ–≤ –∏ –ª–∏—à–Ω–∏—Ö –ø—Ä–æ–±–µ–ª–æ–≤ –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        if not text:
            return ""

        # –£–¥–∞–ª—è–µ–º HTML —Ç–µ–≥–∏
        text = re.sub(r'<[^>]+>', ' ', text)

        # –î–µ–∫–æ–¥–∏—Ä—É–µ–º HTML entities
        text = unescape(text)

        # –£–¥–∞–ª—è–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã –∏ –ø–µ—Ä–µ–Ω–æ—Å—ã
        text = re.sub(r'\s+', ' ', text)

        # –£–±–∏—Ä–∞–µ–º –ø—Ä–æ–±–µ–ª—ã –≤ –Ω–∞—á–∞–ª–µ –∏ –∫–æ–Ω—Ü–µ
        text = text.strip()

        return text

    def extract_rss_content(self, entry) -> dict:
        """
        –£–ª—É—á—à–µ–Ω–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –∏–∑ RSS entry
        –ü—ã—Ç–∞–µ—Ç—Å—è –ø–æ–ª—É—á–∏—Ç—å –º–∞–∫—Å–∏–º—É–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏–∑ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –ø–æ–ª–µ–π RSS
        """
        # –ó–∞–≥–æ–ª–æ–≤–æ–∫
        title = entry.get('title', '–ë–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞')
        title = self.clean_html(title)

        # –ò–∑–≤–ª–µ–∫–∞–µ–º –†–ï–ê–õ–¨–ù–´–ô –∏—Å—Ç–æ—á–Ω–∏–∫ (–°–ú–ò) –∏–∑ RSS
        real_source = None

        # –°–ø–æ—Å–æ–± 1: –ü–æ–ª–µ 'source' –≤ RSS (–ø—Ä–µ–¥–ø–æ—á—Ç–∏—Ç–µ–ª—å–Ω–æ)
        if 'source' in entry and entry['source'] and 'title' in entry['source']:
            real_source = entry['source']['title']

        # –°–ø–æ—Å–æ–± 2: –ò–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞ –Ω–æ–≤–æ—Å—Ç–∏ (–ø–æ—Å–ª–µ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ " - ")
        if not real_source and ' - ' in title:
            parts = title.split(' - ')
            if len(parts) >= 2:
                # –ü–æ—Å–ª–µ–¥–Ω—è—è —á–∞—Å—Ç—å - —ç—Ç–æ –∏—Å—Ç–æ—á–Ω–∏–∫
                real_source = parts[-1].strip()
                # –£–±–∏—Ä–∞–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫ –∏–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞
                title = ' - '.join(parts[:-1]).strip()

        # –û–ø–∏—Å–∞–Ω–∏–µ - –ø—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ –ø–æ–ª—è
        description = ""
        if 'content' in entry and entry['content']:
            # content:encoded - –æ–±—ã—á–Ω–æ —Å–æ–¥–µ—Ä–∂–∏—Ç –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç
            description = entry['content'][0].get('value', '')
        elif 'summary' in entry:
            description = entry.get('summary', '')
        elif 'description' in entry:
            description = entry.get('description', '')

        description = self.clean_html(description)

        # –ï—Å–ª–∏ –æ–ø–∏—Å–∞–Ω–∏–µ —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–æ–µ, –ø–æ–ø—Ä–æ–±—É–µ–º summary_detail
        if len(description) < 50 and 'summary_detail' in entry:
            summary_detail = entry['summary_detail'].get('value', '')
            if len(summary_detail) > len(description):
                description = self.clean_html(summary_detail)

        # –°—Å—ã–ª–∫–∞
        link = entry.get('link', '')

        # –î–∞—Ç–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ - –ø—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã
        published = entry.get('published', '')
        if not published and 'updated' in entry:
            published = entry.get('updated', '')
        if not published and 'created' in entry:
            published = entry.get('created', '')

        # –ê–≤—Ç–æ—Ä
        author = entry.get('author', '')
        if not author and 'authors' in entry and entry['authors']:
            author = entry['authors'][0].get('name', '')

        # –ö–∞—Ç–µ–≥–æ—Ä–∏–∏/—Ç–µ–≥–∏
        tags = []
        if 'tags' in entry:
            tags = [tag.get('term', '') for tag in entry['tags']]

        return {
            'title': title,
            'description': description,
            'link': link,
            'published': published,
            'author': author,
            'tags': tags,
            'real_source': real_source  # –†–µ–∞–ª—å–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫ (–°–ú–ò)
        }

    def save_entities(self, news_id: int, title: str, description: str, conn=None):
        """
        –ò–∑–≤–ª–µ—á—å –∏ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å NER-—Å—É—â–Ω–æ—Å—Ç–∏ –¥–ª—è –Ω–æ–≤–æ—Å—Ç–∏

        Args:
            news_id: ID –Ω–æ–≤–æ—Å—Ç–∏ –≤ –ë–î
            title: –∑–∞–≥–æ–ª–æ–≤–æ–∫ –Ω–æ–≤–æ—Å—Ç–∏
            description: –æ–ø–∏—Å–∞–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–∏
            conn: —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ —Å –ë–î (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        """
        close_conn = False
        if conn is None:
            conn = sqlite3.connect(self.db_path)
            close_conn = True

        try:
            cursor = conn.cursor()

            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å—É—â–Ω–æ—Å—Ç–∏
            result = self.ner_extractor.extract_from_news(title, description)

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∂–¥—É—é —Å—É—â–Ω–æ—Å—Ç—å
            for idx, entity in enumerate(result['all']):
                is_banking = self.ner_extractor.is_banking_entity(entity['text'])
                normalized = entity.get('normalized', entity['text'])

                cursor.execute('''
                    INSERT INTO entities (news_id, entity_text, entity_type, position, is_banking, normalized_text)
                    VALUES (?, ?, ?, ?, ?, ?)
                ''', (news_id, entity['text'], entity['type'], idx, is_banking, normalized))

            conn.commit()

        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è entities: {e}")

        finally:
            if close_conn:
                conn.close()

    def get_embedding(self, text: str) -> np.ndarray:
        """–ü–æ–ª—É—á–∏—Ç—å embedding —á–µ—Ä–µ–∑ BGE-M3"""
        try:
            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É —Ç–µ–∫—Å—Ç–∞
            text = text[:8000]

            # –ò—Å–ø–æ–ª—å–∑—É–µ–º BGE-M3 –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞
            embedding = self.embedding_model.encode(text, normalize_embeddings=True)
            return np.array(embedding, dtype=np.float32)
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è embedding: {e}")
        return None


    async def get_embedding_async(self, text: str, timeout: int = 30) -> Optional[np.ndarray]:
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–µ –ø–æ–ª—É—á–µ–Ω–∏–µ embedding —á–µ—Ä–µ–∑ BGE-M3"""
        try:
            # –ó–∞–ø—É—Å–∫–∞–µ–º —Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –º–µ—Ç–æ–¥ –≤ executor –¥–ª—è –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ—Å—Ç–∏
            loop = asyncio.get_event_loop()
            embedding = await loop.run_in_executor(None, self.get_embedding, text[:8000])
            return embedding
        except Exception as e:
            pass
        return None

    def fetch_and_index_news(self, limit_per_source: int = 200):
        """–ó–∞–≥—Ä—É–∑–∏—Ç—å –∏ –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞—Ç—å –Ω–æ–≤–æ—Å—Ç–∏ –∏–∑ –≤—Å–µ—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤"""
        sources = self.load_sources()
        total_new = 0
        total_updated = 0

        print(f"üì° –ó–∞–≥—Ä—É–∑–∫–∞ –Ω–æ–≤–æ—Å—Ç–µ–π –∏–∑ {len(sources)} –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤...\n")

        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()

        for source in sources:
            print(f"  ‚Ä¢ {source['name']}...", end=" ")
            try:
                feed = feedparser.parse(source['url'], agent='Mozilla/5.0')
                new_count = 0

                for entry in feed.entries[:limit_per_source]:
                    # –£–ª—É—á—à–µ–Ω–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑ RSS
                    content = self.extract_rss_content(entry)

                    title = content['title']
                    description = content['description']
                    link = content['link']
                    published = content['published']

                    if not title or not link:
                        continue

                    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ö–µ—à –¥–ª—è –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏
                    content_hash = hashlib.md5(f"{title}{link}".encode()).hexdigest()

                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —É–∂–µ —ç—Ç–∞ –Ω–æ–≤–æ—Å—Ç—å
                    cursor.execute('SELECT id FROM news WHERE content_hash = ?', (content_hash,))
                    if cursor.fetchone():
                        continue

                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ—á–∏—â–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ RSS
                    full_text = description
                    # –ï—Å–ª–∏ –æ–ø–∏—Å–∞–Ω–∏–µ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–ª–∏–Ω–Ω–æ–µ, –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ –ø–æ–ª–Ω–æ—Å—Ç—å—é
                    embed_text = f"{title}\n\n{description}"

                    # –ü–æ–ª—É—á–∞–µ–º embedding
                    embedding = self.get_embedding(embed_text)
                    if embedding is None:
                        # –ï—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å embedding, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
                        continue

                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –±–∞–∑—É —Å –ø–æ–ª–Ω—ã–º —Ç–µ–∫—Å—Ç–æ–º
                    try:
                        cursor.execute('''
                            INSERT INTO news (
                                hash, source, category, title, description, link,
                                published, embedding, content_hash, full_text
                            )
                            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                        ''', (
                            content_hash,  # –∏—Å–ø–æ–ª—å–∑—É–µ–º content_hash –¥–ª—è –æ–±–æ–∏—Ö –ø–æ–ª–µ–π
                            source['name'],
                            source.get('category', 'general'),
                            title,
                            description,
                            link,
                            published,
                            embedding.tobytes(),
                            content_hash,
                            full_text
                        ))
                    except sqlite3.IntegrityError:
                        # –î—É–±–ª–∏–∫–∞—Ç - –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
                        continue
                    new_count += 1
                    total_new += 1

                    # –ù–µ–±–æ–ª—å—à–∞—è –ø–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
                    time.sleep(0.3)

                print(f"‚úì {new_count} –Ω–æ–≤—ã—Ö")
                time.sleep(0.5)  # –ù–µ–±–æ–ª—å—à–∞—è –ø–∞—É–∑–∞ –º–µ–∂–¥—É –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º–∏

            except Exception as e:
                print(f"‚úó –û—à–∏–±–∫–∞: {e}")

        conn.commit()
        conn.close()

        print(f"\n‚úÖ –í—Å–µ–≥–æ –¥–æ–±–∞–≤–ª–µ–Ω–æ: {total_new} –Ω–æ–≤—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π")
        return total_new

    async def fetch_and_index_news_async(self, limit_per_source: int = 200, max_concurrent: int = 5, max_age_days: int = 0):
        """
        –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –∏ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è –Ω–æ–≤–æ—Å—Ç–µ–π (–Ω–µ –±–ª–æ–∫–∏—Ä—É–µ—Ç FastAPI)

        Args:
            limit_per_source: –º–∞–∫—Å–∏–º—É–º –Ω–æ–≤–æ—Å—Ç–µ–π —Å –æ–¥–Ω–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞
            max_concurrent: –º–∞–∫—Å–∏–º—É–º –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –¥–ª—è embeddings
            max_age_days: –∑–∞–≥—Ä—É–∂–∞—Ç—å —Ç–æ–ª—å–∫–æ –Ω–æ–≤–æ—Å—Ç–∏ –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ N –¥–Ω–µ–π (0 = –≤—Å–µ –Ω–æ–≤–æ—Å—Ç–∏)
        """
        sources = self.load_sources()
        total_new = 0

        print(f"\n{'='*70}")
        print(f"üì° [ASYNC] –ó–∞–≥—Ä—É–∑–∫–∞ –Ω–æ–≤–æ—Å—Ç–µ–π –∏–∑ {len(sources)} –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤...")
        print(f"‚ö° –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤: {max_concurrent}")
        if max_age_days > 0:
            print(f"üìÖ –§–∏–ª—å—Ç—Ä: —Ç–æ–ª—å–∫–æ –Ω–æ–≤–æ—Å—Ç–∏ –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ {max_age_days} –¥–Ω—è(–¥–Ω–µ–π)")
        print(f"{'='*70}\n")

        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()

        async def process_single_article(entry, source_name, category):
            """–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–¥–Ω–æ–π —Å—Ç–∞—Ç—å–∏: —É–ª—É—á—à–µ–Ω–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ RSS + embedding"""
            # –£–ª—É—á—à–µ–Ω–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑ RSS
            content = self.extract_rss_content(entry)

            title = content['title']
            description = content['description']
            link = content['link']
            published = content['published']

            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –†–ï–ê–õ–¨–ù–´–ô –∏—Å—Ç–æ—á–Ω–∏–∫ (–°–ú–ò) –µ—Å–ª–∏ –µ—Å—Ç—å, –∏–Ω–∞—á–µ - –Ω–∞–∑–≤–∞–Ω–∏–µ –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞
            real_source = content.get('real_source')
            final_source = real_source if real_source else source_name

            if not title or not link:
                return None

            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ö–µ—à –¥–ª—è –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏
            content_hash = hashlib.md5(f"{title}{link}".encode()).hexdigest()

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
            cursor.execute('SELECT id FROM news WHERE content_hash = ?', (content_hash,))
            if cursor.fetchone():
                return None  # –£–∂–µ –µ—Å—Ç—å

            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ—á–∏—â–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ RSS
            full_text = description
            embed_text = f"{title}\n\n{description}"

            # –°–æ–∑–¥–∞–µ–º embedding –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ
            embedding = await self.get_embedding_async(embed_text, timeout=30)

            if embedding is None:
                return None

            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –≤—Å—Ç–∞–≤–∫–∏
            return (
                content_hash,  # hash
                final_source,  # source - –†–ï–ê–õ–¨–ù–´–ô –∏—Å—Ç–æ—á–Ω–∏–∫ (–°–ú–ò)!
                category,  # category
                title,
                description,
                link,
                published,
                embedding.tobytes(),
                content_hash,  # content_hash
                full_text  # full_text
            )

        for source in sources:
            source_name = source['name']
            source_url = source['url']
            category = source.get('category', 'general')

            print(f"  ‚Ä¢ {source_name}...", end=" ", flush=True)

            try:
                # –ó–∞–≥—Ä—É–∑–∫–∞ RSS (–≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º —Ç—Ä–µ–¥–µ, —á—Ç–æ–±—ã –Ω–µ –±–ª–æ–∫–∏—Ä–æ–≤–∞—Ç—å)
                loop = asyncio.get_event_loop()
                feed = await loop.run_in_executor(None, lambda: feedparser.parse(source_url, agent='Mozilla/5.0'))

                # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –¥–∞—Ç–µ (—Ç–æ–ª—å–∫–æ —Å–≤–µ–∂–∏–µ –Ω–æ–≤–æ—Å—Ç–∏ –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ N –¥–Ω–µ–π)
                if max_age_days > 0:
                    cutoff_time = datetime.now() - timedelta(days=max_age_days)
                    filtered_entries = []

                    for entry in feed.entries[:limit_per_source]:
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–∞—Ç—É –ø—É–±–ª–∏–∫–∞—Ü–∏–∏
                        if hasattr(entry, 'published_parsed') and entry.published_parsed:
                            try:
                                entry_time = datetime(*entry.published_parsed[:6])
                                if entry_time >= cutoff_time:
                                    filtered_entries.append(entry)
                            except:
                                # –ï—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å –¥–∞—Ç—É - –¥–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤–æ—Å—Ç—å
                                filtered_entries.append(entry)
                        else:
                            # –ï—Å–ª–∏ –Ω–µ—Ç –¥–∞—Ç—ã - –¥–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤–æ—Å—Ç—å (—á—Ç–æ–±—ã –Ω–µ –ø–æ—Ç–µ—Ä—è—Ç—å)
                            filtered_entries.append(entry)

                    entries = filtered_entries
                else:
                    entries = feed.entries[:limit_per_source]

                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å—Ç–∞—Ç—å–∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ (—Å–æ–∑–¥–∞–µ–º embeddings –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ)
                semaphore = asyncio.Semaphore(max_concurrent)

                async def process_with_semaphore(entry):
                    async with semaphore:
                        return await process_single_article(entry, source_name, category)

                # –ó–∞–ø—É—Å–∫–∞–µ–º –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É embeddings
                tasks = [process_with_semaphore(entry) for entry in entries]
                results = await asyncio.gather(*tasks, return_exceptions=True)

                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –ë–î
                new_count = 0
                for result in results:
                    if result and not isinstance(result, Exception):
                        try:
                            cursor.execute('''
                                INSERT INTO news (
                                    hash, source, category, title, description, link,
                                    published, embedding, content_hash, full_text
                                )
                                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                            ''', result)

                            # –ü–æ–ª—É—á–∞–µ–º ID –Ω–æ–≤–æ–π –∑–∞–ø–∏—Å–∏
                            news_id = cursor.lastrowid

                            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º NER-—Å—É—â–Ω–æ—Å—Ç–∏
                            # result[3] = title, result[4] = description
                            self.save_entities(news_id, result[3], result[4], conn)

                            new_count += 1
                            total_new += 1
                        except sqlite3.IntegrityError:
                            # –î—É–±–ª–∏–∫–∞—Ç - –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
                            pass

                conn.commit()
                print(f"‚úì {new_count} –Ω–æ–≤—ã—Ö")

            except Exception as e:
                print(f"‚úó –û—à–∏–±–∫–∞: {e}")

        conn.close()

        print(f"\n‚úÖ [ASYNC] –í—Å–µ–≥–æ –¥–æ–±–∞–≤–ª–µ–Ω–æ: {total_new} –Ω–æ–≤—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π")
        return total_new

    def search_similar(self, query: str, top_k: int = 10, category: str = None) -> List[Dict]:
        """–ü–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞"""
        # –ü–æ–ª—É—á–∞–µ–º embedding –∑–∞–ø—Ä–æ—Å–∞
        query_embedding = self.get_embedding(query)
        if query_embedding is None:
            print("–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å embedding –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞")
            return []

        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()

        # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤—Å–µ –Ω–æ–≤–æ—Å—Ç–∏ (—Å —Ñ–∏–ª—å—Ç—Ä–æ–º –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ)
        if category:
            cursor.execute('SELECT id, title, description, link, source, published, embedding FROM news WHERE category = ?', (category,))
        else:
            cursor.execute('SELECT id, title, description, link, source, published, embedding FROM news')

        results = []
        for row in cursor.fetchall():
            news_id, title, description, link, source, published, embedding_blob = row

            # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º embedding
            news_embedding = np.frombuffer(embedding_blob, dtype=np.float32)

            # –í—ã—á–∏—Å–ª—è–µ–º –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ
            similarity = np.dot(query_embedding, news_embedding) / (
                np.linalg.norm(query_embedding) * np.linalg.norm(news_embedding)
            )

            results.append({
                'id': news_id,
                'title': title,
                'description': description,
                'link': link,
                'source': source,
                'published': published,
                'similarity': float(similarity)
            })

        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Å—Ö–æ–∂–µ—Å—Ç–∏
        results.sort(key=lambda x: x['similarity'], reverse=True)

        conn.close()
        return results[:top_k]

    def get_stats(self) -> Dict:
        """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –±–∞–∑–µ"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()

        cursor.execute('SELECT COUNT(*) FROM news')
        total = cursor.fetchone()[0]

        cursor.execute('SELECT source, COUNT(*) FROM news GROUP BY source')
        by_source = dict(cursor.fetchall())

        cursor.execute('SELECT category, COUNT(*) FROM news GROUP BY category')
        by_category = dict(cursor.fetchall())

        conn.close()

        return {
            'total': total,
            'by_source': by_source,
            'by_category': by_category
        }

def main():
    print("=" * 70)
    print("üöÄ RAG-—Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –Ω–æ–≤–æ—Å—Ç–µ–π (–º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–∞—è –≤–µ—Ä—Å–∏—è)")
    print("=" * 70)
    print()

    system = NewsRAGSystem()

    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–µ–∫—É—â—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
    stats = system.get_stats()
    print(f"üìä –¢–µ–∫—É—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
    print(f"   –í—Å–µ–≥–æ –Ω–æ–≤–æ—Å—Ç–µ–π: {stats['total']}")
    if stats['by_source']:
        print(f"   –ü–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º:")
        for source, count in stats['by_source'].items():
            print(f"     ‚Ä¢ {source}: {count}")
    print()

    # –ò–Ω–¥–µ–∫—Å–∏—Ä—É–µ–º –Ω–æ–≤–æ—Å—Ç–∏
    new_count = system.fetch_and_index_news()

    # –û–±–Ω–æ–≤–ª–µ–Ω–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    if new_count > 0:
        stats = system.get_stats()
        print(f"\nüìä –û–±–Ω–æ–≤–ª–µ–Ω–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
        print(f"   –í—Å–µ–≥–æ –Ω–æ–≤–æ—Å—Ç–µ–π: {stats['total']}")

if __name__ == "__main__":
    main()
