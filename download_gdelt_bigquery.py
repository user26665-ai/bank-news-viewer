#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö GDELT Article List –∏–∑ BigQuery

–§—É–Ω–∫—Ü–∏–∏:
- –ê—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è —á–µ—Ä–µ–∑ –±—Ä–∞—É–∑–µ—Ä (OAuth)
- –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∑–∞ —É–∫–∞–∑–∞–Ω–Ω—ã–π –ø–µ—Ä–∏–æ–¥
- –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ —è–∑—ã–∫—É, —Å—Ç—Ä–∞–Ω–µ, –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
- –≠–∫—Å–ø–æ—Ä—Ç –≤ CSV/JSON
- –ü–∞–∫–µ—Ç–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è –±–æ–ª—å—à–∏—Ö –æ–±—ä–µ–º–æ–≤

–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
    python3 download_gdelt_bigquery.py --start-date 2024-01-01 --end-date 2024-12-31 --language Russian
"""

import pandas as pd
import pandas_gbq
from datetime import datetime, timedelta
import argparse
import os
import sys
import json

# –û—Ç–∫–ª—é—á–∞–µ–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è pandas
import warnings
warnings.filterwarnings('ignore')

def authenticate():
    """
    –ê—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è –≤ Google Cloud
    –û—Ç–∫—Ä–æ–µ—Ç –±—Ä–∞—É–∑–µ—Ä –¥–ª—è –≤—Ö–æ–¥–∞ –≤ Google –∞–∫–∫–∞—É–Ω—Ç
    """
    print("=" * 70)
    print("üîê –ê—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è –≤ Google Cloud")
    print("=" * 70)
    print("\nüìå –í–ê–ñ–ù–û:")
    print("1. –°–µ–π—á–∞—Å –æ—Ç–∫—Ä–æ–µ—Ç—Å—è –±—Ä–∞—É–∑–µ—Ä")
    print("2. –í–æ–π–¥–∏—Ç–µ –≤ —Å–≤–æ–π Google –∞–∫–∫–∞—É–Ω—Ç")
    print("3. –†–∞–∑—Ä–µ—à–∏—Ç–µ –¥–æ—Å—Ç—É–ø –∫ BigQuery")
    print("4. –ú–æ–∂–µ—Ç–µ –∑–∞–∫—Ä—ã—Ç—å –±—Ä–∞—É–∑–µ—Ä –ø–æ—Å–ª–µ —É—Å–ø–µ—à–Ω–æ–π –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏")
    print("\n–≠—Ç–æ –Ω—É–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å —Ç–æ–ª—å–∫–æ –æ–¥–∏–Ω —Ä–∞–∑.")
    print("\n‚è≥ –ó–∞–ø—É—Å–∫–∞—é OAuth flow —á–µ—Ä–µ–∑ 3 —Å–µ–∫—É–Ω–¥—ã...")

    import time
    time.sleep(3)

    # pandas-gbq –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∑–∞–ø—É—Å—Ç–∏—Ç OAuth flow
    # Credentials —Å–æ—Ö—Ä–∞–Ω—è—Ç—Å—è –ª–æ–∫–∞–ª—å–Ω–æ
    return True


def estimate_query_size(query):
    """–û—Ü–µ–Ω–∫–∞ —Ä–∞–∑–º–µ—Ä–∞ –∑–∞–ø—Ä–æ—Å–∞ (–¥–ª—è –∫–æ–Ω—Ç—Ä–æ–ª—è –±–µ—Å–ø–ª–∞—Ç–Ω–æ–≥–æ –ª–∏–º–∏—Ç–∞ 1TB)"""
    # –ü—Ä–∏–º–µ—Ä–Ω–∞—è –æ—Ü–µ–Ω–∫–∞: GDELT GAL —Å–æ–¥–µ—Ä–∂–∏—Ç ~1 –º–ª—Ä–¥ –∑–∞–ø–∏—Å–µ–π
    # –í —Å—Ä–µ–¥–Ω–µ–º 1 –º–µ—Å—è—Ü = ~50 –º–ª–Ω –∑–∞–ø–∏—Å–µ–π = ~50 GB
    # –ì–æ–¥ = ~600 –º–ª–Ω –∑–∞–ø–∏—Å–µ–π = ~600 GB (–≤ –ø—Ä–µ–¥–µ–ª–∞—Ö –±–µ—Å–ø–ª–∞—Ç–Ω–æ–≥–æ –ª–∏–º–∏—Ç–∞)
    print("\nüí° –ü—Ä–∏–º–µ—Ä–Ω–∞—è –æ—Ü–µ–Ω–∫–∞:")
    print("   - 1 –¥–µ–Ω—å GDELT ‚âà 2-5 GB")
    print("   - 1 –º–µ—Å—è—Ü ‚âà 50 GB")
    print("   - 1 –≥–æ–¥ ‚âà 500-600 GB")
    print("   - –ë–µ—Å–ø–ª–∞—Ç–Ω—ã–π –ª–∏–º–∏—Ç BigQuery: 1 TB/–º–µ—Å—è—Ü")


def download_gdelt_batch(start_date, end_date, output_dir="gdelt_data",
                          language=None, country=None, keywords=None,
                          batch_days=7, file_format="csv"):
    """
    –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö GDELT Article List –∏–∑ BigQuery –ø–æ –±–∞—Ç—á–∞–º

    Args:
        start_date: –Ω–∞—á–∞–ª—å–Ω–∞—è –¥–∞—Ç–∞ (YYYY-MM-DD)
        end_date: –∫–æ–Ω–µ—á–Ω–∞—è –¥–∞—Ç–∞ (YYYY-MM-DD)
        output_dir: –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ñ–∞–π–ª–æ–≤
        language: —Ñ–∏–ª—å—Ç—Ä –ø–æ —è–∑—ã–∫—É (–Ω–∞–ø—Ä–∏–º–µ—Ä, "Russian")
        country: —Ñ–∏–ª—å—Ç—Ä –ø–æ —Å—Ç—Ä–∞–Ω–µ (–Ω–∞–ø—Ä–∏–º–µ—Ä, "Russia")
        keywords: —Å–ø–∏—Å–æ–∫ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
        batch_days: —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –≤ –¥–Ω—è—Ö (–¥–ª—è —Ä–∞–∑–±–∏–≤–∫–∏ –±–æ–ª—å—à–∏—Ö –∑–∞–ø—Ä–æ—Å–æ–≤)
        file_format: —Ñ–æ—Ä–º–∞—Ç —Ñ–∞–π–ª–æ–≤ ("csv", "json", "parquet")
    """

    # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è –¥–∞–Ω–Ω—ã—Ö
    os.makedirs(output_dir, exist_ok=True)

    print("\n" + "=" * 70)
    print("üì• –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö GDELT –∏–∑ BigQuery")
    print("=" * 70)
    print(f"\nüìÖ –ü–µ—Ä–∏–æ–¥: {start_date} ‚Üí {end_date}")
    print(f"üóÇÔ∏è  –í—ã—Ö–æ–¥–Ω–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {output_dir}/")
    print(f"üì¶ –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞: {batch_days} –¥–Ω–µ–π")
    print(f"üìÑ –§–æ—Ä–º–∞—Ç: {file_format.upper()}")

    if language:
        print(f"üåê –Ø–∑—ã–∫: {language}")
    if country:
        print(f"üåç –°—Ç—Ä–∞–Ω–∞: {country}")
    if keywords:
        print(f"üîç –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞: {', '.join(keywords)}")

    estimate_query_size(None)

    # –†–∞–∑–±–∏–≤–∞–µ–º –ø–µ—Ä–∏–æ–¥ –Ω–∞ –±–∞—Ç—á–∏
    start = datetime.strptime(start_date, "%Y-%m-%d")
    end = datetime.strptime(end_date, "%Y-%m-%d")

    current = start
    batch_num = 1
    total_rows = 0

    print("\n" + "=" * 70)
    print("‚è≥ –ù–∞—á–∏–Ω–∞—é –∑–∞–≥—Ä—É–∑–∫—É...")
    print("=" * 70)

    while current <= end:
        batch_end = min(current + timedelta(days=batch_days - 1), end)

        print(f"\nüì¶ –ë–∞—Ç—á #{batch_num}: {current.strftime('%Y-%m-%d')} ‚Üí {batch_end.strftime('%Y-%m-%d')}")

        # –§–æ—Ä–º–∏—Ä—É–µ–º SQL –∑–∞–ø—Ä–æ—Å
        query = f"""
        SELECT
            url,
            title,
            seendate,
            socialimage,
            domain,
            language,
            sourcecountry
        FROM
            `gdelt-bq.gdeltv2.gal`
        WHERE
            DATE(seendate) BETWEEN '{current.strftime('%Y-%m-%d')}' AND '{batch_end.strftime('%Y-%m-%d')}'
        """

        # –î–æ–±–∞–≤–ª—è–µ–º —Ñ–∏–ª—å—Ç—Ä—ã
        if language:
            query += f"\n            AND language = '{language}'"
        if country:
            query += f"\n            AND sourcecountry = '{country}'"
        if keywords:
            keyword_filter = " OR ".join([f"LOWER(title) LIKE '%{kw.lower()}%'" for kw in keywords])
            query += f"\n            AND ({keyword_filter})"

        try:
            # –í—ã–ø–æ–ª–Ω—è–µ–º –∑–∞–ø—Ä–æ—Å —á–µ—Ä–µ–∑ pandas-gbq
            print(f"   üîÑ –ó–∞–ø—Ä–∞—à–∏–≤–∞—é –¥–∞–Ω–Ω—ã–µ...")

            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ–µ–∫—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è (–∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –æ–∫—Ä—É–∂–µ–Ω–∏—è –∏–ª–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞)
            project_id = os.environ.get('GOOGLE_CLOUD_PROJECT')
            if not project_id:
                raise ValueError(
                    "‚ùå –ù–µ —É–∫–∞–∑–∞–Ω GOOGLE_CLOUD_PROJECT!\n\n"
                    "–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –æ–∫—Ä—É–∂–µ–Ω–∏—è:\n"
                    "export GOOGLE_CLOUD_PROJECT='your-project-id'\n\n"
                    "–ò–ª–∏ –ø–µ—Ä–µ–¥–∞–π—Ç–µ —á–µ—Ä–µ–∑ –ø–∞—Ä–∞–º–µ—Ç—Ä --project-id\n"
                    "–°–º. –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—é –≤ GDELT_BIGQUERY_SETUP.md"
                )

            df = pandas_gbq.read_gbq(
                query,
                project_id=project_id,
                progress_bar_type='tqdm'
            )

            rows = len(df)
            total_rows += rows

            print(f"   ‚úÖ –ü–æ–ª—É—á–µ–Ω–æ –∑–∞–ø–∏—Å–µ–π: {rows:,}")

            if rows > 0:
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∞–π–ª
                filename = f"gdelt_{current.strftime('%Y%m%d')}_{batch_end.strftime('%Y%m%d')}.{file_format}"
                filepath = os.path.join(output_dir, filename)

                if file_format == "csv":
                    df.to_csv(filepath, index=False, encoding='utf-8')
                elif file_format == "json":
                    df.to_json(filepath, orient='records', force_ascii=False, indent=2)
                elif file_format == "parquet":
                    df.to_parquet(filepath, index=False)

                file_size = os.path.getsize(filepath) / (1024 * 1024)  # MB
                print(f"   üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {filename} ({file_size:.1f} MB)")

            else:
                print(f"   ‚ö†Ô∏è  –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –∑–∞ —ç—Ç–æ—Ç –ø–µ—Ä–∏–æ–¥")

        except Exception as e:
            print(f"   ‚ùå –û—à–∏–±–∫–∞: {e}")
            print(f"   –ü—Ä–æ–ø—É—Å–∫–∞—é –±–∞—Ç—á –∏ –ø—Ä–æ–¥–æ–ª–∂–∞—é...")

        current = batch_end + timedelta(days=1)
        batch_num += 1

    print("\n" + "=" * 70)
    print("‚úÖ –ó–∞–≥—Ä—É–∑–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")
    print("=" * 70)
    print(f"\nüìä –í—Å–µ–≥–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ: {total_rows:,} –∑–∞–ø–∏—Å–µ–π")
    print(f"üìÅ –§–∞–π–ª—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {output_dir}/")

    # –°–æ–∑–¥–∞–µ–º —Å–≤–æ–¥–Ω—ã–π —Ñ–∞–π–ª —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
    metadata = {
        "download_date": datetime.now().isoformat(),
        "period": {
            "start": start_date,
            "end": end_date
        },
        "filters": {
            "language": language,
            "country": country,
            "keywords": keywords
        },
        "total_rows": total_rows,
        "batch_size_days": batch_days,
        "file_format": file_format
    }

    with open(os.path.join(output_dir, "metadata.json"), "w", encoding="utf-8") as f:
        json.dump(metadata, f, indent=2, ensure_ascii=False)

    print(f"üìã –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {output_dir}/metadata.json")

    return total_rows


def main():
    parser = argparse.ArgumentParser(
        description="–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö GDELT Article List –∏–∑ BigQuery",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
–ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:

  # –í—Å–µ —Ä—É—Å—Å–∫–æ—è–∑—ã—á–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏ –∑–∞ 2024 –≥–æ–¥
  python3 download_gdelt_bigquery.py --start-date 2024-01-01 --end-date 2024-12-31 --language Russian

  # –ù–æ–≤–æ—Å—Ç–∏ –∏–∑ –†–æ—Å—Å–∏–∏ –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–π –º–µ—Å—è—Ü
  python3 download_gdelt_bigquery.py --start-date 2024-11-01 --end-date 2024-11-30 --country Russia

  # –ù–æ–≤–æ—Å—Ç–∏ —Å –∫–ª—é—á–µ–≤—ã–º–∏ —Å–ª–æ–≤–∞–º–∏
  python3 download_gdelt_bigquery.py --start-date 2024-01-01 --end-date 2024-12-31 --keywords "–±–∞–Ω–∫,—Å–∞–Ω–∫—Ü–∏–∏,—Ä—É–±–ª—å"

  # –ë—ã—Å—Ç—Ä–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ (–±–æ–ª—å—à–∏–µ –±–∞—Ç—á–∏)
  python3 download_gdelt_bigquery.py --start-date 2024-01-01 --end-date 2024-12-31 --batch-days 30

  # –≠–∫—Å–ø–æ—Ä—Ç –≤ JSON
  python3 download_gdelt_bigquery.py --start-date 2024-01-01 --end-date 2024-12-31 --format json
        """
    )

    parser.add_argument("--start-date", required=True, help="–ù–∞—á–∞–ª—å–Ω–∞—è –¥–∞—Ç–∞ (YYYY-MM-DD)")
    parser.add_argument("--end-date", required=True, help="–ö–æ–Ω–µ—á–Ω–∞—è –¥–∞—Ç–∞ (YYYY-MM-DD)")
    parser.add_argument("--project-id", help="Google Cloud Project ID (–∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é GOOGLE_CLOUD_PROJECT)")
    parser.add_argument("--output-dir", default="gdelt_data", help="–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: gdelt_data)")
    parser.add_argument("--language", help="–§–∏–ª—å—Ç—Ä –ø–æ —è–∑—ã–∫—É (–Ω–∞–ø—Ä–∏–º–µ—Ä: Russian, English)")
    parser.add_argument("--country", help="–§–∏–ª—å—Ç—Ä –ø–æ —Å—Ç—Ä–∞–Ω–µ (–Ω–∞–ø—Ä–∏–º–µ—Ä: Russia, US)")
    parser.add_argument("--keywords", help="–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ —á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é (–Ω–∞–ø—Ä–∏–º–µ—Ä: –±–∞–Ω–∫,—Å–∞–Ω–∫—Ü–∏–∏)")
    parser.add_argument("--batch-days", type=int, default=7, help="–†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –≤ –¥–Ω—è—Ö (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 7)")
    parser.add_argument("--format", choices=["csv", "json", "parquet"], default="csv", help="–§–æ—Ä–º–∞—Ç –≤—ã—Ö–æ–¥–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤")
    parser.add_argument("--no-auth", action="store_true", help="–ü—Ä–æ–ø—É—Å—Ç–∏—Ç—å –∞—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—é (–µ—Å–ª–∏ —É–∂–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∞)")

    args = parser.parse_args()

    # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º project_id –µ—Å–ª–∏ –ø–µ—Ä–µ–¥–∞–Ω
    if args.project_id:
        os.environ['GOOGLE_CLOUD_PROJECT'] = args.project_id

    # –ü–∞—Ä—Å–∏–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
    keywords = None
    if args.keywords:
        keywords = [kw.strip() for kw in args.keywords.split(",")]

    # –ê—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è
    if not args.no_auth:
        authenticate()

    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    try:
        total_rows = download_gdelt_batch(
            start_date=args.start_date,
            end_date=args.end_date,
            output_dir=args.output_dir,
            language=args.language,
            country=args.country,
            keywords=keywords,
            batch_days=args.batch_days,
            file_format=args.format
        )

        print("\nüéâ –ì–æ—Ç–æ–≤–æ!")
        return 0

    except Exception as e:
        print(f"\n‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    sys.exit(main())
