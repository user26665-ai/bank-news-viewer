#!/usr/bin/env python3
"""
–û–±—É—á–µ–Ω–∏–µ LTR –º–æ–¥–µ–ª–∏ –Ω–∞ —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
"""

import json
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
import lightgbm as lgb
import pickle

def load_annotated_dataset(json_path: str):
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç"""
    with open(json_path, 'r', encoding='utf-8') as f:
        data = json.load(f)

    # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã–µ
    annotated = [item for item in data if item['label'] is not None]

    print(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞:")
    print(f"   –í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π: {len(data)}")
    print(f"   –†–∞–∑–º–µ—á–µ–Ω–æ: {len(annotated)}")

    if not annotated:
        raise ValueError("–ù–µ—Ç —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö! –°–Ω–∞—á–∞–ª–∞ —Ä–∞–∑–º–µ—Ç—å—Ç–µ –¥–∞—Ç–∞—Å–µ—Ç.")

    # –°—á–∏—Ç–∞–µ–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ—Ü–µ–Ω–æ–∫
    labels = [item['label'] for item in annotated]
    for i in range(4):
        count = labels.count(i)
        pct = count / len(labels) * 100
        print(f"   –û—Ü–µ–Ω–∫–∞ {i}: {count} ({pct:.1f}%)")

    return annotated

def prepare_training_data(annotated_data):
    """–ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""

    # –°–æ–∑–¥–∞–µ–º DataFrame
    rows = []
    for item in annotated_data:
        row = {
            'query': item['query'],
            'label': item['label'],
            **item['features']
        }
        rows.append(row)

    df = pd.DataFrame(rows)

    # –§–∏—á–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
    feature_columns = [col for col in df.columns if col not in ['query', 'label']]

    X = df[feature_columns]
    y = df['label']

    # –ì—Ä—É–ø–ø—ã –∑–∞–ø—Ä–æ—Å–æ–≤ (–¥–ª—è LTR –≤–∞–∂–Ω–æ!)
    query_groups = df.groupby('query').size().values

    print(f"\nüìà –§–∏—á–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è ({len(feature_columns)}):")
    for col in feature_columns:
        print(f"   ‚Ä¢ {col}")

    return X, y, query_groups, feature_columns

def train_lightgbm_ranker(X, y, query_groups):
    """–û–±—É—á–∞–µ—Ç LightGBM Ranker"""

    print("\nüöÄ –û–±—É—á–µ–Ω–∏–µ LightGBM Ranker...")

    # –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ train/test –ø–æ –≥—Ä—É–ø–ø–∞–º –∑–∞–ø—Ä–æ—Å–æ–≤ (–≤–∞–∂–Ω–æ!)
    n_groups = len(query_groups)
    n_train_groups = int(0.8 * n_groups)

    # –†–∞–∑–¥–µ–ª—è–µ–º –≥—Ä—É–ø–ø—ã
    train_groups = query_groups[:n_train_groups]
    test_groups = query_groups[n_train_groups:]

    # –ù–∞—Ö–æ–¥–∏–º –∏–Ω–¥–µ–∫—Å —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è –≤ –¥–∞–Ω–Ω—ã—Ö
    train_size = sum(train_groups)

    X_train = X[:train_size]
    y_train = y[:train_size]

    X_test = X[train_size:]
    y_test = y[train_size:]

    print(f"   Train: {len(X_train)} –∑–∞–ø–∏—Å–µ–π, {len(train_groups)} –∑–∞–ø—Ä–æ—Å–æ–≤")
    print(f"   Test: {len(X_test)} –∑–∞–ø–∏—Å–µ–π, {len(test_groups)} –∑–∞–ø—Ä–æ—Å–æ–≤")

    # –°–æ–∑–¥–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç—ã LightGBM
    train_data = lgb.Dataset(X_train, label=y_train, group=train_groups)
    test_data = lgb.Dataset(X_test, label=y_test, group=test_groups, reference=train_data)

    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏
    params = {
        'objective': 'lambdarank',
        'metric': 'ndcg',
        'ndcg_eval_at': [1, 3, 5, 10],
        'learning_rate': 0.05,
        'num_leaves': 31,
        'feature_fraction': 0.8,
        'bagging_fraction': 0.8,
        'bagging_freq': 5,
        'verbose': 1,
        'max_depth': 6,
    }

    # –û–±—É—á–µ–Ω–∏–µ
    evals_result = {}
    model = lgb.train(
        params,
        train_data,
        num_boost_round=200,
        valid_sets=[train_data, test_data],
        valid_names=['train', 'test'],
        callbacks=[
            lgb.early_stopping(stopping_rounds=20),
            lgb.log_evaluation(period=10),
            lgb.record_evaluation(evals_result)
        ]
    )

    print("\n‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")

    # –í–∞–∂–Ω–æ—Å—Ç—å —Ñ–∏—á–µ–π
    importance = model.feature_importance(importance_type='gain')
    feature_names = X.columns

    print("\nüìä –í–∞–∂–Ω–æ—Å—Ç—å —Ñ–∏—á–µ–π (top-5):")
    feature_importance = sorted(zip(feature_names, importance), key=lambda x: x[1], reverse=True)
    for i, (name, imp) in enumerate(feature_importance[:5], 1):
        print(f"   {i}. {name}: {imp:.2f}")

    # –ú–µ—Ç—Ä–∏–∫–∏
    best_score = evals_result['test']['ndcg@10'][-1]
    print(f"\nüéØ NDCG@10 –Ω–∞ —Ç–µ—Å—Ç–µ: {best_score:.4f}")

    return model, feature_importance

def save_model(model, feature_columns, output_path='ltr_model.pkl'):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –º–æ–¥–µ–ª—å"""
    model_data = {
        'model': model,
        'feature_columns': feature_columns
    }

    with open(output_path, 'wb') as f:
        pickle.dump(model_data, f)

    print(f"\nüíæ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {output_path}")

def main():
    print("=" * 70)
    print("üéì –û–±—É—á–µ–Ω–∏–µ Learning to Rank –º–æ–¥–µ–ª–∏")
    print("=" * 70)
    print()

    # 1. –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
    annotated = load_annotated_dataset('ltr_dataset.json')

    # 2. –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ
    X, y, query_groups, feature_columns = prepare_training_data(annotated)

    # 3. –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å
    model, feature_importance = train_lightgbm_ranker(X, y, query_groups)

    # 4. –°–æ—Ö—Ä–∞–Ω—è–µ–º
    save_model(model, feature_columns)

    print("\n" + "=" * 70)
    print("‚úÖ –ì–æ—Ç–æ–≤–æ!")
    print("=" * 70)
    print()
    print("üìå –°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏:")
    print("   1. –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ ltr_model.pkl")
    print("   2. –ò–Ω—Ç–µ–≥—Ä–∏—Ä—É–π—Ç–µ –º–æ–¥–µ–ª—å –≤ –ø–æ–∏—Å–∫ —á–µ—Ä–µ–∑ integrate_ltr_model.py")
    print()

if __name__ == "__main__":
    try:
        import lightgbm
    except ImportError:
        print("‚ùå –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ LightGBM: pip install lightgbm")
        exit(1)

    main()
