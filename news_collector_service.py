#!/usr/bin/env python3
"""
News Collector Service (—Å –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–æ–º)
- –ó–∞–≥—Ä—É–∂–∞–µ—Ç RSS —Å –ø–µ—Ä–∏–æ–¥–∏—á–Ω–æ—Å—Ç—å—é
- –°–æ–∑–¥–∞–µ—Ç embeddings
- –ü—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç REST API –¥–ª—è –ø–æ–∏—Å–∫–∞ –Ω–æ–≤–æ—Å—Ç–µ–π

–ó–∞–ø—É—Å–∫: python3 news_collector_service.py
API: http://localhost:8001
"""

from fastapi import FastAPI, HTTPException, BackgroundTasks
from fastapi.responses import JSONResponse
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import List, Optional
import uvicorn
import asyncio
from datetime import datetime
import json

from news_rag_system import NewsRAGSystem

app = FastAPI(title="News Collector API", version="1.0")

# –î–æ–±–∞–≤–ª—è–µ–º CORS middleware –¥–ª—è –≤–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # –†–∞–∑—Ä–µ—à–∞–µ–º –≤—Å–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ (–¥–ª—è –ª–æ–∫–∞–ª—å–Ω–æ–π —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏)
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä RAG —Å–∏—Å—Ç–µ–º—ã
rag = NewsRAGSystem()

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
UPDATE_INTERVAL_SECONDS = 3600  # 1 —á–∞—Å

class SearchRequest(BaseModel):
    query: str
    top_k: int = 20
    category: Optional[str] = None

class NewsItem(BaseModel):
    id: int
    title: str
    description: str
    link: str
    source: str
    published: str
    similarity: float
    keyword_score: Optional[float] = 0
    vector_score: Optional[float] = 0
    bank_boost: Optional[float] = 1.0
    critical_keywords: Optional[int] = 0
    geo_boost: Optional[float] = 1.0

class SearchResponse(BaseModel):
    query: str
    total_found: int
    news: List[NewsItem]
    timestamp: str

class StatsResponse(BaseModel):
    total_news: int
    by_source: dict
    by_category: dict
    last_update: str

# –ó–∞–≥—Ä—É–∑–∫–∞ –±–∞–Ω–∫–æ–≤—Å–∫–∏—Ö –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
def load_bank_keywords():
    try:
        with open("/Users/david/bank_news_agent/news_sources.json", 'r', encoding='utf-8') as f:
            data = json.load(f)
            return data.get('bank_keywords', {})
    except:
        return {"critical": [], "high": [], "exclude": []}

def calculate_banking_relevance(title: str, description: str) -> dict:
    """–í—ã—á–∏—Å–ª–∏—Ç—å —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –Ω–æ–≤–æ—Å—Ç–∏ –¥–ª—è –±–∞–Ω–∫–∞ (–û–¢–ö–õ–Æ–ß–ï–ù–û)"""
    keywords = load_bank_keywords()
    text = f"{title} {description}".lower()

    score = {
        'critical_matches': 0,
        'high_matches': 0,
        'exclude_matches': 0,
        'boost': 1.0  # –í–°–ï –Ω–æ–≤–æ—Å—Ç–∏ –ø–æ–ª—É—á–∞—é—Ç –Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π –±—É—Å—Ç
    }

    # –°—á–∏—Ç–∞–µ–º –¥–ª—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏, –Ω–æ –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º –¥–ª—è –±—É—Å—Ç–∞
    for keyword in keywords.get('critical', []):
        if keyword.lower() in text:
            score['critical_matches'] += 1

    for keyword in keywords.get('high', []):
        if keyword.lower() in text:
            score['high_matches'] += 1

    for keyword in keywords.get('exclude', []):
        if keyword.lower() in text:
            score['exclude_matches'] += 1

    # –ü–†–ò–û–†–ò–¢–ï–ó–ê–¶–ò–Ø –û–¢–ö–õ–Æ–ß–ï–ù–ê - –≤—Å–µ –Ω–æ–≤–æ—Å—Ç–∏ —Ä–∞–≤–Ω—ã
    score['boost'] = 1.0

    return score

def expand_query(query: str) -> list:
    """–†–∞—Å—à–∏—Ä–µ–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞ —Å–∏–Ω–æ–Ω–∏–º–∞–º–∏ –∏ –≤–∞—Ä–∏–∞–Ω—Ç–∞–º–∏ (—Ñ—Ä–∞–∑–æ–≤–æ–µ + —Å–ª–æ–≤–∞—Ä–Ω–æ–µ)"""

    # –§—Ä–∞–∑–æ–≤—ã–µ —Å–∏–Ω–æ–Ω–∏–º—ã (–æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—é—Ç—Å—è –ü–ï–†–ï–î —Ä–∞–∑–±–∏–µ–Ω–∏–µ–º –Ω–∞ —Å–ª–æ–≤–∞)
    phrase_synonyms = {
        '—Å—Ç–∞–≤–∫–∞ —Ü–±': ['–∫–ª—é—á–µ–≤–∞—è —Å—Ç–∞–≤–∫–∞', '—Å—Ç–∞–≤–∫–∞ —Ü–±', '—Å—Ç–∞–≤–∫–∞ —Ü–µ–Ω—Ç—Ä–æ–±–∞–Ω–∫–∞', '—Å—Ç–∞–≤–∫–∞ –±–∞–Ω–∫–∞ —Ä–æ—Å—Å–∏–∏'],
        '–∫–ª—é—á–µ–≤–∞—è —Å—Ç–∞–≤–∫–∞': ['–∫–ª—é—á–µ–≤–∞—è —Å—Ç–∞–≤–∫–∞', '—Å—Ç–∞–≤–∫–∞ —Ü–±', '—Å—Ç–∞–≤–∫–∞ —Ü–µ–Ω—Ç—Ä–æ–±–∞–Ω–∫–∞'],
        '–∫—É—Ä—Å —Ä—É–±–ª—è': ['–∫—É—Ä—Å —Ä—É–±–ª—è', '–∫—É—Ä—Å –¥–æ–ª–ª–∞—Ä–∞', '—Ä—É–±–ª—å –¥–æ–ª–ª–∞—Ä', '–≤–∞–ª—é—Ç–Ω—ã–π –∫—É—Ä—Å'],
        '–∫—É—Ä—Å –¥–æ–ª–ª–∞—Ä–∞': ['–∫—É—Ä—Å –¥–æ–ª–ª–∞—Ä–∞', '–∫—É—Ä—Å —Ä—É–±–ª—è', '–¥–æ–ª–ª–∞—Ä —Ä—É–±–ª—å', '–≤–∞–ª—é—Ç–Ω—ã–π –∫—É—Ä—Å'],
    }

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ—Ä–∞–∑—ã
    query_lower = query.lower()
    for phrase, variants in phrase_synonyms.items():
        if phrase in query_lower:
            # –ï—Å–ª–∏ –Ω–∞—à–ª–∏ —Ñ—Ä–∞–∑—É - –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ª—å–∫–æ –µ—ë –≤–∞—Ä–∏–∞–Ω—Ç—ã
            return variants

    # –°–ª–æ–≤–∞—Ä—å —Å–∏–Ω–æ–Ω–∏–º–æ–≤ –¥–ª—è –æ—Ç–¥–µ–ª—å–Ω—ã—Ö —Å–ª–æ–≤ (–µ—Å–ª–∏ —Ñ—Ä–∞–∑—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã)
    synonyms = {
        '–ª—É–∫–æ–π–ª': ['–ª—É–∫–æ–π–ª', 'lukoil'],
        '—Ä–æ—Å–Ω–µ—Ñ—Ç—å': ['—Ä–æ—Å–Ω–µ—Ñ—Ç—å', 'rosneft'],
        '–≥–∞–∑–ø—Ä–æ–º': ['–≥–∞–∑–ø—Ä–æ–º', 'gazprom'],
        '—Å–±–µ—Ä–±–∞–Ω–∫': ['—Å–±–µ—Ä–±–∞–Ω–∫', 'sberbank', '—Å–±–µ—Ä'],
        '–≤—Ç–±': ['–≤—Ç–±', 'vtb'],
        '—Å–∞–Ω–∫—Ü–∏–∏': ['—Å–∞–Ω–∫—Ü–∏–∏', '–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è'],
        '—Ü–±': ['—Ü–±', '—Ü–µ–Ω—Ç—Ä–æ–±–∞–Ω–∫', '–±–∞–Ω–∫ —Ä–æ—Å—Å–∏–∏'],
        '—Ä—É–±–ª—å': ['—Ä—É–±–ª—å', '—Ä—É–±–ª—è', '—Ä—É–±'],
        '–¥–æ–ª–ª–∞—Ä': ['–¥–æ–ª–ª–∞—Ä', 'usd'],
        '–µ–≤—Ä–æ': ['–µ–≤—Ä–æ', 'eur'],
    }

    words = query.lower().split()
    expanded = set(words)

    for word in words:
        for key, variants in synonyms.items():
            if word == key or key in word:
                expanded.update(variants)

    return list(expanded)

def calculate_recency_boost(published_date: str) -> float:
    """
    –í—ã—á–∏—Å–ª–∏—Ç—å –±—É—Å—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–≤–µ–∂–µ—Å—Ç–∏ –Ω–æ–≤–æ—Å—Ç–∏

    Args:
        published_date: –¥–∞—Ç–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ (ISO format string)

    Returns:
        –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –±—É—Å—Ç–∞ (1.0-1.3)
    """
    from datetime import datetime, timedelta
    from email.utils import parsedate_to_datetime

    try:
        # –ü–∞—Ä—Å–∏–º –¥–∞—Ç—É –ø—É–±–ª–∏–∫–∞—Ü–∏–∏
        if not published_date:
            return 1.0

        # –ü—Ä–æ–±—É–µ–º RFC 2822 —Ñ–æ—Ä–º–∞—Ç (–∏–∑ RSS feeds)
        try:
            pub_date = parsedate_to_datetime(published_date)
        except:
            # Fallback –Ω–∞ ISO —Ñ–æ—Ä–º–∞—Ç
            pub_date = datetime.fromisoformat(published_date.replace('Z', '+00:00'))

        now = datetime.now(pub_date.tzinfo) if pub_date.tzinfo else datetime.now()

        # –í—ã—á–∏—Å–ª—è–µ–º –≤–æ–∑—Ä–∞—Å—Ç –Ω–æ–≤–æ—Å—Ç–∏
        age = now - pub_date
        age_hours = age.total_seconds() / 3600

        # –ü—Ä–∏–º–µ–Ω—è–µ–º –±—É—Å—Ç –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –≤–æ–∑—Ä–∞—Å—Ç–∞
        if age_hours < 24:
            # –ü–æ—Å–ª–µ–¥–Ω–∏–µ 24 —á–∞—Å–∞ - –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –±—É—Å—Ç
            return 1.3
        elif age_hours < 72:
            # –ü–æ—Å–ª–µ–¥–Ω–∏–µ 3 –¥–Ω—è - —Å—Ä–µ–¥–Ω–∏–π –±—É—Å—Ç
            return 1.2
        elif age_hours < 168:
            # –ü–æ—Å–ª–µ–¥–Ω—è—è –Ω–µ–¥–µ–ª—è - –Ω–µ–±–æ–ª—å—à–æ–π –±—É—Å—Ç
            return 1.1
        elif age_hours < 720:
            # –ü–æ—Å–ª–µ–¥–Ω–∏–π –º–µ—Å—è—Ü - –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –±—É—Å—Ç
            return 1.05
        else:
            # –°—Ç–∞—Ä—à–µ –º–µ—Å—è—Ü–∞ - –±–µ–∑ –±—É—Å—Ç–∞
            return 1.0

    except Exception as e:
        # –í —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏ –ø–∞—Ä—Å–∏–Ω–≥–∞ - –±–µ–∑ –±—É—Å—Ç–∞
        return 1.0


def hybrid_search_internal(query: str, top_k: int = 20):
    """–ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫ —Å —É–ª—É—á—à–µ–Ω–∏—è–º–∏: –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã–π –≤–µ—Å, query expansion, –≥–µ–æ-–±—É—Å—Ç, recency boost"""
    import sqlite3
    import numpy as np

    # Query expansion
    expanded_keywords = expand_query(query)

    stop_words = {'–ø—Ä–æ', '–æ', '–æ–±', '–≤', '–Ω–∞', '—Å', '–ø–æ', '–¥–ª—è', '–∫', '—É', '–∏–∑', '–æ—Ç', '–∏', '–∏–ª–∏', '–∞', '–Ω–æ', '–ø–æ–∫–∞–∂–∏', '–Ω–∞–π–¥–∏', '–¥–∞–π'}
    keywords = [k for k in expanded_keywords if k not in stop_words and len(k) > 2]

    conn = sqlite3.connect(rag.db_path)
    cursor = conn.cursor()

    # –¢–µ–∫—Å—Ç–æ–≤—ã–π –ø–æ–∏—Å–∫ —Å –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã–º –≤–µ—Å–æ–º (–±–µ–∑ LOWER –¥–ª—è –ø–æ–¥–¥–µ—Ä–∂–∫–∏ —Ä—É—Å—Å–∫–∏—Ö –±—É–∫–≤)
    keyword_results = {}
    for keyword in keywords:
        # –ò—â–µ–º –æ–±–∞ –≤–∞—Ä–∏–∞–Ω—Ç–∞: lowercase –∏ capitalized
        keyword_lower = keyword.lower()
        keyword_cap = keyword.capitalize()

        cursor.execute('''
            SELECT id, title, description, link, source, published, embedding, full_text
            FROM news
            WHERE title LIKE ? OR title LIKE ?
               OR description LIKE ? OR description LIKE ?
               OR full_text LIKE ? OR full_text LIKE ?
        ''', (f'%{keyword_lower}%', f'%{keyword_cap}%',
              f'%{keyword_lower}%', f'%{keyword_cap}%',
              f'%{keyword_lower}%', f'%{keyword_cap}%'))

        rows = cursor.fetchall()

        for row in rows:
            news_id = row[0]
            title = row[1] or ''
            description = row[2] or ''
            full_text = row[7] or ''

            if news_id not in keyword_results:
                keyword_results[news_id] = {
                    'id': news_id,
                    'title': title,
                    'description': description,
                    'link': row[3],
                    'source': row[4],
                    'published': row[5],
                    'embedding': np.frombuffer(row[6], dtype=np.float32),
                    'keyword_score': 0
                }

            # –ü–û–ó–ò–¶–ò–û–ù–ù–´–ô –í–ï–°
            position_weight = 0

            # –§—Ä–∞–∑—ã (2+ —Å–ª–æ–≤–∞) –ø–æ–ª—É—á–∞—é—Ç –ø–æ–≤—ã—à–µ–Ω–Ω—ã–π –≤–µ—Å
            is_phrase = ' ' in keyword
            title_weight = 10.0 if is_phrase else 5.0
            desc_weight = 3.0 if is_phrase else 1.5
            text_weight = 2.0 if is_phrase else 1.0

            if keyword.lower() in title.lower():
                position_weight += title_weight  # –ó–∞–≥–æ–ª–æ–≤–æ–∫: —Ñ—Ä–∞–∑–∞ x10, —Å–ª–æ–≤–æ x5
            if keyword.lower() in description.lower():
                position_weight += desc_weight  # –û–ø–∏—Å–∞–Ω–∏–µ: —Ñ—Ä–∞–∑–∞ x3, —Å–ª–æ–≤–æ x1.5
            if keyword.lower() in full_text.lower():
                position_weight += text_weight  # –ü–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç: —Ñ—Ä–∞–∑–∞ x2, —Å–ª–æ–≤–æ x1

            keyword_results[news_id]['keyword_score'] += position_weight

    # –ë—É—Å—Ç –∑–∞ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –≤ –∑–∞–≥–æ–ª–æ–≤–∫–µ
    for news_id, data in keyword_results.items():
        title = data['title'].lower()
        # –°—á–∏—Ç–∞–µ–º —Å–∫–æ–ª—å–∫–æ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –∏–∑ –∑–∞–ø—Ä–æ—Å–∞ –µ—Å—Ç—å –≤ –∑–∞–≥–æ–ª–æ–≤–∫–µ
        matched_in_title = sum(1 for kw in keywords if kw.lower() in title)

        if matched_in_title >= 2:
            # 2 —Å–ª–æ–≤–∞ -> x1.3, 3 —Å–ª–æ–≤–∞ -> x1.5, 4+ —Å–ª–æ–≤ -> x1.7
            multi_match_boost = 1.0 + (matched_in_title - 1) * 0.3
            data['keyword_score'] *= multi_match_boost

    # DEBUG: print(f"DEBUG: Total keyword_results: {len(keyword_results)}")

    # –í–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫
    query_embedding = rag.get_embedding(query)
    vector_results = {}

    if query_embedding is not None:
        cursor.execute('SELECT id, title, description, link, source, published, embedding FROM news')

        for row in cursor.fetchall():
            news_id, title, description, link, source, published, embedding_blob = row
            news_embedding = np.frombuffer(embedding_blob, dtype=np.float32)

            similarity = np.dot(query_embedding, news_embedding) / (
                np.linalg.norm(query_embedding) * np.linalg.norm(news_embedding)
            )

            vector_results[news_id] = {
                'id': news_id,
                'title': title,
                'description': description,
                'link': link,
                'source': source,
                'published': published,
                'vector_score': float(similarity)
            }

    conn.close()

    # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Å –±–∞–Ω–∫–æ–≤—Å–∫–æ–π —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å—é
    combined_results = {}
    max_keyword_score = max([r['keyword_score'] for r in keyword_results.values()]) if keyword_results else 1
    # DEBUG: print(f"DEBUG: max_keyword_score={max_keyword_score}")

    for news_id, data in keyword_results.items():
        bank_relevance = calculate_banking_relevance(data['title'], data['description'])

        combined_results[news_id] = {
            'id': data['id'],
            'title': data['title'],
            'description': data['description'],
            'link': data['link'],
            'source': data['source'],
            'published': data['published'],
            'keyword_score': data['keyword_score'] / max_keyword_score if max_keyword_score > 0 else 0,
            'vector_score': vector_results.get(news_id, {}).get('vector_score', 0),
            'bank_boost': bank_relevance['boost'],
            'critical_keywords': bank_relevance['critical_matches'],
            'is_excluded': bank_relevance['exclude_matches'] > 0
        }

    # DEBUG: After keyword_results logging disabled

    # –ù–ï –¥–æ–±–∞–≤–ª—è–µ–º vector-only results - –æ–Ω–∏ —Ä–∞–∑–±–∞–≤–ª—è—é—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—É—é –≤—ã–¥–∞—á—É
    # for news_id, data in vector_results.items():
    #     if news_id not in combined_results:
    #         bank_relevance = calculate_banking_relevance(data['title'], data['description'])
    #
    #         combined_results[news_id] = {
    #             'id': data['id'],
    #             'title': data['title'],
    #             'description': data['description'],
    #             'link': data['link'],
    #             'source': data['source'],
    #             'published': data['published'],
    #             'keyword_score': 0,
    #             'vector_score': data['vector_score'],
    #             'bank_boost': bank_relevance['boost'],
    #             'critical_keywords': bank_relevance['critical_matches'],
    #             'is_excluded': bank_relevance['exclude_matches'] > 0
    #         }

    # –§–∏–Ω–∞–ª—å–Ω—ã–π score —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–º —Å–≤–µ–∂–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π
    for data in combined_results.values():
        if data['keyword_score'] > 0:
            # Keyword match –ø–æ–ª—É—á–∞–µ—Ç –±–æ–ª—å—à–∏–π –≤–µ—Å (85%)
            base_score = 0.85 * data['keyword_score'] + 0.15 * data['vector_score']
        else:
            # –¢–æ–ª—å–∫–æ vector search
            base_score = data['vector_score']

        # –ü—Ä–∏–º–µ–Ω—è–µ–º recency boost - —Å–≤–µ–∂–∏–µ –Ω–æ–≤–æ—Å—Ç–∏ –ø–æ–ª—É—á–∞—é—Ç –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç
        recency_boost = calculate_recency_boost(data.get('published', ''))
        final_score = base_score * recency_boost

        data['similarity'] = final_score
        data['recency_boost'] = recency_boost
        data['geo_boost'] = 1.0
        data['war_penalty'] = 1.0

    results = sorted(combined_results.values(), key=lambda x: x['similarity'], reverse=True)

    # DEBUG: Top 10 results logging disabled

    # –§–ò–õ–¨–¢–†–ê–¶–ò–Ø –û–¢–ö–õ–Æ–ß–ï–ù–ê - –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –≤—Å–µ –Ω–æ–≤–æ—Å—Ç–∏
    return results[:top_k]

# Background task –¥–ª—è –ø–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–æ–≥–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è
async def periodic_update():
    """–ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–µ–π (–∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è)"""
    # –ñ–¥–µ–º 5 –º–∏–Ω—É—Ç –ø–µ—Ä–µ–¥ –ø–µ—Ä–≤—ã–º –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ–º, —á—Ç–æ–±—ã –Ω–µ –±–ª–æ–∫–∏—Ä–æ–≤–∞—Ç—å —Å–µ—Ä–≤–∏—Å –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ
    await asyncio.sleep(300)  # 5 –º–∏–Ω—É—Ç

    while True:
        try:
            print(f"\n[{datetime.now().strftime('%H:%M:%S')}] üîÑ –ù–∞—á–∏–Ω–∞—é –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–µ–π...")
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—É—é –≤–µ—Ä—Å–∏—é - –Ω–µ –±–ª–æ–∫–∏—Ä—É–µ—Ç API!
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤—Å–µ –Ω–æ–≤–æ—Å—Ç–∏ –∏–∑ RSS, –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è –æ—Ç—Ñ–∏–ª—å—Ç—Ä—É–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ
            new_count = await rag.fetch_and_index_news_async(limit_per_source=50, max_concurrent=5)
            print(f"[{datetime.now().strftime('%H:%M:%S')}] ‚úÖ –î–æ–±–∞–≤–ª–µ–Ω–æ {new_count} –Ω–æ–≤—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π")
        except Exception as e:
            print(f"[{datetime.now().strftime('%H:%M:%S')}] ‚ùå –û—à–∏–±–∫–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è: {e}")

        await asyncio.sleep(UPDATE_INTERVAL_SECONDS)

@app.on_event("startup")
async def startup_event():
    """–ó–∞–ø—É—Å–∫ —Ñ–æ–Ω–æ–≤–æ–≥–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è"""
    asyncio.create_task(periodic_update())
    print("=" * 70)
    print("üöÄ News Collector Service –∑–∞–ø—É—â–µ–Ω")
    print(f"üì° API: http://localhost:8001")
    print(f"üìñ Docs: http://localhost:8001/docs")
    print(f"üîÑ –ê–≤—Ç–æ–æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∫–∞–∂–¥—ã–µ {UPDATE_INTERVAL_SECONDS // 60} –º–∏–Ω—É—Ç (–ø–µ—Ä–≤–æ–µ —á–µ—Ä–µ–∑ 5 –º–∏–Ω)")
    print(f"üì∞ –ò—Å—Ç–æ—á–Ω–∏–∫–æ–≤: {len(rag.sources) if hasattr(rag, 'sources') else '255'} (–≤–∫–ª. GDELT)")
    print("=" * 70)

@app.get("/")
async def root():
    return {
        "service": "News Collector API",
        "version": "1.0",
        "status": "running",
        "docs": "/docs"
    }

@app.get("/health")
async def health():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–¥–æ—Ä–æ–≤—å—è —Å–µ—Ä–≤–∏—Å–∞"""
    stats = rag.get_stats()
    return {
        "status": "healthy",
        "total_news": stats['total'],
        "timestamp": datetime.now().isoformat()
    }

@app.get("/stats", response_model=StatsResponse)
async def get_stats():
    """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –±–∞–∑—ã"""
    stats = rag.get_stats()
    return StatsResponse(
        total_news=stats['total'],
        by_source=stats['by_source'],
        by_category=stats['by_category'],
        last_update=datetime.now().isoformat()
    )

@app.post("/search", response_model=SearchResponse)
async def search_news(request: SearchRequest):
    """
    –ü–æ–∏—Å–∫ –Ω–æ–≤–æ—Å—Ç–µ–π (–≥–∏–±—Ä–∏–¥–Ω—ã–π: —Ç–µ–∫—Å—Ç + –≤–µ–∫—Ç–æ—Ä–Ω—ã–π + –±–∞–Ω–∫–æ–≤—Å–∫–∞—è –ø—Ä–∏–æ—Ä–∏—Ç–µ–∑–∞—Ü–∏—è)
    """
    try:
        results = hybrid_search_internal(request.query, request.top_k)

        news_items = [
            NewsItem(
                id=item['id'],
                title=item['title'],
                description=item['description'],
                link=item['link'],
                source=item['source'],
                published=item['published'],
                similarity=item['similarity'],
                keyword_score=item.get('keyword_score', 0),
                vector_score=item.get('vector_score', 0),
                bank_boost=item.get('bank_boost', 1.0),
                critical_keywords=item.get('critical_keywords', 0),
                geo_boost=item.get('geo_boost', 1.0)
            )
            for item in results
        ]

        return SearchResponse(
            query=request.query,
            total_found=len(news_items),
            news=news_items,
            timestamp=datetime.now().isoformat()
        )

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Search error: {str(e)}")

@app.post("/update")
async def trigger_update(background_tasks: BackgroundTasks):
    """–ó–∞–ø—É—Å—Ç–∏—Ç—å –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–µ–π –≤—Ä—É—á–Ω—É—é (–∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è)"""
    async def update_news():
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—É—é –≤–µ—Ä—Å–∏—é - –Ω–µ –±–ª–æ–∫–∏—Ä—É–µ—Ç API!
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤—Å–µ –Ω–æ–≤–æ—Å—Ç–∏ –∏–∑ RSS, –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è –æ—Ç—Ñ–∏–ª—å—Ç—Ä—É–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ
            new_count = await rag.fetch_and_index_news_async(limit_per_source=50, max_concurrent=5)
            print(f"‚úÖ –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ: {new_count} –Ω–æ–≤—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π")
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è: {e}")

    background_tasks.add_task(update_news)
    return {"status": "update_started", "message": "–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∑–∞–ø—É—â–µ–Ω–æ –≤ —Ñ–æ–Ω–µ (async)"}

if __name__ == "__main__":
    uvicorn.run(
        app,
        host="0.0.0.0",
        port=8001,
        log_level="info"
    )
