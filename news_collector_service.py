#!/usr/bin/env python3
"""
News Collector Service (—Å –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–æ–º)
- –ó–∞–≥—Ä—É–∂–∞–µ—Ç RSS —Å –ø–µ—Ä–∏–æ–¥–∏—á–Ω–æ—Å—Ç—å—é
- –°–æ–∑–¥–∞–µ—Ç embeddings
- –ü—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç REST API –¥–ª—è –ø–æ–∏—Å–∫–∞ –Ω–æ–≤–æ—Å—Ç–µ–π

–ó–∞–ø—É—Å–∫: python3 news_collector_service.py
API: http://localhost:8001
"""

from fastapi import FastAPI, HTTPException, BackgroundTasks
from fastapi.responses import JSONResponse
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import List, Optional
import uvicorn
import asyncio
from datetime import datetime
import json

from news_rag_system import NewsRAGSystem

app = FastAPI(title="News Collector API", version="1.0")

# –î–æ–±–∞–≤–ª—è–µ–º CORS middleware –¥–ª—è –≤–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # –†–∞–∑—Ä–µ—à–∞–µ–º –≤—Å–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ (–¥–ª—è –ª–æ–∫–∞–ª—å–Ω–æ–π —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏)
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä RAG —Å–∏—Å—Ç–µ–º—ã
rag = NewsRAGSystem()

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
UPDATE_INTERVAL_SECONDS = 3600  # 1 —á–∞—Å

class SearchRequest(BaseModel):
    query: str
    top_k: int = 20
    category: Optional[str] = None

class EntityTag(BaseModel):
    text: str
    type: str
    is_banking: bool = False

class NewsItem(BaseModel):
    id: int
    title: str
    description: str
    link: str
    source: str
    published: str
    similarity: float
    keyword_score: Optional[float] = 0
    vector_score: Optional[float] = 0
    bank_boost: Optional[float] = 1.0
    critical_keywords: Optional[int] = 0
    geo_boost: Optional[float] = 1.0
    entities: Optional[List[EntityTag]] = []

class SearchResponse(BaseModel):
    query: str
    total_found: int
    news: List[NewsItem]
    timestamp: str

class StatsResponse(BaseModel):
    total_news: int
    by_source: dict
    by_category: dict
    last_update: str

# –ó–∞–≥—Ä—É–∑–∫–∞ –±–∞–Ω–∫–æ–≤—Å–∫–∏—Ö –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
def load_bank_keywords():
    try:
        with open("/Users/david/bank_news_agent/news_sources.json", 'r', encoding='utf-8') as f:
            data = json.load(f)
            return data.get('bank_keywords', {})
    except:
        return {"critical": [], "high": [], "exclude": []}

def calculate_banking_relevance(title: str, description: str) -> dict:
    """–í—ã—á–∏—Å–ª–∏—Ç—å —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –Ω–æ–≤–æ—Å—Ç–∏ –¥–ª—è –±–∞–Ω–∫–∞ (–û–¢–ö–õ–Æ–ß–ï–ù–û)"""
    keywords = load_bank_keywords()
    text = f"{title} {description}".lower()

    score = {
        'critical_matches': 0,
        'high_matches': 0,
        'exclude_matches': 0,
        'boost': 1.0  # –í–°–ï –Ω–æ–≤–æ—Å—Ç–∏ –ø–æ–ª—É—á–∞—é—Ç –Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π –±—É—Å—Ç
    }

    # –°—á–∏—Ç–∞–µ–º –¥–ª—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏, –Ω–æ –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º –¥–ª—è –±—É—Å—Ç–∞
    for keyword in keywords.get('critical', []):
        if keyword.lower() in text:
            score['critical_matches'] += 1

    for keyword in keywords.get('high', []):
        if keyword.lower() in text:
            score['high_matches'] += 1

    for keyword in keywords.get('exclude', []):
        if keyword.lower() in text:
            score['exclude_matches'] += 1

    # –ü–†–ò–û–†–ò–¢–ï–ó–ê–¶–ò–Ø –û–¢–ö–õ–Æ–ß–ï–ù–ê - –≤—Å–µ –Ω–æ–≤–æ—Å—Ç–∏ —Ä–∞–≤–Ω—ã
    score['boost'] = 1.0

    return score

def expand_query(query: str) -> list:
    """–†–∞—Å—à–∏—Ä–µ–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞ —Å–∏–Ω–æ–Ω–∏–º–∞–º–∏ –∏ –≤–∞—Ä–∏–∞–Ω—Ç–∞–º–∏ (—Ñ—Ä–∞–∑–æ–≤–æ–µ + —Å–ª–æ–≤–∞—Ä–Ω–æ–µ)"""

    # –§—Ä–∞–∑–æ–≤—ã–µ —Å–∏–Ω–æ–Ω–∏–º—ã (–æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—é—Ç—Å—è –ü–ï–†–ï–î —Ä–∞–∑–±–∏–µ–Ω–∏–µ–º –Ω–∞ —Å–ª–æ–≤–∞)
    phrase_synonyms = {
        '—Å—Ç–∞–≤–∫–∞ —Ü–±': ['–∫–ª—é—á–µ–≤–∞—è —Å—Ç–∞–≤–∫–∞', '—Å—Ç–∞–≤–∫–∞ —Ü–±', '—Å—Ç–∞–≤–∫–∞ —Ü–µ–Ω—Ç—Ä–æ–±–∞–Ω–∫–∞', '—Å—Ç–∞–≤–∫–∞ –±–∞–Ω–∫–∞ —Ä–æ—Å—Å–∏–∏'],
        '–∫–ª—é—á–µ–≤–∞—è —Å—Ç–∞–≤–∫–∞': ['–∫–ª—é—á–µ–≤–∞—è —Å—Ç–∞–≤–∫–∞', '—Å—Ç–∞–≤–∫–∞ —Ü–±', '—Å—Ç–∞–≤–∫–∞ —Ü–µ–Ω—Ç—Ä–æ–±–∞–Ω–∫–∞'],
        '–∫—É—Ä—Å —Ä—É–±–ª—è': ['–∫—É—Ä—Å —Ä—É–±–ª—è', '–∫—É—Ä—Å –¥–æ–ª–ª–∞—Ä–∞', '—Ä—É–±–ª—å –¥–æ–ª–ª–∞—Ä', '–≤–∞–ª—é—Ç–Ω—ã–π –∫—É—Ä—Å'],
        '–∫—É—Ä—Å –¥–æ–ª–ª–∞—Ä–∞': ['–∫—É—Ä—Å –¥–æ–ª–ª–∞—Ä–∞', '–∫—É—Ä—Å —Ä—É–±–ª—è', '–¥–æ–ª–ª–∞—Ä —Ä—É–±–ª—å', '–≤–∞–ª—é—Ç–Ω—ã–π –∫—É—Ä—Å'],
    }

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ—Ä–∞–∑—ã
    query_lower = query.lower()
    for phrase, variants in phrase_synonyms.items():
        if phrase in query_lower:
            # –ï—Å–ª–∏ –Ω–∞—à–ª–∏ —Ñ—Ä–∞–∑—É - –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ª—å–∫–æ –µ—ë –≤–∞—Ä–∏–∞–Ω—Ç—ã
            return variants

    # –°–ª–æ–≤–∞—Ä—å —Å–∏–Ω–æ–Ω–∏–º–æ–≤ –¥–ª—è –æ—Ç–¥–µ–ª—å–Ω—ã—Ö —Å–ª–æ–≤ (–µ—Å–ª–∏ —Ñ—Ä–∞–∑—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã)
    synonyms = {
        '–ª—É–∫–æ–π–ª': ['–ª—É–∫–æ–π–ª', 'lukoil'],
        '—Ä–æ—Å–Ω–µ—Ñ—Ç—å': ['—Ä–æ—Å–Ω–µ—Ñ—Ç—å', 'rosneft'],
        '–≥–∞–∑–ø—Ä–æ–º': ['–≥–∞–∑–ø—Ä–æ–º', 'gazprom'],
        '—Å–±–µ—Ä–±–∞–Ω–∫': ['—Å–±–µ—Ä–±–∞–Ω–∫', 'sberbank', '—Å–±–µ—Ä'],
        '–≤—Ç–±': ['–≤—Ç–±', 'vtb'],
        '—Å–∞–Ω–∫—Ü–∏–∏': ['—Å–∞–Ω–∫—Ü–∏–∏', '–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è'],
        '—Ü–±': ['—Ü–±', '—Ü–µ–Ω—Ç—Ä–æ–±–∞–Ω–∫', '–±–∞–Ω–∫ —Ä–æ—Å—Å–∏–∏'],
        '—Ä—É–±–ª—å': ['—Ä—É–±–ª—å', '—Ä—É–±–ª—è', '—Ä—É–±'],
        '–¥–æ–ª–ª–∞—Ä': ['–¥–æ–ª–ª–∞—Ä', 'usd'],
        '–µ–≤—Ä–æ': ['–µ–≤—Ä–æ', 'eur'],
    }

    words = query.lower().split()
    expanded = set(words)

    for word in words:
        for key, variants in synonyms.items():
            if word == key or key in word:
                expanded.update(variants)

    return list(expanded)

def calculate_recency_boost(published_date: str) -> float:
    """
    –í—ã—á–∏—Å–ª–∏—Ç—å –±—É—Å—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–≤–µ–∂–µ—Å—Ç–∏ –Ω–æ–≤–æ—Å—Ç–∏

    Args:
        published_date: –¥–∞—Ç–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ (ISO format string)

    Returns:
        –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –±—É—Å—Ç–∞ (1.0-1.3)
    """
    from datetime import datetime, timedelta
    from email.utils import parsedate_to_datetime

    try:
        # –ü–∞—Ä—Å–∏–º –¥–∞—Ç—É –ø—É–±–ª–∏–∫–∞—Ü–∏–∏
        if not published_date:
            return 1.0

        # –ü—Ä–æ–±—É–µ–º RFC 2822 —Ñ–æ—Ä–º–∞—Ç (–∏–∑ RSS feeds)
        try:
            pub_date = parsedate_to_datetime(published_date)
        except:
            # Fallback –Ω–∞ ISO —Ñ–æ—Ä–º–∞—Ç
            pub_date = datetime.fromisoformat(published_date.replace('Z', '+00:00'))

        now = datetime.now(pub_date.tzinfo) if pub_date.tzinfo else datetime.now()

        # –í—ã—á–∏—Å–ª—è–µ–º –≤–æ–∑—Ä–∞—Å—Ç –Ω–æ–≤–æ—Å—Ç–∏
        age = now - pub_date
        age_hours = age.total_seconds() / 3600

        # –ü—Ä–∏–º–µ–Ω—è–µ–º –±—É—Å—Ç –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –≤–æ–∑—Ä–∞—Å—Ç–∞
        if age_hours < 24:
            # –ü–æ—Å–ª–µ–¥–Ω–∏–µ 24 —á–∞—Å–∞ - –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –±—É—Å—Ç
            return 1.3
        elif age_hours < 72:
            # –ü–æ—Å–ª–µ–¥–Ω–∏–µ 3 –¥–Ω—è - —Å—Ä–µ–¥–Ω–∏–π –±—É—Å—Ç
            return 1.2
        elif age_hours < 168:
            # –ü–æ—Å–ª–µ–¥–Ω—è—è –Ω–µ–¥–µ–ª—è - –Ω–µ–±–æ–ª—å—à–æ–π –±—É—Å—Ç
            return 1.1
        elif age_hours < 720:
            # –ü–æ—Å–ª–µ–¥–Ω–∏–π –º–µ—Å—è—Ü - –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –±—É—Å—Ç
            return 1.05
        else:
            # –°—Ç–∞—Ä—à–µ –º–µ—Å—è—Ü–∞ - –±–µ–∑ –±—É—Å—Ç–∞
            return 1.0

    except Exception as e:
        # –í —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏ –ø–∞—Ä—Å–∏–Ω–≥–∞ - –±–µ–∑ –±—É—Å—Ç–∞
        return 1.0


def hybrid_search_internal(query: str, top_k: int = 20):
    """–ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫ —Å —É–ª—É—á—à–µ–Ω–∏—è–º–∏: –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã–π –≤–µ—Å, query expansion, NER-–±—É—Å—Ç, recency boost"""
    import sqlite3
    import numpy as np
    from news_ner import NewsNERExtractor

    # Query expansion
    expanded_keywords = expand_query(query)

    stop_words = {
        # –ü—Ä–µ–¥–ª–æ–≥–∏ –∏ —Å–æ—é–∑—ã
        '–ø—Ä–æ', '–æ', '–æ–±', '–≤', '–Ω–∞', '—Å', '–ø–æ', '–¥–ª—è', '–∫', '—É', '–∏–∑', '–æ—Ç', '–∏', '–∏–ª–∏', '–∞', '–Ω–æ', '–∑–∞', '–ø–µ—Ä–µ–¥', '–º–µ–∂–¥—É', '–ø–æ–¥', '–Ω–∞–¥',
        # –ö–æ–º–∞–Ω–¥—ã
        '–ø–æ–∫–∞–∂–∏', '–Ω–∞–π–¥–∏', '–¥–∞–π', '–∏—â–∏', '—Å–º–æ—Ç—Ä–∏',
    }
    keywords = [k for k in expanded_keywords if k not in stop_words and len(k) > 2]

    # –ò–∑–≤–ª–µ–∫–∞–µ–º NER-—Å—É—â–Ω–æ—Å—Ç–∏ –∏–∑ –∑–∞–ø—Ä–æ—Å–∞
    ner_extractor = NewsNERExtractor()
    query_entities = ner_extractor.extract_from_news(query, "")

    # –°–æ–∑–¥–∞–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–æ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö NER-—Å—É—â–Ω–æ—Å—Ç–µ–π –∏–∑ –∑–∞–ø—Ä–æ—Å–∞
    query_ner_normalized = set()
    for entity in query_entities['all']:
        normalized = entity.get('normalized', entity['text'])
        query_ner_normalized.add(normalized.lower())

    conn = sqlite3.connect(rag.db_path)
    cursor = conn.cursor()

    # –¢–µ–∫—Å—Ç–æ–≤—ã–π –ø–æ–∏—Å–∫ —Å –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã–º –≤–µ—Å–æ–º (–±–µ–∑ LOWER –¥–ª—è –ø–æ–¥–¥–µ—Ä–∂–∫–∏ —Ä—É—Å—Å–∫–∏—Ö –±—É–∫–≤)
    keyword_results = {}

    import re
    import pymorphy2

    morph = pymorphy2.MorphAnalyzer()

    def word_in_text(word: str, text: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∏—Ç—å —á—Ç–æ —Å–ª–æ–≤–æ –µ—Å—Ç—å –≤ —Ç–µ–∫—Å—Ç–µ –∫–∞–∫ —Ü–µ–ª–æ–µ —Å–ª–æ–≤–æ (–Ω–µ –ø–æ–¥—Å—Ç—Ä–æ–∫–∞)"""
        # –†–µ–≥—É–ª—è—Ä–Ω–æ–µ –≤—ã—Ä–∞–∂–µ–Ω–∏–µ: –≥—Ä–∞–Ω–∏—Ü—ã —Å–ª–æ–≤ –≤–æ–∫—Ä—É–≥ –∫–ª—é—á–µ–≤–æ–≥–æ —Å–ª–æ–≤–∞
        pattern = r'\b' + re.escape(word.lower()) + r'\b'
        return bool(re.search(pattern, text.lower(), re.UNICODE))

    def get_word_forms(word: str) -> set:
        """–ü–æ–ª—É—á–∏—Ç—å –≤—Å–µ —Ñ–æ—Ä–º—ã —Å–ª–æ–≤–∞ (–ü—É—Ç–∏–Ω, –ü—É—Ç–∏–Ω–∞, –ü—É—Ç–∏–Ω—É, etc.)"""
        forms = {word.lower()}  # –ë–∞–∑–æ–≤–∞—è —Ñ–æ—Ä–º–∞

        # –ü–∞—Ä—Å–∏–º —Å–ª–æ–≤–æ –∏ –ø–æ–ª—É—á–∞–µ–º –≤—Å–µ –µ–≥–æ —Ñ–æ—Ä–º—ã
        parsed = morph.parse(word)
        if parsed:
            lexeme = parsed[0].lexeme  # –í—Å–µ —Ñ–æ—Ä–º—ã —Å–ª–æ–≤–∞
            for form in lexeme:
                forms.add(form.word.lower())

        return forms

    for keyword in keywords:
        # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ —Ñ–æ—Ä–º—ã —Å–ª–æ–≤–∞ –∑–∞—Ä–∞–Ω–µ–µ
        word_forms = get_word_forms(keyword)

        # –°–æ–∑–¥–∞–µ–º SQL –∑–∞–ø—Ä–æ—Å —Å–æ –≤—Å–µ–º–∏ —Ñ–æ—Ä–º–∞–º–∏ —Å–ª–æ–≤–∞
        # –î–ª—è –∫–∞–∂–¥–æ–π —Ñ–æ—Ä–º—ã: lowercase –∏ capitalized
        like_conditions = []
        like_params = []

        for word_form in word_forms:
            form_lower = word_form.lower()
            form_cap = word_form.capitalize()
            form_upper = word_form.upper()
            like_conditions.append("title LIKE ? OR title LIKE ? OR title LIKE ?")
            like_conditions.append("description LIKE ? OR description LIKE ? OR description LIKE ?")
            like_conditions.append("full_text LIKE ? OR full_text LIKE ? OR full_text LIKE ?")
            like_params.extend([f'%{form_lower}%', f'%{form_cap}%', f'%{form_upper}%'] * 3)

        sql_query = f'''
            SELECT id, title, description, link, source, published, embedding, full_text
            FROM news
            WHERE {' OR '.join(like_conditions)}
        '''

        cursor.execute(sql_query, like_params)
        rows = cursor.fetchall()

        for row in rows:
            news_id = row[0]
            title = row[1] or ''
            description = row[2] or ''
            full_text = row[7] or ''

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ —Ö–æ—Ç—è –±—ã –æ–¥–Ω–∞ —Ñ–æ—Ä–º–∞ —Å–ª–æ–≤–∞ –µ—Å—Ç—å –∫–∞–∫ —Ü–µ–ª–æ–µ —Å–ª–æ–≤–æ
            found = False
            for word_form in word_forms:
                if (word_in_text(word_form, title) or
                    word_in_text(word_form, description) or
                    word_in_text(word_form, full_text)):
                    found = True
                    break

            if not found:
                continue  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –µ—Å–ª–∏ –Ω–∏ –æ–¥–Ω–∞ —Ñ–æ—Ä–º–∞ —Å–ª–æ–≤–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞

            if news_id not in keyword_results:
                keyword_results[news_id] = {
                    'id': news_id,
                    'title': title,
                    'description': description,
                    'link': row[3],
                    'source': row[4],
                    'published': row[5],
                    'embedding': np.frombuffer(row[6], dtype=np.float32),
                    'keyword_score': 0
                }

            # –ü–û–ó–ò–¶–ò–û–ù–ù–´–ô –í–ï–° —Å NER-–±—É—Å—Ç–æ–º
            position_weight = 0

            # –§—Ä–∞–∑—ã (2+ —Å–ª–æ–≤–∞) –ø–æ–ª—É—á–∞—é—Ç –ø–æ–≤—ã—à–µ–Ω–Ω—ã–π –≤–µ—Å
            is_phrase = ' ' in keyword
            title_weight = 10.0 if is_phrase else 5.0
            desc_weight = 3.0 if is_phrase else 1.5
            text_weight = 2.0 if is_phrase else 1.0

            # NER-–±—É—Å—Ç: –µ—Å–ª–∏ –∫–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ —è–≤–ª—è–µ—Ç—Å—è NER-—Å—É—â–Ω–æ—Å—Ç—å—é –∏–∑ –∑–∞–ø—Ä–æ—Å–∞
            is_ner_entity = keyword.lower() in query_ner_normalized
            ner_multiplier = 5.0 if is_ner_entity else 1.0

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –ª—é–±–æ–π –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–π —Ñ–æ—Ä–º—ã —Å–ª–æ–≤–∞
            found_in_title = any(word_in_text(form, title) for form in word_forms)
            found_in_description = any(word_in_text(form, description) for form in word_forms)
            found_in_full_text = any(word_in_text(form, full_text) for form in word_forms)

            if found_in_title:
                position_weight += title_weight * ner_multiplier  # NER –≤ –∑–∞–≥–æ–ª–æ–≤–∫–µ: x5
            if found_in_description:
                position_weight += desc_weight * ner_multiplier  # NER –≤ –æ–ø–∏—Å–∞–Ω–∏–∏: x5
            if found_in_full_text:
                position_weight += text_weight * ner_multiplier  # NER –≤ —Ç–µ–∫—Å—Ç–µ: x5

            keyword_results[news_id]['keyword_score'] += position_weight

    # –ë—É—Å—Ç –∑–∞ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –≤ –∑–∞–≥–æ–ª–æ–≤–∫–µ
    for news_id, data in keyword_results.items():
        title = data['title']
        # –°—á–∏—Ç–∞–µ–º —Å–∫–æ–ª—å–∫–æ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –∏–∑ –∑–∞–ø—Ä–æ—Å–∞ –µ—Å—Ç—å –≤ –∑–∞–≥–æ–ª–æ–≤–∫–µ (—Å —É—á–µ—Ç–æ–º –º–æ—Ä—Ñ. —Ñ–æ—Ä–º)
        matched_in_title = 0
        for kw in keywords:
            kw_forms = get_word_forms(kw)
            if any(word_in_text(form, title) for form in kw_forms):
                matched_in_title += 1

        if matched_in_title >= 2:
            # 2 —Å–ª–æ–≤–∞ -> x1.3, 3 —Å–ª–æ–≤–∞ -> x1.5, 4+ —Å–ª–æ–≤ -> x1.7
            multi_match_boost = 1.0 + (matched_in_title - 1) * 0.3
            data['keyword_score'] *= multi_match_boost

    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π NER-–±—É—Å—Ç: –ø—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Å —Ä–µ–∞–ª—å–Ω—ã–º–∏ NER-—Å—É—â–Ω–æ—Å—Ç—è–º–∏ –∏–∑ –ë–î
    if query_ner_normalized:
        for news_id in keyword_results.keys():
            cursor.execute('''
                SELECT COUNT(DISTINCT normalized_text)
                FROM entities
                WHERE news_id = ? AND LOWER(normalized_text) IN ({})
            '''.format(','.join('?' * len(query_ner_normalized))),
            [news_id] + list(query_ner_normalized))

            ner_matches = cursor.fetchone()[0]
            if ner_matches > 0:
                # –ö–∞–∂–¥–∞—è —Å–æ–≤–ø–∞–≤—à–∞—è NER-—Å—É—â–Ω–æ—Å—Ç—å –¥–∞–µ—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –±—É—Å—Ç x1.4
                ner_match_boost = 1.0 + (ner_matches * 0.4)
                keyword_results[news_id]['keyword_score'] *= ner_match_boost
                keyword_results[news_id]['ner_matches'] = ner_matches

    # DEBUG: print(f"DEBUG: Total keyword_results: {len(keyword_results)}")

    # –í–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫
    query_embedding = rag.get_embedding(query)
    print(f"DEBUG: query_embedding shape: {query_embedding.shape if query_embedding is not None else None}")
    vector_results = {}

    if query_embedding is not None:
        cursor.execute('SELECT id, title, description, link, source, published, embedding FROM news')

        for row in cursor.fetchall():
            news_id, title, description, link, source, published, embedding_blob = row
            news_embedding = np.frombuffer(embedding_blob, dtype=np.float32)

            similarity = np.dot(query_embedding, news_embedding) / (
                np.linalg.norm(query_embedding) * np.linalg.norm(news_embedding)
            )

            vector_results[news_id] = {
                'id': news_id,
                'title': title,
                'description': description,
                'link': link,
                'source': source,
                'published': published,
                'vector_score': float(similarity)
            }

    conn.close()

    # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Å –±–∞–Ω–∫–æ–≤—Å–∫–æ–π —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å—é
    combined_results = {}
    max_keyword_score = max([r['keyword_score'] for r in keyword_results.values()]) if keyword_results else 1
    # DEBUG: print(f"DEBUG: max_keyword_score={max_keyword_score}")

    for news_id, data in keyword_results.items():
        bank_relevance = calculate_banking_relevance(data['title'], data['description'])

        combined_results[news_id] = {
            'id': data['id'],
            'title': data['title'],
            'description': data['description'],
            'link': data['link'],
            'source': data['source'],
            'published': data['published'],
            'keyword_score': data['keyword_score'] / max_keyword_score if max_keyword_score > 0 else 0,
            'vector_score': vector_results.get(news_id, {}).get('vector_score', 0),
            'bank_boost': bank_relevance['boost'],
            'critical_keywords': bank_relevance['critical_matches'],
            'is_excluded': bank_relevance['exclude_matches'] > 0
        }

    # DEBUG: After keyword_results logging disabled

    # –ù–ï –¥–æ–±–∞–≤–ª—è–µ–º vector-only results - –æ–Ω–∏ —Ä–∞–∑–±–∞–≤–ª—è—é—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—É—é –≤—ã–¥–∞—á—É
    # for news_id, data in vector_results.items():
    #     if news_id not in combined_results:
    #         bank_relevance = calculate_banking_relevance(data['title'], data['description'])
    #
    #         combined_results[news_id] = {
    #             'id': data['id'],
    #             'title': data['title'],
    #             'description': data['description'],
    #             'link': data['link'],
    #             'source': data['source'],
    #             'published': data['published'],
    #             'keyword_score': 0,
    #             'vector_score': data['vector_score'],
    #             'bank_boost': bank_relevance['boost'],
    #             'critical_keywords': bank_relevance['critical_matches'],
    #             'is_excluded': bank_relevance['exclude_matches'] > 0
    #         }

    # –§–∏–Ω–∞–ª—å–Ω—ã–π score —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–º —Å–≤–µ–∂–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π
    for data in combined_results.values():
        if data['keyword_score'] > 0:
            # Keyword match –ø–æ–ª—É—á–∞–µ—Ç –±–æ–ª—å—à–∏–π –≤–µ—Å (85%)
            base_score = 0.85 * data['keyword_score'] + 0.15 * data['vector_score']
        else:
            # –¢–æ–ª—å–∫–æ vector search
            base_score = data['vector_score']

        # –ü—Ä–∏–º–µ–Ω—è–µ–º recency boost - —Å–≤–µ–∂–∏–µ –Ω–æ–≤–æ—Å—Ç–∏ –ø–æ–ª—É—á–∞—é—Ç –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç
        recency_boost = calculate_recency_boost(data.get('published', ''))
        final_score = base_score * recency_boost

        data['similarity'] = final_score
        data['recency_boost'] = recency_boost
        data['geo_boost'] = 1.0
        data['war_penalty'] = 1.0

    results = sorted(combined_results.values(), key=lambda x: x['similarity'], reverse=True)

    # DEBUG: Top 10 results logging disabled

    # –§–ò–õ–¨–¢–†–ê–¶–ò–Ø –û–¢–ö–õ–Æ–ß–ï–ù–ê - –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –≤—Å–µ –Ω–æ–≤–æ—Å—Ç–∏
    return results[:top_k]

# Background task –¥–ª—è –ø–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–æ–≥–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è
async def periodic_update():
    """–ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–µ–π (–∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è)"""
    # –ñ–¥–µ–º 5 –º–∏–Ω—É—Ç –ø–µ—Ä–µ–¥ –ø–µ—Ä–≤—ã–º –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ–º, —á—Ç–æ–±—ã –Ω–µ –±–ª–æ–∫–∏—Ä–æ–≤–∞—Ç—å —Å–µ—Ä–≤–∏—Å –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ
    await asyncio.sleep(300)  # 5 –º–∏–Ω—É—Ç

    while True:
        try:
            print(f"\n[{datetime.now().strftime('%H:%M:%S')}] üîÑ –ù–∞—á–∏–Ω–∞—é –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–µ–π...")
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—É—é –≤–µ—Ä—Å–∏—é - –Ω–µ –±–ª–æ–∫–∏—Ä—É–µ—Ç API!
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤—Å–µ –Ω–æ–≤–æ—Å—Ç–∏ –∏–∑ RSS, –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è –æ—Ç—Ñ–∏–ª—å—Ç—Ä—É–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ
            new_count = await rag.fetch_and_index_news_async(limit_per_source=50, max_concurrent=5)
            print(f"[{datetime.now().strftime('%H:%M:%S')}] ‚úÖ –î–æ–±–∞–≤–ª–µ–Ω–æ {new_count} –Ω–æ–≤—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π")
        except Exception as e:
            print(f"[{datetime.now().strftime('%H:%M:%S')}] ‚ùå –û—à–∏–±–∫–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è: {e}")

        await asyncio.sleep(UPDATE_INTERVAL_SECONDS)

@app.on_event("startup")
async def startup_event():
    """–ó–∞–ø—É—Å–∫ —Ñ–æ–Ω–æ–≤–æ–≥–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è"""
    asyncio.create_task(periodic_update())
    print("=" * 70)
    print("üöÄ News Collector Service –∑–∞–ø—É—â–µ–Ω")
    print(f"üì° API: http://localhost:8001")
    print(f"üìñ Docs: http://localhost:8001/docs")
    print(f"üîÑ –ê–≤—Ç–æ–æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∫–∞–∂–¥—ã–µ {UPDATE_INTERVAL_SECONDS // 60} –º–∏–Ω—É—Ç (–ø–µ—Ä–≤–æ–µ —á–µ—Ä–µ–∑ 5 –º–∏–Ω)")
    print(f"üì∞ –ò—Å—Ç–æ—á–Ω–∏–∫–æ–≤: {len(rag.sources) if hasattr(rag, 'sources') else '255'} (–≤–∫–ª. GDELT)")
    print("=" * 70)

@app.get("/")
async def root():
    return {
        "service": "News Collector API",
        "version": "1.0",
        "status": "running",
        "docs": "/docs"
    }

@app.get("/health")
async def health():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–¥–æ—Ä–æ–≤—å—è —Å–µ—Ä–≤–∏—Å–∞"""
    stats = rag.get_stats()
    return {
        "status": "healthy",
        "total_news": stats['total'],
        "timestamp": datetime.now().isoformat()
    }

@app.get("/stats", response_model=StatsResponse)
async def get_stats():
    """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –±–∞–∑—ã"""
    stats = rag.get_stats()
    return StatsResponse(
        total_news=stats['total'],
        by_source=stats['by_source'],
        by_category=stats['by_category'],
        last_update=datetime.now().isoformat()
    )

def get_news_entities_tags(news_id: int) -> List[EntityTag]:
    """–ü–æ–ª—É—á–∏—Ç—å NER-—Ç–µ–≥–∏ –¥–ª—è –Ω–æ–≤–æ—Å—Ç–∏"""
    import sqlite3
    try:
        conn = sqlite3.connect(rag.db_path)
        cursor = conn.cursor()

        cursor.execute('''
            SELECT entity_text, entity_type, is_banking
            FROM entities
            WHERE news_id = ?
            ORDER BY position
        ''', (news_id,))

        entities = []
        for row in cursor.fetchall():
            entities.append(EntityTag(
                text=row[0],
                type=row[1],
                is_banking=bool(row[2])
            ))

        conn.close()
        return entities
    except Exception as e:
        print(f"Error loading entities for news {news_id}: {e}")
        return []

@app.post("/search", response_model=SearchResponse)
async def search_news(request: SearchRequest):
    """
    –ü–æ–∏—Å–∫ –Ω–æ–≤–æ—Å—Ç–µ–π (–≥–∏–±—Ä–∏–¥–Ω—ã–π: —Ç–µ–∫—Å—Ç + –≤–µ–∫—Ç–æ—Ä–Ω—ã–π + –±–∞–Ω–∫–æ–≤—Å–∫–∞—è –ø—Ä–∏–æ—Ä–∏—Ç–µ–∑–∞—Ü–∏—è)
    """
    try:
        results = hybrid_search_internal(request.query, request.top_k)

        news_items = []
        for item in results:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º NER-—Å—É—â–Ω–æ—Å—Ç–∏ –¥–ª—è –∫–∞–∂–¥–æ–π –Ω–æ–≤–æ—Å—Ç–∏
            entities = get_news_entities_tags(item['id'])

            news_items.append(NewsItem(
                id=item['id'],
                title=item['title'],
                description=item['description'],
                link=item['link'],
                source=item['source'],
                published=item['published'],
                similarity=item['similarity'],
                keyword_score=item.get('keyword_score', 0),
                vector_score=item.get('vector_score', 0),
                bank_boost=item.get('bank_boost', 1.0),
                critical_keywords=item.get('critical_keywords', 0),
                geo_boost=item.get('geo_boost', 1.0),
                entities=entities
            ))

        return SearchResponse(
            query=request.query,
            total_found=len(news_items),
            news=news_items,
            timestamp=datetime.now().isoformat()
        )

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Search error: {str(e)}")

@app.post("/update")
async def trigger_update(background_tasks: BackgroundTasks):
    """–ó–∞–ø—É—Å—Ç–∏—Ç—å –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–µ–π –≤—Ä—É—á–Ω—É—é (–∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è)"""
    async def update_news():
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—É—é –≤–µ—Ä—Å–∏—é - –Ω–µ –±–ª–æ–∫–∏—Ä—É–µ—Ç API!
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤—Å–µ –Ω–æ–≤–æ—Å—Ç–∏ –∏–∑ RSS, –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è –æ—Ç—Ñ–∏–ª—å—Ç—Ä—É–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ
            new_count = await rag.fetch_and_index_news_async(limit_per_source=50, max_concurrent=5)
            print(f"‚úÖ –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ: {new_count} –Ω–æ–≤—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π")
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è: {e}")

    background_tasks.add_task(update_news)
    return {"status": "update_started", "message": "–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∑–∞–ø—É—â–µ–Ω–æ –≤ —Ñ–æ–Ω–µ (async)"}

@app.get("/entities/search/{entity_text}")
async def search_by_entity(entity_text: str, limit: int = 20):
    """
    –ù–∞–π—Ç–∏ –Ω–æ–≤–æ—Å—Ç–∏, —Å–æ–¥–µ—Ä–∂–∞—â–∏–µ —É–∫–∞–∑–∞–Ω–Ω—É—é NER-—Å—É—â–Ω–æ—Å—Ç—å
    """
    try:
        import sqlite3
        conn = sqlite3.connect(rag.db_path)
        cursor = conn.cursor()

        cursor.execute('''
            SELECT DISTINCT n.id, n.title, n.description, n.link, n.source, n.published
            FROM news n
            INNER JOIN entities e ON n.id = e.news_id
            WHERE e.entity_text LIKE ?
            ORDER BY n.published DESC
            LIMIT ?
        ''', (f'%{entity_text}%', limit))

        news_items = []
        for row in cursor.fetchall():
            news_items.append({
                'id': row[0],
                'title': row[1],
                'description': row[2],
                'link': row[3],
                'source': row[4],
                'published': row[5]
            })

        conn.close()

        return {
            'entity': entity_text,
            'total_found': len(news_items),
            'news': news_items
        }

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error searching by entity: {str(e)}")

@app.get("/entities/stats")
async def get_entities_stats():
    """
    –ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ NER-—Å—É—â–Ω–æ—Å—Ç—è–º
    """
    try:
        import sqlite3
        conn = sqlite3.connect(rag.db_path)
        cursor = conn.cursor()

        # –¢–æ–ø –ø–µ—Ä—Å–æ–Ω (–≥—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–π —Ñ–æ—Ä–º–µ)
        cursor.execute('''
            SELECT normalized_text, COUNT(DISTINCT news_id) as count
            FROM entities
            WHERE entity_type = 'person' AND normalized_text IS NOT NULL
            GROUP BY normalized_text
            ORDER BY count DESC
            LIMIT 10
        ''')
        top_persons = [{'name': row[0], 'count': row[1]} for row in cursor.fetchall()]

        # –¢–æ–ø –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π (–∏—Å–∫–ª—é—á–∞–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –°–ú–ò, –≥—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–π —Ñ–æ—Ä–º–µ)
        cursor.execute('''
            SELECT e.normalized_text, COUNT(DISTINCT e.news_id) as count
            FROM entities e
            INNER JOIN news n ON e.news_id = n.id
            WHERE e.entity_type = 'organization'
            AND e.normalized_text IS NOT NULL
            AND LOWER(e.entity_text) NOT LIKE '%' || LOWER(n.source) || '%'
            AND LOWER(n.source) NOT LIKE '%' || LOWER(e.entity_text) || '%'
            GROUP BY e.normalized_text
            ORDER BY count DESC
            LIMIT 10
        ''')
        top_organizations = [{'name': row[0], 'count': row[1]} for row in cursor.fetchall()]

        # –¢–æ–ø –ª–æ–∫–∞—Ü–∏–π (–≥—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–π —Ñ–æ—Ä–º–µ)
        cursor.execute('''
            SELECT normalized_text, COUNT(DISTINCT news_id) as count
            FROM entities
            WHERE entity_type = 'location' AND normalized_text IS NOT NULL
            GROUP BY normalized_text
            ORDER BY count DESC
            LIMIT 10
        ''')
        top_locations = [{'name': row[0], 'count': row[1]} for row in cursor.fetchall()]

        # –ë–∞–Ω–∫–æ–≤—Å–∫–∏–µ —Å—É—â–Ω–æ—Å—Ç–∏ (–∏—Å–∫–ª—é—á–∞–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –°–ú–ò, –≥—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–π —Ñ–æ—Ä–º–µ)
        cursor.execute('''
            SELECT e.normalized_text, COUNT(DISTINCT e.news_id) as count
            FROM entities e
            INNER JOIN news n ON e.news_id = n.id
            WHERE e.is_banking = 1
            AND e.normalized_text IS NOT NULL
            AND LOWER(e.entity_text) NOT LIKE '%' || LOWER(n.source) || '%'
            AND LOWER(n.source) NOT LIKE '%' || LOWER(e.entity_text) || '%'
            GROUP BY e.normalized_text
            ORDER BY count DESC
            LIMIT 10
        ''')
        banking_entities = [{'name': row[0], 'count': row[1]} for row in cursor.fetchall()]

        # –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        cursor.execute('SELECT COUNT(DISTINCT entity_text) FROM entities')
        total_unique_entities = cursor.fetchone()[0]

        cursor.execute('SELECT COUNT(*) FROM entities')
        total_entity_mentions = cursor.fetchone()[0]

        conn.close()

        return {
            'total_unique_entities': total_unique_entities,
            'total_mentions': total_entity_mentions,
            'top_persons': top_persons,
            'top_organizations': top_organizations,
            'top_locations': top_locations,
            'banking_entities': banking_entities
        }

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error fetching entity stats: {str(e)}")

@app.get("/entities/id/{news_id}")
async def get_news_entities(news_id: int):
    """
    –ü–æ–ª—É—á–∏—Ç—å –≤—Å–µ NER-—Å—É—â–Ω–æ—Å—Ç–∏ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –Ω–æ–≤–æ—Å—Ç–∏
    """
    try:
        import sqlite3
        conn = sqlite3.connect(rag.db_path)
        cursor = conn.cursor()

        cursor.execute('''
            SELECT entity_text, entity_type, is_banking
            FROM entities
            WHERE news_id = ?
            ORDER BY position
        ''', (news_id,))

        entities = []
        for row in cursor.fetchall():
            entities.append({
                'text': row[0],
                'type': row[1],
                'is_banking': bool(row[2])
            })

        conn.close()

        return {
            'news_id': news_id,
            'entities': entities,
            'total': len(entities)
        }

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error fetching entities: {str(e)}")

@app.get("/entities/trends")
async def get_entity_trends(days: int = 30, entity_type: Optional[str] = None, top_n: int = 10):
    """
    –ü–æ–ª—É—á–∏—Ç—å —Ç—Ä–µ–Ω–¥—ã —É–ø–æ–º–∏–Ω–∞–Ω–∏—è NER-—Å—É—â–Ω–æ—Å—Ç–µ–π –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ N –¥–Ω–µ–π

    Args:
        days: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–Ω–µ–π –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 30)
        entity_type: —Ñ–∏–ª—å—Ç—Ä –ø–æ —Ç–∏–ø—É —Å—É—â–Ω–æ—Å—Ç–∏ (person/organization/location)
        top_n: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–ø —Å—É—â–Ω–æ—Å—Ç–µ–π –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
    """
    try:
        import sqlite3
        from datetime import datetime, timedelta
        from email.utils import parsedate_to_datetime
        from collections import defaultdict

        conn = sqlite3.connect(rag.db_path)
        cursor = conn.cursor()

        # –í—ã—á–∏—Å–ª—è–µ–º –¥–∞—Ç—É –æ—Ç—Å–µ—á–∫–∏ (–¥–µ–ª–∞–µ–º timezone-aware)
        from datetime import timezone
        cutoff_date = datetime.now(timezone.utc) - timedelta(days=days)

        # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –Ω–æ–≤–æ—Å—Ç–∏ —Å —Å—É—â–Ω–æ—Å—Ç—è–º–∏ (–∏—Å–∫–ª—é—á–∞–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –Ω–æ–≤–æ—Å—Ç–µ–π, –≥—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–π —Ñ–æ—Ä–º–µ)
        type_filter = f"AND e.entity_type = '{entity_type}'" if entity_type else ""

        cursor.execute(f'''
            SELECT e.normalized_text, n.published, e.news_id, n.source
            FROM entities e
            INNER JOIN news n ON e.news_id = n.id
            WHERE e.normalized_text IS NOT NULL
            {type_filter}
        ''')

        # –ü–∞—Ä—Å–∏–º –¥–∞—Ç—ã –∏ —Ñ–∏–ª—å—Ç—Ä—É–µ–º
        entity_mentions = defaultdict(lambda: defaultdict(set))

        for row in cursor.fetchall():
            normalized_text, published, news_id, source = row

            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –µ—Å–ª–∏ —Å—É—â–Ω–æ—Å—Ç—å —Å–æ–≤–ø–∞–¥–∞–µ—Ç —Å –∏—Å—Ç–æ—á–Ω–∏–∫–æ–º (—ç—Ç–æ –Ω–∞–∑–≤–∞–Ω–∏–µ –°–ú–ò, –∞ –Ω–µ —É–ø–æ–º–∏–Ω–∞–Ω–∏–µ)
            if normalized_text.lower() in source.lower() or source.lower() in normalized_text.lower():
                continue

            try:
                # –ü–∞—Ä—Å–∏–º –¥–∞—Ç—É –∏–∑ RFC 2822 —Ñ–æ—Ä–º–∞—Ç–∞
                pub_date = parsedate_to_datetime(published)

                # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ –¥–∞—Ç–µ
                if pub_date >= cutoff_date:
                    date_str = pub_date.strftime('%Y-%m-%d')
                    entity_mentions[normalized_text][date_str].add(news_id)
            except:
                continue

        # –ü–æ–ª—É—á–∞–µ–º —Ç–æ–ø —Å—É—â–Ω–æ—Å—Ç–µ–π –ø–æ –æ–±—â–µ–º—É –∫–æ–ª–∏—á–µ—Å—Ç–≤—É —É–ø–æ–º–∏–Ω–∞–Ω–∏–π
        entity_totals = {}
        for entity, dates_dict in entity_mentions.items():
            total = sum(len(news_ids) for news_ids in dates_dict.values())
            entity_totals[entity] = total

        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –∏ –±–µ—Ä–µ–º —Ç–æ–ø N
        top_entities = sorted(entity_totals.items(), key=lambda x: x[1], reverse=True)[:top_n]
        top_entity_names = [entity for entity, _ in top_entities]

        # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –¥–∞—Ç –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ
        dates = []
        current_date = cutoff_date.date()
        end_date = datetime.now().date()

        while current_date <= end_date:
            dates.append(current_date.strftime('%Y-%m-%d'))
            current_date += timedelta(days=1)

        # –§–æ—Ä–º–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –≥—Ä–∞—Ñ–∏–∫–∞
        datasets = []
        for entity in top_entity_names:
            data = [len(entity_mentions[entity].get(date, set())) for date in dates]
            datasets.append({
                'label': entity,
                'data': data
            })

        conn.close()

        return {
            'dates': dates,
            'datasets': datasets,
            'period_days': days,
            'entity_type': entity_type or 'all',
            'top_n': top_n
        }

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error fetching entity trends: {str(e)}")

if __name__ == "__main__":
    uvicorn.run(
        app,
        host="0.0.0.0",
        port=8001,
        log_level="info"
    )
