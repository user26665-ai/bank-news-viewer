#!/usr/bin/env python3
"""
News Collector Service (—Å –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–æ–º)
- –ó–∞–≥—Ä—É–∂–∞–µ—Ç RSS —Å –ø–µ—Ä–∏–æ–¥–∏—á–Ω–æ—Å—Ç—å—é
- –°–æ–∑–¥–∞–µ—Ç embeddings
- –ü—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç REST API –¥–ª—è –ø–æ–∏—Å–∫–∞ –Ω–æ–≤–æ—Å—Ç–µ–π

–ó–∞–ø—É—Å–∫: python3 news_collector_service.py
API: http://localhost:8001
"""

from fastapi import FastAPI, HTTPException, BackgroundTasks
from fastapi.responses import JSONResponse
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import List, Optional, Dict
import uvicorn
import asyncio
from datetime import datetime, timedelta
import json
import sqlite3

from news_rag_system import NewsRAGSystem
import os

app = FastAPI(title="News Collector API", version="1.0")

# –î–æ–±–∞–≤–ª—è–µ–º CORS middleware –¥–ª—è –≤–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # –†–∞–∑—Ä–µ—à–∞–µ–º –≤—Å–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ (–¥–ª—è –ª–æ–∫–∞–ª—å–Ω–æ–π —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏)
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä RAG —Å–∏—Å—Ç–µ–º—ã (—Å LTR –µ—Å–ª–∏ –º–æ–¥–µ–ª—å —Å—É—â–µ—Å—Ç–≤—É–µ—Ç)
LTR_MODEL_PATH = "/Users/david/bank_news_agent/ltr_model.pkl"

if os.path.exists(LTR_MODEL_PATH):
    from integrate_ltr_model import LTRNewsRAGSystem
    rag = LTRNewsRAGSystem(ltr_model_path=LTR_MODEL_PATH)
    print("‚úì –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è LTR-—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ")
else:
    rag = NewsRAGSystem()
    print("‚Ñπ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –ø–æ–∏—Å–∫ (LTR –º–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞)")

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
UPDATE_INTERVAL_SECONDS = 3600  # 1 —á–∞—Å

class SearchRequest(BaseModel):
    query: str
    top_k: int = 20
    category: Optional[str] = None

class EntityTag(BaseModel):
    text: str
    type: str
    is_banking: bool = False

class NewsItem(BaseModel):
    id: int
    title: str
    description: str
    link: str
    source: str
    published: str
    similarity: float
    keyword_score: Optional[float] = 0
    vector_score: Optional[float] = 0
    bank_boost: Optional[float] = 1.0
    critical_keywords: Optional[int] = 0
    geo_boost: Optional[float] = 1.0
    ltr_score: Optional[float] = None  # LTR-—Å–∫–æ—Ä –µ—Å–ª–∏ –º–æ–¥–µ–ª—å –¥–æ—Å—Ç—É–ø–Ω–∞
    entities: Optional[List[EntityTag]] = []

class SearchResponse(BaseModel):
    query: str
    total_found: int
    news: List[NewsItem]
    timestamp: str

class StatsResponse(BaseModel):
    total_news: int
    by_source: dict
    by_category: dict
    last_update: str

# –ó–∞–≥—Ä—É–∑–∫–∞ –±–∞–Ω–∫–æ–≤—Å–∫–∏—Ö –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
def load_bank_keywords():
    try:
        with open("/Users/david/bank_news_agent/news_sources.json", 'r', encoding='utf-8') as f:
            data = json.load(f)
            return data.get('bank_keywords', {})
    except:
        return {"critical": [], "high": [], "exclude": []}

def calculate_banking_relevance(title: str, description: str) -> dict:
    """–í—ã—á–∏—Å–ª–∏—Ç—å —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –Ω–æ–≤–æ—Å—Ç–∏ –¥–ª—è –±–∞–Ω–∫–∞ (–û–¢–ö–õ–Æ–ß–ï–ù–û)"""
    keywords = load_bank_keywords()
    text = f"{title} {description}".lower()

    score = {
        'critical_matches': 0,
        'high_matches': 0,
        'exclude_matches': 0,
        'boost': 1.0  # –í–°–ï –Ω–æ–≤–æ—Å—Ç–∏ –ø–æ–ª—É—á–∞—é—Ç –Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π –±—É—Å—Ç
    }

    # –°—á–∏—Ç–∞–µ–º –¥–ª—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏, –Ω–æ –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º –¥–ª—è –±—É—Å—Ç–∞
    for keyword in keywords.get('critical', []):
        if keyword.lower() in text:
            score['critical_matches'] += 1

    for keyword in keywords.get('high', []):
        if keyword.lower() in text:
            score['high_matches'] += 1

    for keyword in keywords.get('exclude', []):
        if keyword.lower() in text:
            score['exclude_matches'] += 1

    # –ü–†–ò–û–†–ò–¢–ï–ó–ê–¶–ò–Ø –û–¢–ö–õ–Æ–ß–ï–ù–ê - –≤—Å–µ –Ω–æ–≤–æ—Å—Ç–∏ —Ä–∞–≤–Ω—ã
    score['boost'] = 1.0

    return score

def expand_query(query: str) -> list:
    """–†–∞—Å—à–∏—Ä–µ–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞ —Å–∏–Ω–æ–Ω–∏–º–∞–º–∏ –∏ –≤–∞—Ä–∏–∞–Ω—Ç–∞–º–∏ (—Ñ—Ä–∞–∑–æ–≤–æ–µ + —Å–ª–æ–≤–∞—Ä–Ω–æ–µ)"""

    # –§—Ä–∞–∑–æ–≤—ã–µ —Å–∏–Ω–æ–Ω–∏–º—ã (–æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—é—Ç—Å—è –ü–ï–†–ï–î —Ä–∞–∑–±–∏–µ–Ω–∏–µ–º –Ω–∞ —Å–ª–æ–≤–∞)
    phrase_synonyms = {
        '—Å—Ç–∞–≤–∫–∞ —Ü–±': ['–∫–ª—é—á–µ–≤–∞—è —Å—Ç–∞–≤–∫–∞', '—Å—Ç–∞–≤–∫–∞ —Ü–±', '—Å—Ç–∞–≤–∫–∞ —Ü–µ–Ω—Ç—Ä–æ–±–∞–Ω–∫–∞', '—Å—Ç–∞–≤–∫–∞ –±–∞–Ω–∫–∞ —Ä–æ—Å—Å–∏–∏'],
        '–∫–ª—é—á–µ–≤–∞—è —Å—Ç–∞–≤–∫–∞': ['–∫–ª—é—á–µ–≤–∞—è —Å—Ç–∞–≤–∫–∞', '—Å—Ç–∞–≤–∫–∞ —Ü–±', '—Å—Ç–∞–≤–∫–∞ —Ü–µ–Ω—Ç—Ä–æ–±–∞–Ω–∫–∞'],
        '–∫—É—Ä—Å —Ä—É–±–ª—è': ['–∫—É—Ä—Å —Ä—É–±–ª—è', '–∫—É—Ä—Å –¥–æ–ª–ª–∞—Ä–∞', '—Ä—É–±–ª—å –¥–æ–ª–ª–∞—Ä', '–≤–∞–ª—é—Ç–Ω—ã–π –∫—É—Ä—Å'],
        '–∫—É—Ä—Å –¥–æ–ª–ª–∞—Ä–∞': ['–∫—É—Ä—Å –¥–æ–ª–ª–∞—Ä–∞', '–∫—É—Ä—Å —Ä—É–±–ª—è', '–¥–æ–ª–ª–∞—Ä —Ä—É–±–ª—å', '–≤–∞–ª—é—Ç–Ω—ã–π –∫—É—Ä—Å'],
    }

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ—Ä–∞–∑—ã
    query_lower = query.lower()
    for phrase, variants in phrase_synonyms.items():
        if phrase in query_lower:
            # –ï—Å–ª–∏ –Ω–∞—à–ª–∏ —Ñ—Ä–∞–∑—É - –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ª—å–∫–æ –µ—ë –≤–∞—Ä–∏–∞–Ω—Ç—ã
            return variants

    # –°–ª–æ–≤–∞—Ä—å —Å–∏–Ω–æ–Ω–∏–º–æ–≤ –¥–ª—è –æ—Ç–¥–µ–ª—å–Ω—ã—Ö —Å–ª–æ–≤ (–µ—Å–ª–∏ —Ñ—Ä–∞–∑—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã)
    synonyms = {
        '–ª—É–∫–æ–π–ª': ['–ª—É–∫–æ–π–ª', 'lukoil'],
        '—Ä–æ—Å–Ω–µ—Ñ—Ç—å': ['—Ä–æ—Å–Ω–µ—Ñ—Ç—å', 'rosneft'],
        '–≥–∞–∑–ø—Ä–æ–º': ['–≥–∞–∑–ø—Ä–æ–º', 'gazprom'],
        '—Å–±–µ—Ä–±–∞–Ω–∫': ['—Å–±–µ—Ä–±–∞–Ω–∫', 'sberbank', '—Å–±–µ—Ä'],
        '–≤—Ç–±': ['–≤—Ç–±', 'vtb'],
        '—Å–∞–Ω–∫—Ü–∏–∏': ['—Å–∞–Ω–∫—Ü–∏–∏', '–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è'],
        '—Ü–±': ['—Ü–±', '—Ü–µ–Ω—Ç—Ä–æ–±–∞–Ω–∫', '–±–∞–Ω–∫ —Ä–æ—Å—Å–∏–∏'],
        '—Ä—É–±–ª—å': ['—Ä—É–±–ª—å', '—Ä—É–±–ª—è', '—Ä—É–±'],
        '–¥–æ–ª–ª–∞—Ä': ['–¥–æ–ª–ª–∞—Ä', 'usd'],
        '–µ–≤—Ä–æ': ['–µ–≤—Ä–æ', 'eur'],
    }

    words = query.lower().split()
    expanded = set(words)

    for word in words:
        for key, variants in synonyms.items():
            if word == key or key in word:
                expanded.update(variants)

    return list(expanded)

def calculate_recency_boost(published_date: str) -> float:
    """
    –í—ã—á–∏—Å–ª–∏—Ç—å –±—É—Å—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–≤–µ–∂–µ—Å—Ç–∏ –Ω–æ–≤–æ—Å—Ç–∏

    Args:
        published_date: –¥–∞—Ç–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ (ISO format string)

    Returns:
        –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –±—É—Å—Ç–∞ (1.0-1.3)
    """
    from datetime import datetime, timedelta
    from email.utils import parsedate_to_datetime

    try:
        # –ü–∞—Ä—Å–∏–º –¥–∞—Ç—É –ø—É–±–ª–∏–∫–∞—Ü–∏–∏
        if not published_date:
            return 1.0

        # –ü—Ä–æ–±—É–µ–º RFC 2822 —Ñ–æ—Ä–º–∞—Ç (–∏–∑ RSS feeds)
        try:
            pub_date = parsedate_to_datetime(published_date)
        except:
            # Fallback –Ω–∞ ISO —Ñ–æ—Ä–º–∞—Ç
            pub_date = datetime.fromisoformat(published_date.replace('Z', '+00:00'))

        now = datetime.now(pub_date.tzinfo) if pub_date.tzinfo else datetime.now()

        # –í—ã—á–∏—Å–ª—è–µ–º –≤–æ–∑—Ä–∞—Å—Ç –Ω–æ–≤–æ—Å—Ç–∏
        age = now - pub_date
        age_hours = age.total_seconds() / 3600

        # –ü—Ä–∏–º–µ–Ω—è–µ–º –±—É—Å—Ç –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –≤–æ–∑—Ä–∞—Å—Ç–∞
        if age_hours < 24:
            # –ü–æ—Å–ª–µ–¥–Ω–∏–µ 24 —á–∞—Å–∞ - –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –±—É—Å—Ç
            return 1.3
        elif age_hours < 72:
            # –ü–æ—Å–ª–µ–¥–Ω–∏–µ 3 –¥–Ω—è - —Å—Ä–µ–¥–Ω–∏–π –±—É—Å—Ç
            return 1.2
        elif age_hours < 168:
            # –ü–æ—Å–ª–µ–¥–Ω—è—è –Ω–µ–¥–µ–ª—è - –Ω–µ–±–æ–ª—å—à–æ–π –±—É—Å—Ç
            return 1.1
        elif age_hours < 720:
            # –ü–æ—Å–ª–µ–¥–Ω–∏–π –º–µ—Å—è—Ü - –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –±—É—Å—Ç
            return 1.05
        else:
            # –°—Ç–∞—Ä—à–µ –º–µ—Å—è—Ü–∞ - –±–µ–∑ –±—É—Å—Ç–∞
            return 1.0

    except Exception as e:
        # –í —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏ –ø–∞—Ä—Å–∏–Ω–≥–∞ - –±–µ–∑ –±—É—Å—Ç–∞
        return 1.0


def hybrid_search_internal(query: str, top_k: int = 20):
    """–ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫ —Å —É–ª—É—á—à–µ–Ω–∏—è–º–∏: –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã–π –≤–µ—Å, query expansion, NER-–±—É—Å—Ç, recency boost"""
    import sqlite3
    import numpy as np
    from news_ner import NewsNERExtractor

    # Query expansion
    expanded_keywords = expand_query(query)

    stop_words = {
        # –ü—Ä–µ–¥–ª–æ–≥–∏ –∏ —Å–æ—é–∑—ã
        '–ø—Ä–æ', '–æ', '–æ–±', '–≤', '–Ω–∞', '—Å', '–ø–æ', '–¥–ª—è', '–∫', '—É', '–∏–∑', '–æ—Ç', '–∏', '–∏–ª–∏', '–∞', '–Ω–æ', '–∑–∞', '–ø–µ—Ä–µ–¥', '–º–µ–∂–¥—É', '–ø–æ–¥', '–Ω–∞–¥',
        # –ö–æ–º–∞–Ω–¥—ã
        '–ø–æ–∫–∞–∂–∏', '–Ω–∞–π–¥–∏', '–¥–∞–π', '–∏—â–∏', '—Å–º–æ—Ç—Ä–∏',
    }
    keywords = [k for k in expanded_keywords if k not in stop_words and len(k) > 2]

    # –ò–∑–≤–ª–µ–∫–∞–µ–º NER-—Å—É—â–Ω–æ—Å—Ç–∏ –∏–∑ –∑–∞–ø—Ä–æ—Å–∞
    ner_extractor = NewsNERExtractor()
    query_entities = ner_extractor.extract_from_news(query, "")

    # –°–æ–∑–¥–∞–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–æ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö NER-—Å—É—â–Ω–æ—Å—Ç–µ–π –∏–∑ –∑–∞–ø—Ä–æ—Å–∞
    query_ner_normalized = set()
    for entity in query_entities['all']:
        normalized = entity.get('normalized', entity['text'])
        query_ner_normalized.add(normalized.lower())

    conn = sqlite3.connect(rag.db_path)
    cursor = conn.cursor()

    # –¢–µ–∫—Å—Ç–æ–≤—ã–π –ø–æ–∏—Å–∫ —Å –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã–º –≤–µ—Å–æ–º (–±–µ–∑ LOWER –¥–ª—è –ø–æ–¥–¥–µ—Ä–∂–∫–∏ —Ä—É—Å—Å–∫–∏—Ö –±—É–∫–≤)
    keyword_results = {}

    import re
    import pymorphy2

    morph = pymorphy2.MorphAnalyzer()

    def word_in_text(word: str, text: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∏—Ç—å —á—Ç–æ —Å–ª–æ–≤–æ –µ—Å—Ç—å –≤ —Ç–µ–∫—Å—Ç–µ –∫–∞–∫ —Ü–µ–ª–æ–µ —Å–ª–æ–≤–æ (–Ω–µ –ø–æ–¥—Å—Ç—Ä–æ–∫–∞)"""
        # –†–µ–≥—É–ª—è—Ä–Ω–æ–µ –≤—ã—Ä–∞–∂–µ–Ω–∏–µ: –≥—Ä–∞–Ω–∏—Ü—ã —Å–ª–æ–≤ –≤–æ–∫—Ä—É–≥ –∫–ª—é—á–µ–≤–æ–≥–æ —Å–ª–æ–≤–∞
        pattern = r'\b' + re.escape(word.lower()) + r'\b'
        return bool(re.search(pattern, text.lower(), re.UNICODE))

    def get_word_forms(word: str) -> set:
        """–ü–æ–ª—É—á–∏—Ç—å –≤—Å–µ —Ñ–æ—Ä–º—ã —Å–ª–æ–≤–∞ (–ü—É—Ç–∏–Ω, –ü—É—Ç–∏–Ω–∞, –ü—É—Ç–∏–Ω—É, etc.)"""
        forms = {word.lower()}  # –ë–∞–∑–æ–≤–∞—è —Ñ–æ—Ä–º–∞

        # –ü–∞—Ä—Å–∏–º —Å–ª–æ–≤–æ –∏ –ø–æ–ª—É—á–∞–µ–º –≤—Å–µ –µ–≥–æ —Ñ–æ—Ä–º—ã
        parsed = morph.parse(word)
        if parsed:
            lexeme = parsed[0].lexeme  # –í—Å–µ —Ñ–æ—Ä–º—ã —Å–ª–æ–≤–∞
            for form in lexeme:
                forms.add(form.word.lower())

        return forms

    for keyword in keywords:
        # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ —Ñ–æ—Ä–º—ã —Å–ª–æ–≤–∞ –∑–∞—Ä–∞–Ω–µ–µ
        word_forms = get_word_forms(keyword)

        # –°–æ–∑–¥–∞–µ–º SQL –∑–∞–ø—Ä–æ—Å —Å–æ –≤—Å–µ–º–∏ —Ñ–æ—Ä–º–∞–º–∏ —Å–ª–æ–≤–∞
        # –î–ª—è –∫–∞–∂–¥–æ–π —Ñ–æ—Ä–º—ã: lowercase –∏ capitalized
        like_conditions = []
        like_params = []

        for word_form in word_forms:
            form_lower = word_form.lower()
            form_cap = word_form.capitalize()
            form_upper = word_form.upper()
            like_conditions.append("title LIKE ? OR title LIKE ? OR title LIKE ?")
            like_conditions.append("description LIKE ? OR description LIKE ? OR description LIKE ?")
            like_conditions.append("full_text LIKE ? OR full_text LIKE ? OR full_text LIKE ?")
            like_params.extend([f'%{form_lower}%', f'%{form_cap}%', f'%{form_upper}%'] * 3)

        sql_query = f'''
            SELECT id, title, description, link, source, published, embedding, full_text
            FROM news
            WHERE {' OR '.join(like_conditions)}
        '''

        cursor.execute(sql_query, like_params)
        rows = cursor.fetchall()

        for row in rows:
            news_id = row[0]
            title = row[1] or ''
            description = row[2] or ''
            full_text = row[7] or ''

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ —Ö–æ—Ç—è –±—ã –æ–¥–Ω–∞ —Ñ–æ—Ä–º–∞ —Å–ª–æ–≤–∞ –µ—Å—Ç—å –∫–∞–∫ —Ü–µ–ª–æ–µ —Å–ª–æ–≤–æ
            found = False
            for word_form in word_forms:
                if (word_in_text(word_form, title) or
                    word_in_text(word_form, description) or
                    word_in_text(word_form, full_text)):
                    found = True
                    break

            if not found:
                continue  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –µ—Å–ª–∏ –Ω–∏ –æ–¥–Ω–∞ —Ñ–æ—Ä–º–∞ —Å–ª–æ–≤–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞

            if news_id not in keyword_results:
                keyword_results[news_id] = {
                    'id': news_id,
                    'title': title,
                    'description': description,
                    'link': row[3],
                    'source': row[4],
                    'published': row[5],
                    'embedding': np.frombuffer(row[6], dtype=np.float32),
                    'keyword_score': 0
                }

            # –ü–û–ó–ò–¶–ò–û–ù–ù–´–ô –í–ï–° —Å NER-–±—É—Å—Ç–æ–º
            position_weight = 0

            # –§—Ä–∞–∑—ã (2+ —Å–ª–æ–≤–∞) –ø–æ–ª—É—á–∞—é—Ç –ø–æ–≤—ã—à–µ–Ω–Ω—ã–π –≤–µ—Å
            is_phrase = ' ' in keyword
            title_weight = 10.0 if is_phrase else 5.0
            desc_weight = 3.0 if is_phrase else 1.5
            text_weight = 2.0 if is_phrase else 1.0

            # NER-–±—É—Å—Ç: –µ—Å–ª–∏ –∫–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ —è–≤–ª—è–µ—Ç—Å—è NER-—Å—É—â–Ω–æ—Å—Ç—å—é –∏–∑ –∑–∞–ø—Ä–æ—Å–∞
            is_ner_entity = keyword.lower() in query_ner_normalized
            ner_multiplier = 5.0 if is_ner_entity else 1.0

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –ª—é–±–æ–π –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–π —Ñ–æ—Ä–º—ã —Å–ª–æ–≤–∞
            found_in_title = any(word_in_text(form, title) for form in word_forms)
            found_in_description = any(word_in_text(form, description) for form in word_forms)
            found_in_full_text = any(word_in_text(form, full_text) for form in word_forms)

            if found_in_title:
                position_weight += title_weight * ner_multiplier  # NER –≤ –∑–∞–≥–æ–ª–æ–≤–∫–µ: x5
            if found_in_description:
                position_weight += desc_weight * ner_multiplier  # NER –≤ –æ–ø–∏—Å–∞–Ω–∏–∏: x5
            if found_in_full_text:
                position_weight += text_weight * ner_multiplier  # NER –≤ —Ç–µ–∫—Å—Ç–µ: x5

            keyword_results[news_id]['keyword_score'] += position_weight

    # –ë—É—Å—Ç –∑–∞ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –≤ –∑–∞–≥–æ–ª–æ–≤–∫–µ
    for news_id, data in keyword_results.items():
        title = data['title']
        # –°—á–∏—Ç–∞–µ–º —Å–∫–æ–ª—å–∫–æ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –∏–∑ –∑–∞–ø—Ä–æ—Å–∞ –µ—Å—Ç—å –≤ –∑–∞–≥–æ–ª–æ–≤–∫–µ (—Å —É—á–µ—Ç–æ–º –º–æ—Ä—Ñ. —Ñ–æ—Ä–º)
        matched_in_title = 0
        for kw in keywords:
            kw_forms = get_word_forms(kw)
            if any(word_in_text(form, title) for form in kw_forms):
                matched_in_title += 1

        if matched_in_title >= 2:
            # 2 —Å–ª–æ–≤–∞ -> x1.3, 3 —Å–ª–æ–≤–∞ -> x1.5, 4+ —Å–ª–æ–≤ -> x1.7
            multi_match_boost = 1.0 + (matched_in_title - 1) * 0.3
            data['keyword_score'] *= multi_match_boost

    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π NER-–±—É—Å—Ç: –ø—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Å —Ä–µ–∞–ª—å–Ω—ã–º–∏ NER-—Å—É—â–Ω–æ—Å—Ç—è–º–∏ –∏–∑ –ë–î
    if query_ner_normalized:
        for news_id in keyword_results.keys():
            cursor.execute('''
                SELECT COUNT(DISTINCT normalized_text)
                FROM entities
                WHERE news_id = ? AND LOWER(normalized_text) IN ({})
            '''.format(','.join('?' * len(query_ner_normalized))),
            [news_id] + list(query_ner_normalized))

            ner_matches = cursor.fetchone()[0]
            if ner_matches > 0:
                # –ö–∞–∂–¥–∞—è —Å–æ–≤–ø–∞–≤—à–∞—è NER-—Å—É—â–Ω–æ—Å—Ç—å –¥–∞–µ—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –±—É—Å—Ç x1.4
                ner_match_boost = 1.0 + (ner_matches * 0.4)
                keyword_results[news_id]['keyword_score'] *= ner_match_boost
                keyword_results[news_id]['ner_matches'] = ner_matches

    # DEBUG: print(f"DEBUG: Total keyword_results: {len(keyword_results)}")

    # –í–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫
    query_embedding = rag.get_embedding(query)
    print(f"DEBUG: query_embedding shape: {query_embedding.shape if query_embedding is not None else None}")
    vector_results = {}

    if query_embedding is not None:
        cursor.execute('SELECT id, title, description, link, source, published, embedding FROM news')

        for row in cursor.fetchall():
            news_id, title, description, link, source, published, embedding_blob = row
            news_embedding = np.frombuffer(embedding_blob, dtype=np.float32)

            similarity = np.dot(query_embedding, news_embedding) / (
                np.linalg.norm(query_embedding) * np.linalg.norm(news_embedding)
            )

            vector_results[news_id] = {
                'id': news_id,
                'title': title,
                'description': description,
                'link': link,
                'source': source,
                'published': published,
                'vector_score': float(similarity)
            }

    conn.close()

    # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Å –±–∞–Ω–∫–æ–≤—Å–∫–æ–π —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å—é
    combined_results = {}
    max_keyword_score = max([r['keyword_score'] for r in keyword_results.values()]) if keyword_results else 1
    # DEBUG: print(f"DEBUG: max_keyword_score={max_keyword_score}")

    for news_id, data in keyword_results.items():
        bank_relevance = calculate_banking_relevance(data['title'], data['description'])

        combined_results[news_id] = {
            'id': data['id'],
            'title': data['title'],
            'description': data['description'],
            'link': data['link'],
            'source': data['source'],
            'published': data['published'],
            'keyword_score': data['keyword_score'] / max_keyword_score if max_keyword_score > 0 else 0,
            'vector_score': vector_results.get(news_id, {}).get('vector_score', 0),
            'bank_boost': bank_relevance['boost'],
            'critical_keywords': bank_relevance['critical_matches'],
            'is_excluded': bank_relevance['exclude_matches'] > 0
        }

    # DEBUG: After keyword_results logging disabled

    # –ù–ï –¥–æ–±–∞–≤–ª—è–µ–º vector-only results - –æ–Ω–∏ —Ä–∞–∑–±–∞–≤–ª—è—é—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—É—é –≤—ã–¥–∞—á—É
    # for news_id, data in vector_results.items():
    #     if news_id not in combined_results:
    #         bank_relevance = calculate_banking_relevance(data['title'], data['description'])
    #
    #         combined_results[news_id] = {
    #             'id': data['id'],
    #             'title': data['title'],
    #             'description': data['description'],
    #             'link': data['link'],
    #             'source': data['source'],
    #             'published': data['published'],
    #             'keyword_score': 0,
    #             'vector_score': data['vector_score'],
    #             'bank_boost': bank_relevance['boost'],
    #             'critical_keywords': bank_relevance['critical_matches'],
    #             'is_excluded': bank_relevance['exclude_matches'] > 0
    #         }

    # –§–∏–Ω–∞–ª—å–Ω—ã–π score —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–º —Å–≤–µ–∂–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π
    for data in combined_results.values():
        if data['keyword_score'] > 0:
            # Keyword match –ø–æ–ª—É—á–∞–µ—Ç –±–æ–ª—å—à–∏–π –≤–µ—Å (85%)
            base_score = 0.85 * data['keyword_score'] + 0.15 * data['vector_score']
        else:
            # –¢–æ–ª—å–∫–æ vector search
            base_score = data['vector_score']

        # –ü—Ä–∏–º–µ–Ω—è–µ–º recency boost - —Å–≤–µ–∂–∏–µ –Ω–æ–≤–æ—Å—Ç–∏ –ø–æ–ª—É—á–∞—é—Ç –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç
        recency_boost = calculate_recency_boost(data.get('published', ''))
        final_score = base_score * recency_boost

        data['similarity'] = final_score
        data['recency_boost'] = recency_boost
        data['geo_boost'] = 1.0
        data['war_penalty'] = 1.0

    results = sorted(combined_results.values(), key=lambda x: x['similarity'], reverse=True)

    # DEBUG: Top 10 results logging disabled

    # –§–ò–õ–¨–¢–†–ê–¶–ò–Ø –û–¢–ö–õ–Æ–ß–ï–ù–ê - –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –≤—Å–µ –Ω–æ–≤–æ—Å—Ç–∏
    return results[:top_k]

# Background task –¥–ª—è –ø–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–æ–≥–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è
async def periodic_update():
    """–ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–µ–π (–∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è)"""
    # –ñ–¥–µ–º 5 –º–∏–Ω—É—Ç –ø–µ—Ä–µ–¥ –ø–µ—Ä–≤—ã–º –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ–º, —á—Ç–æ–±—ã –Ω–µ –±–ª–æ–∫–∏—Ä–æ–≤–∞—Ç—å —Å–µ—Ä–≤–∏—Å –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ
    await asyncio.sleep(300)  # 5 –º–∏–Ω—É—Ç

    while True:
        try:
            print(f"\n[{datetime.now().strftime('%H:%M:%S')}] üîÑ –ù–∞—á–∏–Ω–∞—é –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–µ–π...")
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—É—é –≤–µ—Ä—Å–∏—é - –Ω–µ –±–ª–æ–∫–∏—Ä—É–µ—Ç API!
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤—Å–µ –Ω–æ–≤–æ—Å—Ç–∏ –∏–∑ RSS, –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è –æ—Ç—Ñ–∏–ª—å—Ç—Ä—É–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ
            new_count = await rag.fetch_and_index_news_async(limit_per_source=50, max_concurrent=5)
            print(f"[{datetime.now().strftime('%H:%M:%S')}] ‚úÖ –î–æ–±–∞–≤–ª–µ–Ω–æ {new_count} –Ω–æ–≤—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π")
        except Exception as e:
            print(f"[{datetime.now().strftime('%H:%M:%S')}] ‚ùå –û—à–∏–±–∫–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è: {e}")

        await asyncio.sleep(UPDATE_INTERVAL_SECONDS)

@app.on_event("startup")
async def startup_event():
    """–ó–∞–ø—É—Å–∫ —Ñ–æ–Ω–æ–≤–æ–≥–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è"""
    asyncio.create_task(periodic_update())
    print("=" * 70)
    print("üöÄ News Collector Service –∑–∞–ø—É—â–µ–Ω")
    print(f"üì° API: http://localhost:8001")
    print(f"üìñ Docs: http://localhost:8001/docs")
    print(f"üîÑ –ê–≤—Ç–æ–æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∫–∞–∂–¥—ã–µ {UPDATE_INTERVAL_SECONDS // 60} –º–∏–Ω—É—Ç (–ø–µ—Ä–≤–æ–µ —á–µ—Ä–µ–∑ 5 –º–∏–Ω)")
    print(f"üì∞ –ò—Å—Ç–æ—á–Ω–∏–∫–æ–≤: {len(rag.sources) if hasattr(rag, 'sources') else '255'} (–≤–∫–ª. GDELT)")
    print("=" * 70)

@app.get("/")
async def root():
    return {
        "service": "News Collector API",
        "version": "1.0",
        "status": "running",
        "docs": "/docs"
    }

@app.get("/health")
async def health():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–¥–æ—Ä–æ–≤—å—è —Å–µ—Ä–≤–∏—Å–∞"""
    stats = rag.get_stats()
    return {
        "status": "healthy",
        "total_news": stats['total'],
        "timestamp": datetime.now().isoformat()
    }

@app.get("/stats", response_model=StatsResponse)
async def get_stats():
    """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –±–∞–∑—ã"""
    stats = rag.get_stats()
    return StatsResponse(
        total_news=stats['total'],
        by_source=stats['by_source'],
        by_category=stats['by_category'],
        last_update=datetime.now().isoformat()
    )

def get_news_entities_tags(news_id: int) -> List[EntityTag]:
    """–ü–æ–ª—É—á–∏—Ç—å NER-—Ç–µ–≥–∏ –¥–ª—è –Ω–æ–≤–æ—Å—Ç–∏"""
    import sqlite3
    try:
        conn = sqlite3.connect(rag.db_path)
        cursor = conn.cursor()

        cursor.execute('''
            SELECT entity_text, entity_type, is_banking
            FROM entities
            WHERE news_id = ?
            ORDER BY position
        ''', (news_id,))

        entities = []
        for row in cursor.fetchall():
            entities.append(EntityTag(
                text=row[0],
                type=row[1],
                is_banking=bool(row[2])
            ))

        conn.close()
        return entities
    except Exception as e:
        print(f"Error loading entities for news {news_id}: {e}")
        return []

@app.post("/search", response_model=SearchResponse)
async def search_news(request: SearchRequest):
    """
    –ü–æ–∏—Å–∫ –Ω–æ–≤–æ—Å—Ç–µ–π (—Å LTR-–ø–µ—Ä–µ—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ–º –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–æ)
    """
    try:
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º rag.search_similar –∫–æ—Ç–æ—Ä—ã–π –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø—Ä–∏–º–µ–Ω—è–µ—Ç LTR –µ—Å–ª–∏ –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞
        results = rag.search_similar(request.query, top_k=request.top_k)

        news_items = []
        for item in results:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º NER-—Å—É—â–Ω–æ—Å—Ç–∏ –¥–ª—è –∫–∞–∂–¥–æ–π –Ω–æ–≤–æ—Å—Ç–∏
            entities = get_news_entities_tags(item['id'])

            news_items.append(NewsItem(
                id=item['id'],
                title=item['title'],
                description=item['description'],
                link=item['link'],
                source=item['source'],
                published=item['published'],
                similarity=item.get('similarity', 0),
                keyword_score=item.get('keyword_score', 0),
                vector_score=item.get('vector_score', 0),
                bank_boost=item.get('bank_boost', 1.0),
                critical_keywords=item.get('critical_keywords', 0),
                geo_boost=item.get('geo_boost', 1.0),
                ltr_score=item.get('ltr_score', None),  # LTR-—Å–∫–æ—Ä –µ—Å–ª–∏ –µ—Å—Ç—å
                entities=entities
            ))

        return SearchResponse(
            query=request.query,
            total_found=len(news_items),
            news=news_items,
            timestamp=datetime.now().isoformat()
        )

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Search error: {str(e)}")

@app.get("/api/latest")
async def get_latest_news(limit: int = 20):
    """
    –ü–æ–ª—É—á–∏—Ç—å –ø–æ—Å–ª–µ–¥–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–∏ (–æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–æ –¥–∞—Ç–µ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏)
    """
    try:
        import sqlite3
        from email.utils import parsedate_to_datetime
        from datetime import datetime as dt

        conn = sqlite3.connect(rag.db_path)
        cursor = conn.cursor()

        # –ü–æ–ª—É—á–∞–µ–º –Ω–æ–≤–æ—Å—Ç–∏ –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 3 –≥–æ–¥–∞ (—á—Ç–æ–±—ã –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –ø–æ –¥–∞—Ç–µ)
        cursor.execute('''
            SELECT id, title, description, link, source, published
            FROM news
            WHERE published LIKE '%2025%'
               OR published LIKE '%2024%'
               OR published LIKE '%2023%'
        ''')

        rows = cursor.fetchall()
        conn.close()

        # –ü–∞—Ä—Å–∏–º –¥–∞—Ç—ã –∏ —Å–æ—Ä—Ç–∏—Ä—É–µ–º
        news_with_dates = []
        for row in rows:
            news_id, title, description, link, source, published = row

            try:
                # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã –¥–∞—Ç
                try:
                    # RFC 2822 —Ñ–æ—Ä–º–∞—Ç: "Tue, 12 Mar 2019 06:06:29 GMT"
                    parsed_date = parsedate_to_datetime(published)
                except:
                    # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç: "14 Nov 2025 04:45:00 +0000"
                    try:
                        parsed_date = dt.strptime(published, "%d %b %Y %H:%M:%S %z")
                    except:
                        # –ï—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ–∫—É—â—É—é –¥–∞—Ç—É –º–∏–Ω—É—Å ID
                        parsed_date = dt(2000, 1, 1)

                news_with_dates.append({
                    'id': news_id,
                    'title': title,
                    'description': description,
                    'link': link,
                    'source': source,
                    'published': published,
                    'parsed_date': parsed_date
                })
            except Exception as e:
                continue

        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –¥–∞—Ç–µ (–æ—Ç –Ω–æ–≤—ã—Ö –∫ —Å—Ç–∞—Ä—ã–º)
        news_with_dates.sort(key=lambda x: x['parsed_date'], reverse=True)

        # –ë–µ—Ä–µ–º —Ç–æ–ø-N
        top_news = news_with_dates[:limit]

        news_items = []
        for news_data in top_news:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º NER-—Å—É—â–Ω–æ—Å—Ç–∏ –¥–ª—è –∫–∞–∂–¥–æ–π –Ω–æ–≤–æ—Å—Ç–∏
            entities = get_news_entities_tags(news_data['id'])

            news_items.append(NewsItem(
                id=news_data['id'],
                title=news_data['title'],
                description=news_data['description'],
                link=news_data['link'],
                source=news_data['source'],
                published=news_data['published'],
                similarity=0.0,
                keyword_score=0.0,
                vector_score=0.0,
                bank_boost=1.0,
                critical_keywords=0,
                geo_boost=1.0,
                entities=entities
            ))

        return SearchResponse(
            query="",
            total_found=len(news_items),
            news=news_items,
            timestamp=datetime.now().isoformat()
        )

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error fetching latest news: {str(e)}")

@app.post("/update")
async def trigger_update(background_tasks: BackgroundTasks):
    """–ó–∞–ø—É—Å—Ç–∏—Ç—å –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–µ–π –≤—Ä—É—á–Ω—É—é (–∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è)"""
    async def update_news():
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—É—é –≤–µ—Ä—Å–∏—é - –Ω–µ –±–ª–æ–∫–∏—Ä—É–µ—Ç API!
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤—Å–µ –Ω–æ–≤–æ—Å—Ç–∏ –∏–∑ RSS, –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è –æ—Ç—Ñ–∏–ª—å—Ç—Ä—É–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ
            new_count = await rag.fetch_and_index_news_async(limit_per_source=50, max_concurrent=5)
            print(f"‚úÖ –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ: {new_count} –Ω–æ–≤—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π")
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è: {e}")

    background_tasks.add_task(update_news)
    return {"status": "update_started", "message": "–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∑–∞–ø—É—â–µ–Ω–æ –≤ —Ñ–æ–Ω–µ (async)"}

@app.get("/entities/search/{entity_text}")
async def search_by_entity(entity_text: str, limit: int = 20):
    """
    –ù–∞–π—Ç–∏ –Ω–æ–≤–æ—Å—Ç–∏, —Å–æ–¥–µ—Ä–∂–∞—â–∏–µ —É–∫–∞–∑–∞–Ω–Ω—É—é NER-—Å—É—â–Ω–æ—Å—Ç—å
    """
    try:
        import sqlite3
        conn = sqlite3.connect(rag.db_path)
        cursor = conn.cursor()

        cursor.execute('''
            SELECT DISTINCT n.id, n.title, n.description, n.link, n.source, n.published
            FROM news n
            INNER JOIN entities e ON n.id = e.news_id
            WHERE e.entity_text LIKE ?
            ORDER BY n.published DESC
            LIMIT ?
        ''', (f'%{entity_text}%', limit))

        news_items = []
        for row in cursor.fetchall():
            news_items.append({
                'id': row[0],
                'title': row[1],
                'description': row[2],
                'link': row[3],
                'source': row[4],
                'published': row[5]
            })

        conn.close()

        return {
            'entity': entity_text,
            'total_found': len(news_items),
            'news': news_items
        }

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error searching by entity: {str(e)}")

@app.get("/entities/stats")
async def get_entities_stats():
    """
    –ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ NER-—Å—É—â–Ω–æ—Å—Ç—è–º
    """
    try:
        import sqlite3
        conn = sqlite3.connect(rag.db_path)
        cursor = conn.cursor()

        # –¢–æ–ø –ø–µ—Ä—Å–æ–Ω (–≥—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–π —Ñ–æ—Ä–º–µ)
        cursor.execute('''
            SELECT normalized_text, COUNT(DISTINCT news_id) as count
            FROM entities
            WHERE entity_type = 'person' AND normalized_text IS NOT NULL
            GROUP BY normalized_text
            ORDER BY count DESC
            LIMIT 10
        ''')
        top_persons = [{'name': row[0], 'count': row[1]} for row in cursor.fetchall()]

        # –¢–æ–ø –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π (–∏—Å–∫–ª—é—á–∞–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –°–ú–ò, –≥—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–π —Ñ–æ—Ä–º–µ)
        cursor.execute('''
            SELECT e.normalized_text, COUNT(DISTINCT e.news_id) as count
            FROM entities e
            INNER JOIN news n ON e.news_id = n.id
            WHERE e.entity_type = 'organization'
            AND e.normalized_text IS NOT NULL
            AND LOWER(e.entity_text) NOT LIKE '%' || LOWER(n.source) || '%'
            AND LOWER(n.source) NOT LIKE '%' || LOWER(e.entity_text) || '%'
            GROUP BY e.normalized_text
            ORDER BY count DESC
            LIMIT 10
        ''')
        top_organizations = [{'name': row[0], 'count': row[1]} for row in cursor.fetchall()]

        # –¢–æ–ø –ª–æ–∫–∞—Ü–∏–π (–≥—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–π —Ñ–æ—Ä–º–µ)
        cursor.execute('''
            SELECT normalized_text, COUNT(DISTINCT news_id) as count
            FROM entities
            WHERE entity_type = 'location' AND normalized_text IS NOT NULL
            GROUP BY normalized_text
            ORDER BY count DESC
            LIMIT 10
        ''')
        top_locations = [{'name': row[0], 'count': row[1]} for row in cursor.fetchall()]

        # –ë–∞–Ω–∫–æ–≤—Å–∫–∏–µ —Å—É—â–Ω–æ—Å—Ç–∏ (–∏—Å–∫–ª—é—á–∞–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –°–ú–ò, –≥—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–π —Ñ–æ—Ä–º–µ)
        cursor.execute('''
            SELECT e.normalized_text, COUNT(DISTINCT e.news_id) as count
            FROM entities e
            INNER JOIN news n ON e.news_id = n.id
            WHERE e.is_banking = 1
            AND e.normalized_text IS NOT NULL
            AND LOWER(e.entity_text) NOT LIKE '%' || LOWER(n.source) || '%'
            AND LOWER(n.source) NOT LIKE '%' || LOWER(e.entity_text) || '%'
            GROUP BY e.normalized_text
            ORDER BY count DESC
            LIMIT 10
        ''')
        banking_entities = [{'name': row[0], 'count': row[1]} for row in cursor.fetchall()]

        # –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        cursor.execute('SELECT COUNT(DISTINCT entity_text) FROM entities')
        total_unique_entities = cursor.fetchone()[0]

        cursor.execute('SELECT COUNT(*) FROM entities')
        total_entity_mentions = cursor.fetchone()[0]

        conn.close()

        return {
            'total_unique_entities': total_unique_entities,
            'total_mentions': total_entity_mentions,
            'top_persons': top_persons,
            'top_organizations': top_organizations,
            'top_locations': top_locations,
            'banking_entities': banking_entities
        }

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error fetching entity stats: {str(e)}")

@app.get("/entities/id/{news_id}")
async def get_news_entities(news_id: int):
    """
    –ü–æ–ª—É—á–∏—Ç—å –≤—Å–µ NER-—Å—É—â–Ω–æ—Å—Ç–∏ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –Ω–æ–≤–æ—Å—Ç–∏
    """
    try:
        import sqlite3
        conn = sqlite3.connect(rag.db_path)
        cursor = conn.cursor()

        cursor.execute('''
            SELECT entity_text, entity_type, is_banking
            FROM entities
            WHERE news_id = ?
            ORDER BY position
        ''', (news_id,))

        entities = []
        for row in cursor.fetchall():
            entities.append({
                'text': row[0],
                'type': row[1],
                'is_banking': bool(row[2])
            })

        conn.close()

        return {
            'news_id': news_id,
            'entities': entities,
            'total': len(entities)
        }

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error fetching entities: {str(e)}")

@app.get("/entities/trends")
async def get_entity_trends(days: int = 30, entity_type: Optional[str] = None, top_n: int = 10):
    """
    –ü–æ–ª—É—á–∏—Ç—å —Ç—Ä–µ–Ω–¥—ã —É–ø–æ–º–∏–Ω–∞–Ω–∏—è NER-—Å—É—â–Ω–æ—Å—Ç–µ–π –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ N –¥–Ω–µ–π

    Args:
        days: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–Ω–µ–π –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 30)
        entity_type: —Ñ–∏–ª—å—Ç—Ä –ø–æ —Ç–∏–ø—É —Å—É—â–Ω–æ—Å—Ç–∏ (person/organization/location)
        top_n: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–ø —Å—É—â–Ω–æ—Å—Ç–µ–π –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
    """
    try:
        import sqlite3
        from datetime import datetime, timedelta
        from email.utils import parsedate_to_datetime
        from collections import defaultdict

        conn = sqlite3.connect(rag.db_path)
        cursor = conn.cursor()

        # –í—ã—á–∏—Å–ª—è–µ–º –¥–∞—Ç—É –æ—Ç—Å–µ—á–∫–∏ (–¥–µ–ª–∞–µ–º timezone-aware)
        from datetime import timezone
        cutoff_date = datetime.now(timezone.utc) - timedelta(days=days)

        # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –Ω–æ–≤–æ—Å—Ç–∏ —Å —Å—É—â–Ω–æ—Å—Ç—è–º–∏ (–∏—Å–∫–ª—é—á–∞–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –Ω–æ–≤–æ—Å—Ç–µ–π, –≥—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–π —Ñ–æ—Ä–º–µ)
        type_filter = f"AND e.entity_type = '{entity_type}'" if entity_type else ""

        cursor.execute(f'''
            SELECT e.normalized_text, n.published, e.news_id, n.source
            FROM entities e
            INNER JOIN news n ON e.news_id = n.id
            WHERE e.normalized_text IS NOT NULL
            {type_filter}
        ''')

        # –ü–∞—Ä—Å–∏–º –¥–∞—Ç—ã –∏ —Ñ–∏–ª—å—Ç—Ä—É–µ–º
        entity_mentions = defaultdict(lambda: defaultdict(set))

        for row in cursor.fetchall():
            normalized_text, published, news_id, source = row

            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –µ—Å–ª–∏ —Å—É—â–Ω–æ—Å—Ç—å —Å–æ–≤–ø–∞–¥–∞–µ—Ç —Å –∏—Å—Ç–æ—á–Ω–∏–∫–æ–º (—ç—Ç–æ –Ω–∞–∑–≤–∞–Ω–∏–µ –°–ú–ò, –∞ –Ω–µ —É–ø–æ–º–∏–Ω–∞–Ω–∏–µ)
            if normalized_text.lower() in source.lower() or source.lower() in normalized_text.lower():
                continue

            try:
                # –ü–∞—Ä—Å–∏–º –¥–∞—Ç—É –∏–∑ RFC 2822 —Ñ–æ—Ä–º–∞—Ç–∞
                pub_date = parsedate_to_datetime(published)

                # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ –¥–∞—Ç–µ
                if pub_date >= cutoff_date:
                    date_str = pub_date.strftime('%Y-%m-%d')
                    entity_mentions[normalized_text][date_str].add(news_id)
            except:
                continue

        # –ü–æ–ª—É—á–∞–µ–º —Ç–æ–ø —Å—É—â–Ω–æ—Å—Ç–µ–π –ø–æ –æ–±—â–µ–º—É –∫–æ–ª–∏—á–µ—Å—Ç–≤—É —É–ø–æ–º–∏–Ω–∞–Ω–∏–π
        entity_totals = {}
        for entity, dates_dict in entity_mentions.items():
            total = sum(len(news_ids) for news_ids in dates_dict.values())
            entity_totals[entity] = total

        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –∏ –±–µ—Ä–µ–º —Ç–æ–ø N
        top_entities = sorted(entity_totals.items(), key=lambda x: x[1], reverse=True)[:top_n]
        top_entity_names = [entity for entity, _ in top_entities]

        # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –¥–∞—Ç –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ
        dates = []
        current_date = cutoff_date.date()
        end_date = datetime.now().date()

        while current_date <= end_date:
            dates.append(current_date.strftime('%Y-%m-%d'))
            current_date += timedelta(days=1)

        # –§–æ—Ä–º–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –≥—Ä–∞—Ñ–∏–∫–∞
        datasets = []
        for entity in top_entity_names:
            data = [len(entity_mentions[entity].get(date, set())) for date in dates]
            datasets.append({
                'label': entity,
                'data': data
            })

        conn.close()

        return {
            'dates': dates,
            'datasets': datasets,
            'period_days': days,
            'entity_type': entity_type or 'all',
            'top_n': top_n
        }

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error fetching entity trends: {str(e)}")

@app.get("/api/trends/daily")
async def get_daily_trends(top_n: int = 20):
    """
    –ü–æ–ª—É—á–∏—Ç—å —Ç–æ–ø-N —Ç—Ä–µ–Ω–¥–æ–≤ –ø–æ –∏–∑–º–µ–Ω–µ–Ω–∏—é —É–ø–æ–º–∏–Ω–∞–Ω–∏–π (–≤—á–µ—Ä–∞ vs —Å–µ–≥–æ–¥–Ω—è)
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—É—â–Ω–æ—Å—Ç–∏ —Å –Ω–∞–∏–±–æ–ª—å—à–∏–º –∏–∑–º–µ–Ω–µ–Ω–∏–µ–º –≤ –∞–±—Å–æ–ª—é—Ç–Ω–æ–º –∑–Ω–∞—á–µ–Ω–∏–∏
    """
    try:
        import sqlite3
        from datetime import datetime, timedelta
        from email.utils import parsedate_to_datetime

        conn = sqlite3.connect(rag.db_path)
        cursor = conn.cursor()

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å–µ–≥–æ–¥–Ω—è –∏ –≤—á–µ—Ä–∞
        now = datetime.now()
        today_start = now.replace(hour=0, minute=0, second=0, microsecond=0)
        yesterday_start = today_start - timedelta(days=1)

        # –ü–æ–ª—É—á–∞–µ–º —É–ø–æ–º–∏–Ω–∞–Ω–∏—è —Å—É—â–Ω–æ—Å—Ç–µ–π –∑–∞ —Å–µ–≥–æ–¥–Ω—è
        cursor.execute('''
            SELECT e.normalized_text, COUNT(DISTINCT e.news_id) as count
            FROM entities e
            INNER JOIN news n ON e.news_id = n.id
            WHERE e.normalized_text IS NOT NULL
            GROUP BY e.normalized_text
        ''')

        all_entities = {}
        for row in cursor.fetchall():
            entity_name, count = row
            all_entities[entity_name] = {'today': 0, 'yesterday': 0}

        # –°—á–∏—Ç–∞–µ–º —É–ø–æ–º–∏–Ω–∞–Ω–∏—è –ø–æ –¥–Ω—è–º
        cursor.execute('''
            SELECT e.normalized_text, n.published
            FROM entities e
            INNER JOIN news n ON e.news_id = n.id
            WHERE e.normalized_text IS NOT NULL
        ''')

        for row in cursor.fetchall():
            entity_name, published = row

            if entity_name not in all_entities:
                all_entities[entity_name] = {'today': 0, 'yesterday': 0}

            try:
                # –ü–∞—Ä—Å–∏–º –¥–∞—Ç—É
                try:
                    pub_date = parsedate_to_datetime(published)
                except:
                    # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç
                    pub_date = datetime.strptime(published, "%d %b %Y %H:%M:%S %z")

                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –¥–µ–Ω—å (—Å—Ä–∞–≤–Ω–∏–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –¥–∞—Ç—ã, –∏–≥–Ω–æ—Ä–∏—Ä—É—è –≤—Ä–µ–º—è –∏ timezone)
                pub_date_only = pub_date.date() if hasattr(pub_date, 'date') else pub_date
                today_only = today_start.date()
                yesterday_only = yesterday_start.date()

                if pub_date_only == today_only:
                    all_entities[entity_name]['today'] += 1
                elif pub_date_only == yesterday_only:
                    all_entities[entity_name]['yesterday'] += 1
            except:
                continue

        conn.close()

        # –í—ã—á–∏—Å–ª—è–µ–º –∏–∑–º–µ–Ω–µ–Ω–∏—è
        trends = []
        for entity_name, counts in all_entities.items():
            today_count = counts['today']
            yesterday_count = counts['yesterday']

            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –µ—Å–ª–∏ –Ω–µ—Ç —É–ø–æ–º–∏–Ω–∞–Ω–∏–π –Ω–∏ –≤—á–µ—Ä–∞, –Ω–∏ —Å–µ–≥–æ–¥–Ω—è
            if today_count == 0 and yesterday_count == 0:
                continue

            change = today_count - yesterday_count

            trends.append({
                'entity': entity_name,
                'today': today_count,
                'yesterday': yesterday_count,
                'change': change,
                'change_abs': abs(change)
            })

        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –∞–±—Å–æ–ª—é—Ç–Ω–æ–º—É –∏–∑–º–µ–Ω–µ–Ω–∏—é (–Ω–∞–∏–±–æ–ª—å—à–∏–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è –≤–≤–µ—Ä—Ö—É)
        trends.sort(key=lambda x: x['change_abs'], reverse=True)

        # –¢–æ–ø-N
        top_trends = trends[:top_n]

        return {
            'trends': top_trends,
            'total_entities': len(all_entities),
            'timestamp': datetime.now().isoformat()
        }

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error fetching daily trends: {str(e)}")

@app.get("/api/entity/{entity_name}/timeline")
async def get_entity_timeline(entity_name: str, days: int = 30):
    """
    –ü–æ–ª—É—á–∏—Ç—å –≤—Ä–µ–º–µ–Ω–Ω–æ–π —Ä—è–¥ —É–ø–æ–º–∏–Ω–∞–Ω–∏–π —Å—É—â–Ω–æ—Å—Ç–∏ –ø–æ –¥–Ω—è–º
    """
    try:
        conn = sqlite3.connect(rag.db_path)
        cursor = conn.cursor()

        # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ —É–ø–æ–º–∏–Ω–∞–Ω–∏—è —Å—É—â–Ω–æ—Å—Ç–∏ —Å –¥–∞—Ç–∞–º–∏
        cursor.execute('''
            SELECT n.published
            FROM news n
            JOIN entities e ON n.id = e.news_id
            WHERE LOWER(e.entity_text) = LOWER(?)
            ORDER BY n.published DESC
        ''', (entity_name,))

        # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —É–ø–æ–º–∏–Ω–∞–Ω–∏—è –ø–æ –¥–Ω—è–º
        daily_counts = {}
        all_dates = []

        for row in cursor.fetchall():
            pub_str = row[0]
            if pub_str:
                try:
                    # –ü–∞—Ä—Å–∏–º —Ä–∞–∑–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã –¥–∞—Ç
                    if pub_str.startswith(('Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun')):
                        # RFC 2822 —Ñ–æ—Ä–º–∞—Ç (–∏–∑ RSS): "Wed, 30 Apr 2025 07:00:00 GMT"
                        from email.utils import parsedate_to_datetime
                        pub_date = parsedate_to_datetime(pub_str)
                    else:
                        pub_date = datetime.fromisoformat(pub_str.replace('Z', '+00:00'))

                    date_key = pub_date.date().isoformat()
                    daily_counts[date_key] = daily_counts.get(date_key, 0) + 1
                    all_dates.append(pub_date.date())
                except Exception as e:
                    # –õ–æ–≥–∏—Ä—É–µ–º –æ—à–∏–±–∫–∏ –ø–∞—Ä—Å–∏–Ω–≥–∞ —Ç–æ–ª—å–∫–æ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
                    # print(f"Could not parse date '{pub_str}': {e}")
                    continue

        conn.close()

        # –ï—Å–ª–∏ –Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç–æ–π –≥—Ä–∞—Ñ–∏–∫ –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ N –¥–Ω–µ–π –æ—Ç —Å–µ–≥–æ–¥–Ω—è
        if not all_dates:
            today = datetime.now().date()
            timeline = []
            for i in range(days - 1, -1, -1):
                date = today - timedelta(days=i)
                timeline.append({
                    'date': date.isoformat(),
                    'count': 0
                })
        else:
            # –°—Ç—Ä–æ–∏–º –≥—Ä–∞—Ñ–∏–∫ –æ—Ç –ø–æ—Å–ª–µ–¥–Ω–µ–π –¥–∞—Ç—ã —É–ø–æ–º–∏–Ω–∞–Ω–∏—è –Ω–∞–∑–∞–¥ –Ω–∞ N –¥–Ω–µ–π
            latest_date = max(all_dates)
            timeline = []

            for i in range(days - 1, -1, -1):
                date = latest_date - timedelta(days=i)
                date_str = date.isoformat()
                count = daily_counts.get(date_str, 0)
                timeline.append({
                    'date': date_str,
                    'count': count
                })

        return {
            'entity': entity_name,
            'timeline': timeline,
            'total_mentions': sum(daily_counts.values())
        }

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error fetching entity timeline: {str(e)}")

@app.get("/api/entity/{entity_name}/news")
async def get_entity_news(entity_name: str, limit: int = 20):
    """
    –ü–æ–ª—É—á–∏—Ç—å –Ω–æ–≤–æ—Å—Ç–∏ —Å —É–ø–æ–º–∏–Ω–∞–Ω–∏–µ–º –¥–∞–Ω–Ω–æ–π —Å—É—â–Ω–æ—Å—Ç–∏
    """
    try:
        conn = sqlite3.connect(rag.db_path)
        cursor = conn.cursor()

        # –ü–æ–ª—É—á–∞–µ–º –Ω–æ–≤–æ—Å—Ç–∏ —Å —ç—Ç–æ–π —Å—É—â–Ω–æ—Å—Ç—å—é
        cursor.execute('''
            SELECT DISTINCT n.id, n.title, n.description, n.link, n.source, n.published
            FROM news n
            JOIN entities e ON n.id = e.news_id
            WHERE LOWER(e.entity_text) = LOWER(?)
            ORDER BY n.published DESC
            LIMIT ?
        ''', (entity_name, limit))

        news_list = []
        for row in cursor.fetchall():
            news_list.append({
                'id': row[0],
                'title': row[1],
                'description': row[2],
                'url': row[3],
                'source': row[4],
                'published': row[5]
            })

        conn.close()

        return {
            'entity': entity_name,
            'news': news_list,
            'total': len(news_list)
        }

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error fetching entity news: {str(e)}")

@app.post("/api/ltr/generate_candidates")
async def generate_ltr_candidates(request: SearchRequest):
    """
    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ —Å —Ñ–∏—á–∞–º–∏ –¥–ª—è LTR-—Ä–∞–∑–º–µ—Ç–∫–∏ –ø–æ –∑–∞–ø—Ä–æ—Å—É
    """
    try:
        from ltr_dataset_generator import LTRDatasetGenerator

        generator = LTRDatasetGenerator()
        top_k = min(request.top_k, 20)  # –ú–∞–∫—Å–∏–º—É–º 20 –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤

        candidates = generator.generate_candidates(request.query, top_k=top_k)

        return {
            'query': request.query,
            'candidates': candidates,
            'total': len(candidates)
        }

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error generating LTR candidates: {str(e)}")


class RetrainRequest(BaseModel):
    """–ó–∞–ø—Ä–æ—Å –Ω–∞ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã–º –¥–∞—Ç–∞—Å–µ—Ç–æ–º"""
    dataset: List[Dict]  # –°–ø–∏—Å–æ–∫ —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö –ø–∞—Ä {query, news_id, features, label}


@app.post("/api/ltr/retrain")
async def retrain_ltr_model(request: RetrainRequest):
    """
    –ü–µ—Ä–µ–æ–±—É—á–∞–µ—Ç LTR –º–æ–¥–µ–ª—å –Ω–∞ —Ä–∞–∑–º–µ—á–µ–Ω–Ω–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ä—ã–µ –∏ –Ω–æ–≤—ã–µ feature importances
    """
    try:
        import pandas as pd
        import numpy as np
        from sklearn.model_selection import train_test_split
        from lightgbm import LGBMRanker
        import pickle
        from datetime import datetime

        # 1. –ü–æ–ª—É—á–∞–µ–º —Å—Ç–∞—Ä—ã–µ feature importances
        global rag
        old_importances = {}
        if hasattr(rag, 'ltr_model') and rag.ltr_model:
            feature_names = rag.feature_columns
            importances = rag.ltr_model.feature_importances_
            old_importances = {name: float(imp) for name, imp in zip(feature_names, importances)}

        # 2. –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ (label –Ω–µ null)
        labeled_data = [item for item in request.dataset if item.get('label') is not None]

        if len(labeled_data) < 10:
            raise HTTPException(status_code=400, detail=f"–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö: {len(labeled_data)}. –ù—É–∂–Ω–æ –º–∏–Ω–∏–º—É–º 10.")

        # 3. –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
        df = pd.DataFrame(labeled_data)

        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ñ–∏—á–∏ –∏–∑ —Å–ª–æ–≤–∞—Ä—è features
        feature_columns = list(labeled_data[0]['features'].keys())
        X = np.array([list(item['features'].values()) for item in labeled_data])
        y = df['label'].values

        # –ì—Ä—É–ø–ø—ã –∑–∞–ø—Ä–æ—Å–æ–≤ (–¥–ª—è LTR –≤–∞–∂–Ω–æ!)
        queries = df['query'].values
        query_groups = []
        current_query = None
        current_count = 0

        for q in queries:
            if q != current_query:
                if current_count > 0:
                    query_groups.append(current_count)
                current_query = q
                current_count = 1
            else:
                current_count += 1
        query_groups.append(current_count)

        # 4. Train/val split (80/20)
        n_queries = len(query_groups)
        split_idx = int(0.8 * n_queries)

        train_size = sum(query_groups[:split_idx])

        X_train = X[:train_size]
        y_train = y[:train_size]
        train_groups = query_groups[:split_idx]

        X_val = X[train_size:]
        y_val = y[train_size:]
        val_groups = query_groups[split_idx:]

        # 5. –û–±—É—á–∞–µ–º –Ω–æ–≤—É—é –º–æ–¥–µ–ª—å
        model = LGBMRanker(
            objective='lambdarank',
            metric='ndcg',
            n_estimators=100,
            learning_rate=0.05,
            num_leaves=31,
            max_depth=6,
            min_child_samples=5,
            random_state=42,
            n_jobs=-1
        )

        model.fit(
            X_train, y_train,
            group=train_groups,
            eval_set=[(X_val, y_val)],
            eval_group=[val_groups],
            eval_metric='ndcg',
            callbacks=[],
        )

        # 6. –ü–æ–ª—É—á–∞–µ–º –Ω–æ–≤—ã–µ feature importances
        new_importances = {name: float(imp) for name, imp in zip(feature_columns, model.feature_importances_)}

        # 7. –í—ã—á–∏—Å–ª—è–µ–º –∏–∑–º–µ–Ω–µ–Ω–∏—è
        changes = {}
        for feature in feature_columns:
            old_val = old_importances.get(feature, 0)
            new_val = new_importances.get(feature, 0)
            changes[feature] = {
                'old': old_val,
                'new': new_val,
                'delta': new_val - old_val,
                'delta_percent': ((new_val - old_val) / old_val * 100) if old_val > 0 else 0
            }

        # 8. –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        # Backup —Å—Ç–∞—Ä–æ–π –º–æ–¥–µ–ª–∏
        import shutil
        if os.path.exists('ltr_model.pkl'):
            shutil.copy('ltr_model.pkl', f'ltr_model_backup_{timestamp}.pkl')

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –Ω–æ–≤—É—é –º–æ–¥–µ–ª—å
        model_data = {
            'model': model,
            'feature_columns': feature_columns,
            'trained_on': timestamp,
            'n_samples': len(labeled_data),
            'n_queries': len(set(df['query']))
        }

        with open('ltr_model.pkl', 'wb') as f:
            pickle.dump(model_data, f)

        # 9. –ü–µ—Ä–µ–∑–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –≤ –ø–∞–º—è—Ç–∏
        rag.ltr_model = model
        rag.feature_columns = feature_columns

        # 10. –í—ã—á–∏—Å–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏ –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
        from sklearn.metrics import ndcg_score

        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
        y_pred_val = model.predict(X_val)

        # NDCG –ø–æ –≥—Ä—É–ø–ø–∞–º
        ndcg_scores = []
        start_idx = 0
        for group_size in val_groups:
            end_idx = start_idx + group_size
            y_true_group = y_val[start_idx:end_idx].reshape(1, -1)
            y_pred_group = y_pred_val[start_idx:end_idx].reshape(1, -1)

            if len(y_true_group[0]) > 0:
                ndcg = ndcg_score(y_true_group, y_pred_group)
                ndcg_scores.append(ndcg)

            start_idx = end_idx

        avg_ndcg = float(np.mean(ndcg_scores)) if ndcg_scores else 0.0

        return {
            'success': True,
            'message': f'–ú–æ–¥–µ–ª—å –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∞ –Ω–∞ {len(labeled_data)} –ø—Ä–∏–º–µ—Ä–∞—Ö ({len(set(df["query"]))} –∑–∞–ø—Ä–æ—Å–æ–≤)',
            'old_importances': old_importances,
            'new_importances': new_importances,
            'changes': changes,
            'metrics': {
                'val_ndcg': avg_ndcg,
                'n_train_samples': len(X_train),
                'n_val_samples': len(X_val),
                'n_queries_total': len(set(df['query'])),
            },
            'backup_file': f'ltr_model_backup_{timestamp}.pkl'
        }

    except Exception as e:
        import traceback
        raise HTTPException(status_code=500, detail=f"Error retraining model: {str(e)}\n{traceback.format_exc()}")


if __name__ == "__main__":
    uvicorn.run(
        app,
        host="0.0.0.0",
        port=8001,
        log_level="info"
    )
